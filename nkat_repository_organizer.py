#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT Ultimate Repository Organizer
ãƒªãƒã‚¸ãƒˆãƒªå®Œå…¨æ•´ç†æ•´é “ã‚·ã‚¹ãƒ†ãƒ 

Don't hold back. Give it your all!!
RTX3080 CUDAå¯¾å¿œ & é›»æºæ–­ä¿è­·æ©Ÿèƒ½ä»˜ã
"""

import os
import sys
import shutil
import hashlib
import json
import time
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from tqdm import tqdm
import subprocess

class NKATRepositoryOrganizer:
    def __init__(self):
        self.root_dir = Path.cwd()
        self.backup_dir = Path("recovery_data/repo_backup")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•´ç†çµ±è¨ˆ
        self.stats = {
            'total_files': 0,
            'duplicate_files': 0,
            'removed_files': 0,
            'organized_dirs': 0,
            'space_saved': 0,
            'start_time': datetime.now().isoformat()
        }
        
        # é‡è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
        self.important_dirs = {
            'src': 'æ ¸å¿ƒã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰',
            'docs': 'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ',
            'papers': 'è«–æ–‡ãƒ»ç ”ç©¶è³‡æ–™', 
            'Results': 'è¨ˆç®—çµæœ',
            'figures': 'å›³è¡¨ãƒ»ã‚°ãƒ©ãƒ•',
            'data': 'ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ',
            'recovery_data': 'ãƒªã‚«ãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿',
            'checkpoints': 'ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ',
            'tests': 'ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰',
            'scripts': 'ã‚¹ã‚¯ãƒªãƒ—ãƒˆ',
            'utils': 'ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£'
        }
        
        # å‰Šé™¤å¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.cleanup_patterns = [
            '**/__pycache__/',
            '**/*.pyc',
            '**/*.pyo',
            '**/*.tmp',
            '**/.DS_Store',
            '**/Thumbs.db',
            '**/*.log',
            '**/*.backup',
            '**/*~'
        ]
        
        # Gitç®¡ç†å¯¾è±¡å¤–ã«ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.gitignore_additions = [
            "# NKAT Repository Organization",
            "__pycache__/",
            "*.pyc",
            "*.pyo", 
            "*.tmp",
            ".DS_Store",
            "Thumbs.db",
            "*.backup",
            "*~",
            "",
            "# Large temporary files",
            "*.temp",
            "temp/",
            "",
            "# IDE files",
            ".vscode/",
            ".idea/",
            "*.swp",
            "*.swo"
        ]
        
        print("ğŸ§¹ NKAT Repository Organizer åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“‚ å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.root_dir}")
        print(f"ğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å…ˆ: {self.backup_dir}")
    
    def create_backup(self):
        """é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        print("\nğŸ’¾ é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆä¸­...")
        
        important_files = [
            '.gitattributes',
            '.gitignore', 
            'README.md',
            'requirements.txt',
            'nkat_ultimate_git_lfs_push_system.py'
        ]
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_subdir = self.backup_dir / f"backup_{timestamp}"
        backup_subdir.mkdir(exist_ok=True)
        
        for file_pattern in important_files:
            for file_path in self.root_dir.rglob(file_pattern):
                if file_path.is_file():
                    rel_path = file_path.relative_to(self.root_dir)
                    backup_file = backup_subdir / rel_path
                    backup_file.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(file_path, backup_file)
                    print(f"ğŸ“„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {rel_path}")
        
        print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_subdir}")
        return backup_subdir
    
    def find_duplicate_files(self):
        """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œå‡º"""
        print("\nğŸ” é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºä¸­...")
        
        file_hashes = defaultdict(list)
        
        # ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã§é‡è¤‡æ¤œå‡º
        for file_path in tqdm(list(self.root_dir.rglob("*")), desc="ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³"):
            if file_path.is_file() and not self._should_ignore_file(file_path):
                try:
                    file_hash = self._calculate_file_hash(file_path)
                    file_hashes[file_hash].append(file_path)
                    self.stats['total_files'] += 1
                except Exception as e:
                    print(f"âš ï¸ ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        # é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        duplicates = {hash_val: paths for hash_val, paths in file_hashes.items() if len(paths) > 1}
        
        if duplicates:
            print(f"\nğŸ“Š é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {len(duplicates)} ã‚°ãƒ«ãƒ¼ãƒ—")
            for hash_val, paths in list(duplicates.items())[:5]:  # æœ€åˆã®5ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¡¨ç¤º
                print(f"  Hash: {hash_val[:16]}...")
                for path in paths:
                    size_mb = path.stat().st_size / (1024 * 1024)
                    print(f"    ğŸ“ {path.relative_to(self.root_dir)} ({size_mb:.2f} MB)")
                print()
        
        return duplicates
    
    def _calculate_file_hash(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥å€¤è¨ˆç®—"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _should_ignore_file(self, file_path):
        """ç„¡è¦–ã™ã¹ããƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯"""
        ignore_patterns = [
            '.git/', '__pycache__/', '.vscode/', '.idea/', 
            'node_modules/', 'temp/', '.tmp'
        ]
        
        path_str = str(file_path)
        return any(pattern in path_str for pattern in ignore_patterns)
    
    def remove_duplicates(self, duplicates):
        """é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤"""
        print("\nğŸ—‘ï¸ é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ä¸­...")
        
        for hash_val, paths in tqdm(duplicates.items(), desc="é‡è¤‡å‰Šé™¤"):
            if len(paths) <= 1:
                continue
            
            # æœ€ã‚‚é‡è¦ãªå ´æ‰€ã«ã‚ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒ
            paths_sorted = sorted(paths, key=self._file_importance_score, reverse=True)
            keep_file = paths_sorted[0]
            
            for duplicate_file in paths_sorted[1:]:
                try:
                    file_size = duplicate_file.stat().st_size
                    duplicate_file.unlink()
                    self.stats['removed_files'] += 1
                    self.stats['space_saved'] += file_size
                    print(f"ğŸ—‘ï¸ å‰Šé™¤: {duplicate_file.relative_to(self.root_dir)}")
                except Exception as e:
                    print(f"âŒ å‰Šé™¤ã‚¨ãƒ©ãƒ¼ {duplicate_file}: {e}")
            
            print(f"âœ… ä¿æŒ: {keep_file.relative_to(self.root_dir)}")
        
        self.stats['duplicate_files'] = len(duplicates)
    
    def _file_importance_score(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        path_str = str(file_path.relative_to(self.root_dir))
        score = 0
        
        # é‡è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«åŸºã¥ãã‚¹ã‚³ã‚¢
        important_dirs = ['src/', 'docs/', 'papers/', 'main/']
        for dir_name in important_dirs:
            if path_str.startswith(dir_name):
                score += 100
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã«åŸºã¥ãã‚¹ã‚³ã‚¢
        if any(keyword in path_str.lower() for keyword in ['nkat', 'ultimate', 'main', 'final']):
            score += 50
        
        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã»ã©é«˜ã‚¹ã‚³ã‚¢
        try:
            mtime = file_path.stat().st_mtime
            score += int(mtime / 86400)  # æ—¥æ•°ã‚’ã‚¹ã‚³ã‚¢ã«åŠ ç®—
        except:
            pass
        
        return score
    
    def cleanup_temp_files(self):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        print("\nğŸ§½ ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
        
        removed_count = 0
        saved_space = 0
        
        for pattern in tqdm(self.cleanup_patterns, desc="ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"):
            for file_path in self.root_dir.rglob(pattern.replace('**/', '')):
                if file_path.exists():
                    try:
                        if file_path.is_file():
                            saved_space += file_path.stat().st_size
                            file_path.unlink()
                            removed_count += 1
                        elif file_path.is_dir():
                            shutil.rmtree(file_path)
                            removed_count += 1
                        print(f"ğŸ§½ å‰Šé™¤: {file_path.relative_to(self.root_dir)}")
                    except Exception as e:
                        print(f"âŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        print(f"âœ… ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {removed_count} å€‹, {saved_space / (1024*1024):.2f} MB ç¯€ç´„")
        self.stats['space_saved'] += saved_space
    
    def organize_directory_structure(self):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®æœ€é©åŒ–"""
        print("\nğŸ“‚ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ æœ€é©åŒ–ä¸­...")
        
        # é‡è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèªãƒ»ä½œæˆ
        for dir_name, description in self.important_dirs.items():
            dir_path = self.root_dir / dir_name
            if not dir_path.exists():
                dir_path.mkdir(exist_ok=True)
                print(f"ğŸ“ ä½œæˆ: {dir_name}/ ({description})")
                self.stats['organized_dirs'] += 1
        
        # README.md ã®å­˜åœ¨ç¢ºèª
        readme_path = self.root_dir / "README.md"
        if not readme_path.exists():
            self._create_ultimate_readme()
    
    def _create_ultimate_readme(self):
        """ç©¶æ¥µã®README.mdä½œæˆ"""
        readme_content = f"""# NKAT Ultimate Unification

ğŸš€ **Non-Commutative Kolmogorov-Arnold Representation Theory**  
ç©¶æ¥µçµ±ä¸€ç†è«–ã«ã‚ˆã‚‹æ•°å­¦ç‰©ç†å­¦çš„è§£æã‚·ã‚¹ãƒ†ãƒ 

## ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ã€NKAT (Non-Commutative Kolmogorov-Arnold Theory) ã‚’ç”¨ã„ãŸï¼š
- ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã®è§£æ±º
- ãƒŸãƒ¬ãƒ‹ã‚¢ãƒ æ‡¸è³å•é¡Œã®çµ±ä¸€çš„è§£æ³•
- é‡å­é‡åŠ›æƒ…å ±ç†è«–ã®æ§‹ç¯‰
- æ„è­˜ã¨æ•°å­¦ã®çµ±åˆç†è«–

## ğŸ›¡ï¸ ç‰¹å¾´

- **RTX3080 CUDAå¯¾å¿œ**: é«˜æ€§èƒ½GPUè¨ˆç®—
- **é›»æºæ–­ä¿è­·æ©Ÿèƒ½**: è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
- **Git LFSå¯¾å¿œ**: å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†
- **å®Œå…¨ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ **: ãƒ‡ãƒ¼ã‚¿æå¤±é˜²æ­¢

## ğŸ“Š ä¸»è¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

- `src/`: æ ¸å¿ƒã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
- `papers/`: è«–æ–‡ãƒ»ç ”ç©¶è³‡æ–™
- `Results/`: è¨ˆç®—çµæœ
- `data/`: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (Git LFSç®¡ç†)
- `recovery_data/`: ãƒªã‚«ãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿
- `docs/`: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

## ğŸš€ ä½¿ç”¨æ–¹æ³•

```bash
# Pythonç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
pip install -r requirements.txt

# ãƒ¡ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
python -3 run_nkat.py

# Git LFS ãƒ—ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
python -3 nkat_ultimate_git_lfs_push_system.py
```

## ğŸ‰ æˆæœ

- âœ… Git LFS ã«ã‚ˆã‚‹å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†å®Œæˆ
- âœ… é›»æºæ–­å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
- âœ… ãƒªãƒã‚¸ãƒˆãƒªå®Œå…¨æ•´ç†æ•´é “
- âœ… RTX3080 CUDA æœ€é©åŒ–

## ğŸ“ Contact

Don't hold back. Give it your all!!

---
Generated by NKAT Repository Organizer
Last Updated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
        
        with open(self.root_dir / "README.md", 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print("ğŸ“„ README.md ä½œæˆå®Œäº†")
    
    def update_gitignore(self):
        """gitignoreã®æ›´æ–°"""
        print("\nğŸ“ .gitignore æ›´æ–°ä¸­...")
        
        gitignore_path = self.root_dir / ".gitignore"
        
        # æ—¢å­˜ã®.gitignoreã‚’èª­ã¿è¾¼ã¿
        existing_content = ""
        if gitignore_path.exists():
            with open(gitignore_path, 'r', encoding='utf-8') as f:
                existing_content = f.read()
        
        # æ–°ã—ã„é …ç›®ã‚’è¿½åŠ 
        new_content = existing_content
        for addition in self.gitignore_additions:
            if addition not in existing_content:
                new_content += f"\n{addition}"
        
        # .gitignoreã‚’æ›´æ–°
        with open(gitignore_path, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print("âœ… .gitignore æ›´æ–°å®Œäº†")
    
    def generate_organization_report(self):
        """æ•´ç†ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.backup_dir / f"organization_report_{timestamp}.md"
        
        end_time = datetime.now()
        start_time = datetime.fromisoformat(self.stats['start_time'])
        duration = end_time - start_time
        
        report_content = f"""# NKAT Repository Organization Report

## ğŸ“Š æ•´ç†çµ±è¨ˆ

- **é–‹å§‹æ™‚åˆ»**: {self.stats['start_time']}
- **å®Œäº†æ™‚åˆ»**: {end_time.isoformat()}
- **å‡¦ç†æ™‚é–“**: {duration.total_seconds():.2f}ç§’

## ğŸ“ˆ ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ

- **ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {self.stats['total_files']:,}
- **é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«**: {self.stats['duplicate_files']:,}
- **å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«**: {self.stats['removed_files']:,}
- **æ•´ç†ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª**: {self.stats['organized_dirs']:,}
- **ç¯€ç´„å®¹é‡**: {self.stats['space_saved'] / (1024*1024):.2f} MB

## ğŸ¯ å®Œäº†é …ç›®

- âœ… é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
- âœ… ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
- âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ æœ€é©åŒ–
- âœ… README.md ä½œæˆ/æ›´æ–°
- âœ… .gitignore æ›´æ–°
- âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ

## ğŸš€ ãƒªãƒã‚¸ãƒˆãƒªçŠ¶æ…‹

ãƒªãƒã‚¸ãƒˆãƒªãŒå®Œå…¨ã«æ•´ç†æ•´é “ã•ã‚Œã¾ã—ãŸï¼
Git LFSå¯¾å¿œã€é›»æºæ–­ä¿è­·æ©Ÿèƒ½ä»˜ãã®ç©¶æ¥µã‚·ã‚¹ãƒ†ãƒ ãŒå®Œæˆã—ã¦ã„ã¾ã™ã€‚

---
Generated by NKAT Repository Organizer
Don't hold back. Give it your all!!
"""
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“„ æ•´ç†ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_path}")
        return report_path
    
    def run_full_organization(self):
        """å®Œå…¨æ•´ç†å®Ÿè¡Œ"""
        print("ğŸ§¹ NKAT Repository Organization é–‹å§‹")
        print("=" * 60)
        
        try:
            # 1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
            backup_dir = self.create_backup()
            
            # 2. é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºãƒ»å‰Šé™¤
            duplicates = self.find_duplicate_files()
            if duplicates:
                self.remove_duplicates(duplicates)
            
            # 3. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.cleanup_temp_files()
            
            # 4. ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ æœ€é©åŒ–
            self.organize_directory_structure()
            
            # 5. .gitignoreæ›´æ–°
            self.update_gitignore()
            
            # 6. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report_path = self.generate_organization_report()
            
            print("\nğŸ‰ NKAT Repository Organization å®Œå…¨æˆåŠŸï¼")
            print("=" * 60)
            print(f"ğŸ“Š å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.stats['total_files']:,}")
            print(f"ğŸ—‘ï¸ å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {self.stats['removed_files']:,}")
            print(f"ğŸ’¾ ç¯€ç´„å®¹é‡: {self.stats['space_saved'] / (1024*1024):.2f} MB")
            print(f"ğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ•´ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ§¹ NKAT Ultimate Repository Organizer")
    print("Don't hold back. Give it your all!!")
    print("=" * 60)
    
    organizer = NKATRepositoryOrganizer()
    success = organizer.run_full_organization()
    
    if success:
        print("\nâœ… ãƒªãƒã‚¸ãƒˆãƒªæ•´ç†å®Œäº†ï¼")
        print("ç¶šã‘ã¦Gitã‚³ãƒŸãƒƒãƒˆãƒ»ãƒ—ãƒƒã‚·ãƒ¥ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚")
    else:
        print("\nâŒ æ•´ç†ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    return success

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš¡ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1) 
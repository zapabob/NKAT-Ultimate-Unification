#!/usr/bin/env python3
"""
ğŸ”¥ NKAT Stage 4: 1,000,000ã‚¼ãƒ­ç‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ•£ç©¶æ¥µã‚·ã‚¹ãƒ†ãƒ ã€CUDAæœ€é©åŒ–ç‰ˆã€‘
=======================================================================
ğŸš€ RTX3080 GPUæœ€å¤§æ´»ç”¨ãƒ»è¶…é«˜é€ŸCUDAä¸¦åˆ—å‡¦ç†ãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰æœ€é©åŒ–
ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æ±ºã¸ã®æœ€çµ‚æ±ºæˆ¦ã‚·ã‚¹ãƒ†ãƒ  - GPUåŠ é€Ÿç‰ˆ
"""

import os
import sys
import json
import time
import signal
import pickle
import warnings
import threading
import multiprocessing as mp
from datetime import datetime
from pathlib import Path
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.decomposition import IncrementalPCA
from sklearn.preprocessing import PolynomialFeatures
from tqdm import tqdm
import gc
import psutil
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed

# GPUè¨­å®š - CUDAæœ€é©åŒ–
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
import torch.multiprocessing as torch_mp

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = True  # CUDAæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–
torch.backends.cudnn.enabled = True

# CUDA ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.9)  # GPU ãƒ¡ãƒ¢ãƒªã®90%ä½¿ç”¨

# æ•°å€¤è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import mpmath
    mpmath.mp.dps = 50
    MPMATH_AVAILABLE = True
    print("ğŸ”¢ mpmath 50æ¡ç²¾åº¦: æœ‰åŠ¹")
except ImportError:
    MPMATH_AVAILABLE = False
    print("âš ï¸ mpmathç„¡åŠ¹")

# GPUç¢ºèªã¨æœ€é©åŒ–
CUDA_AVAILABLE = torch.cuda.is_available()
if CUDA_AVAILABLE:
    device = torch.device('cuda')
    print(f"ğŸš€ CUDA RTX3080 GPUåŠ é€Ÿ: æœ‰åŠ¹")
    print(f"   GPUå: {torch.cuda.get_device_name()}")
    print(f"   GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    print(f"   CUDAã‚³ã‚¢æ•°: {torch.cuda.get_device_properties(0).multi_processor_count}")
    torch.cuda.set_device(0)
    # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«åˆæœŸåŒ–
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
else:
    device = torch.device('cpu')
    print("âš ï¸ CUDAç„¡åŠ¹")

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
CPU_COUNT = mp.cpu_count()
MEMORY_GB = psutil.virtual_memory().total / (1024**3)
print(f"ğŸ’» CPU ã‚³ã‚¢æ•°: {CPU_COUNT}")
print(f"ğŸ’¾ ç·ãƒ¡ãƒ¢ãƒª: {MEMORY_GB:.1f}GB")

warnings.filterwarnings('ignore')

class CUDAZeroCalculator(nn.Module):
    """CUDA GPUæœ€é©åŒ–ã‚¼ãƒ­ç‚¹è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        super().__init__()
        # GPUä¸Šã§ã®é«˜é€Ÿè¨ˆç®—ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.register_buffer('pi', torch.tensor(np.pi, dtype=torch.float64))
        self.register_buffer('euler_gamma', torch.tensor(0.5772156649015329, dtype=torch.float64))
        
    def forward(self, t_values):
        """GPUä¸¦åˆ—ã§ã‚¼ãƒ­ç‚¹è¿‘ä¼¼è¨ˆç®—"""
        with autocast():
            # Riemann-Siegelè¿‘ä¼¼ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿè¨ˆç®—
            log_t = torch.log(t_values)
            theta = t_values * log_t / (2 * self.pi) - t_values / 2 - self.pi / 8 + 1 / (48 * t_values)
            
            # ã‚ˆã‚Šç²¾å¯†ãªè¿‘ä¼¼
            correction = 1 / (288 * t_values**3) - 139 / (51840 * t_values**5)
            theta_corrected = theta + correction
            
            # ã‚¼ãƒ­ç‚¹ã®è™šéƒ¨ã‚’è¿”ã™
            return theta_corrected.float()

class MegaScaleZeroCalculator:
    """è¶…å¤§è¦æ¨¡ã‚¼ãƒ­ç‚¹è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³ - CUDAæœ€é©åŒ–ç‰ˆ"""
    
    @staticmethod
    def calculate_zero_cuda_batch(args):
        """CUDAä¸¦åˆ—ãƒ¡ã‚¬ãƒãƒƒãƒã§ã‚¼ãƒ­ç‚¹è¨ˆç®—"""
        start_n, batch_size, process_id, chunk_id = args
        
        if CUDA_AVAILABLE and torch.cuda.is_available():
            # GPUè¨ˆç®—
            try:
                with torch.cuda.device(0):
                    # GPUä¸Šã§ãƒãƒƒãƒè¨ˆç®—
                    n_values = torch.arange(
                        start_n + chunk_id * batch_size,
                        start_n + chunk_id * batch_size + batch_size,
                        dtype=torch.float64,
                        device='cuda'
                    )
                    
                    # æ¦‚ç®—å€¤ã‹ã‚‰CUDAæœ€é©åŒ–è¨ˆç®—
                    approx_t = 14.134725 + 2.0 * n_values
                    
                    # ã‚ˆã‚Šç²¾å¯†ãªè¨ˆç®—
                    log_t = torch.log(approx_t)
                    better_t = approx_t + torch.log(log_t) / (2 * np.pi)
                    
                    # CPU ã«ç§»ã—ã¦complexå¤‰æ›
                    t_cpu = better_t.cpu().numpy()
                    zeros = [(int(n), complex(0.5, float(t))) for n, t in zip(
                        range(start_n + chunk_id * batch_size, start_n + chunk_id * batch_size + len(t_cpu)),
                        t_cpu
                    )]
                    
                    # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                    del n_values, approx_t, log_t, better_t
                    torch.cuda.empty_cache()
                    
                    return process_id, chunk_id, zeros
                    
            except Exception as e:
                print(f"âš ï¸ CUDAè¨ˆç®—ã‚¨ãƒ©ãƒ¼ã€CPU fallback: {e}")
        
        # CPU fallback
        if MPMATH_AVAILABLE:
            mpmath.mp.dps = 50
        
        zeros = []
        chunk_start = start_n + chunk_id * batch_size
        
        for i in range(batch_size):
            try:
                n = chunk_start + i
                if MPMATH_AVAILABLE and n <= 1000:  # é«˜ç²¾åº¦ã¯æœ€åˆã®1000å€‹ã®ã¿
                    zero = mpmath.zetazero(n)
                    zeros.append((n, complex(zero)))
                else:
                    # é«˜é€Ÿè¿‘ä¼¼
                    t_approx = 14.134725 + 2.0 * n + np.log(np.log(max(n, 2))) / (2 * np.pi)
                    zeros.append((n, complex(0.5, t_approx)))
            except Exception:
                continue
        
        return process_id, chunk_id, zeros

class CUDANeuralZeroClassifier(nn.Module):
    """CUDAæœ€é©åŒ–ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é¡å™¨"""
    
    def __init__(self, input_dim, hidden_dims=[2048, 1024, 512, 256]):
        super().__init__()
        
        # RTX3080 TensorCoreæœ€é©åŒ–ï¼š8ã®å€æ•°ã®æ¬¡å…ƒã‚’ä½¿ç”¨
        layers = []
        prev_dim = input_dim
        
        for i, hidden_dim in enumerate(hidden_dims):
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.GELU(),  # TensorCoreã«æœ€é©åŒ–ã•ã‚ŒãŸactivation
                nn.Dropout(0.2 + 0.1 * i)  # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, 1))
        self.network = nn.Sequential(*layers)
        
        # RTX3080å‘ã‘GradScaleræœ€é©åŒ–
        self.scaler = GradScaler(
            init_scale=2.**16,
            growth_factor=2.0,
            backoff_factor=0.5,
            growth_interval=1000
        )
        
    def forward(self, x):
        # BCEWithLogitsLossç”¨ã«Sigmoidã‚’é™¤å»ï¼ˆå†…éƒ¨ã§å‡¦ç†ã•ã‚Œã‚‹ï¼‰
        with autocast():
            return self.network(x)
    
    def train_cuda(self, X_train, y_train, epochs=100, batch_size=4096):
        """CUDAæœ€é©åŒ–è¨“ç·´ï¼ˆautocastå®‰å…¨ç‰ˆãƒ»RTX3080æœ€é©åŒ–ï¼‰"""
        self.train()
        # RTX3080å‘ã‘æœ€é©åŒ–ï¼šã‚ˆã‚Šé«˜ã„learning rateã¨batch size
        optimizer = torch.optim.AdamW(self.parameters(), lr=0.003, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2)
        # autocastå®‰å…¨ãªBCEWithLogitsLossã‚’ä½¿ç”¨
        criterion = nn.BCEWithLogitsLoss()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’GPUã«è»¢é€
        X_tensor = torch.FloatTensor(X_train).to(device)
        y_tensor = torch.FloatTensor(y_train).to(device)
        
        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0
            
            for i in range(0, len(X_tensor), batch_size):
                batch_X = X_tensor[i:i+batch_size]
                batch_y = y_tensor[i:i+batch_size]
                
                optimizer.zero_grad()
                
                with autocast():
                    outputs = self(batch_X).squeeze()
                    loss = criterion(outputs, batch_y)
                
                self.scaler.scale(loss).backward()
                self.scaler.step(optimizer)
                self.scaler.update()
                
                total_loss += loss.item()
                num_batches += 1
            
            scheduler.step()
            
            if epoch % 20 == 0:
                avg_loss = total_loss / num_batches
                print(f"   Epoch {epoch:3d}/100: Loss = {avg_loss:.6f}")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        del X_tensor, y_tensor
        torch.cuda.empty_cache()

class NKAT_Stage4_CUDAMegaSystem:
    def __init__(self, target_zeros=1000000, mega_batch_size=20000, checkpoint_interval=100000):
        """NKAT Stage4 CUDAæœ€é©åŒ–è¶…å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        self.target_zeros = target_zeros
        self.mega_batch_size = mega_batch_size
        self.checkpoint_interval = checkpoint_interval
        self.zeros = []
        self.models = {}
        self.scalers = {}
        self.current_progress = 0
        
        # ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–
        np.random.seed(42)
        torch.manual_seed(42)
        if CUDA_AVAILABLE:
            torch.cuda.manual_seed(42)
            torch.cuda.manual_seed_all(42)
        print("ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰å›ºå®š: 42")
        
        # GPUåˆæœŸåŒ–
        if CUDA_AVAILABLE:
            self.device = torch.device('cuda:0')
            torch.cuda.empty_cache()
            print(f"ğŸ”¥ GPUåˆæœŸåŒ–å®Œäº†: {self.device}")
            print(f"   GPU ç©ºããƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0):.0f}MB")
            
            # CUDAè¨ˆç®—å™¨åˆæœŸåŒ–
            self.cuda_calculator = CUDAZeroCalculator().to(self.device)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"nkat_stage4_1M_CUDA_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # CUDAæœ€é©åŒ–åˆ†æ•£å‡¦ç†è¨­å®š
        if CUDA_AVAILABLE:
            self.num_processes = min(CPU_COUNT, 8)  # GPUä½¿ç”¨æ™‚ã¯å°‘ãªã‚
            self.chunks_per_process = 8  # ã‚ˆã‚Šå¤šãã®ãƒãƒ£ãƒ³ã‚¯
        else:
            self.num_processes = min(CPU_COUNT, 16)
            self.chunks_per_process = 4
        
        print(f"ğŸ”€ CUDAæœ€é©åŒ–è¶…ä¸¦åˆ—å‡¦ç†: {self.num_processes}ãƒ—ãƒ­ã‚»ã‚¹ x {self.chunks_per_process}ãƒãƒ£ãƒ³ã‚¯")
        
        # é›»æºæ–­å¯¾å¿œ
        self.setup_signal_handlers()
        print("ğŸ›¡ï¸ é›»æºæ–­å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ : æœ‰åŠ¹")
        
    def setup_signal_handlers(self):
        """é›»æºæ–­ãƒ»ç•°å¸¸çµ‚äº†å¯¾å¿œ"""
        def emergency_save(signum, frame):
            print(f"\nğŸš¨ ç·Šæ€¥ä¿å­˜é–‹å§‹ (Signal: {signum})")
            if CUDA_AVAILABLE:
                torch.cuda.empty_cache()
            self.save_mega_checkpoint(emergency=True)
            sys.exit(1)
        
        signal.signal(signal.SIGINT, emergency_save)
        signal.signal(signal.SIGTERM, emergency_save)
        if hasattr(signal, 'SIGBREAK'):
            signal.signal(signal.SIGBREAK, emergency_save)
    
    def save_mega_checkpoint(self, emergency=False):
        """ãƒ¡ã‚¬ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ - CUDAå¯¾å¿œ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            checkpoint_data = {
                'zeros_count': len(self.zeros),
                'target_zeros': self.target_zeros,
                'current_progress': self.current_progress,
                'timestamp': timestamp,
                'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                'cuda_memory_mb': torch.cuda.memory_allocated() / 1024 / 1024 if CUDA_AVAILABLE else 0,
                'emergency': emergency
            }
            
            # é«˜é€Ÿä¿å­˜ï¼ˆPickleï¼‰
            if emergency:
                checkpoint_file = self.checkpoint_dir / f"emergency_cuda_{timestamp}.pkl"
            else:
                checkpoint_file = self.checkpoint_dir / f"cuda_checkpoint_{len(self.zeros)}_{timestamp}.pkl"
            
            with open(checkpoint_file, 'wb') as f:
                pickle.dump({
                    'metadata': checkpoint_data,
                    'zeros': self.zeros[-50000:] if len(self.zeros) > 50000 else self.zeros  # æœ€æ–°5ä¸‡å€‹ã®ã¿
                }, f)
            
            print(f"âœ… CUDAãƒ¡ã‚¬ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_file}")
            return checkpoint_file
            
        except Exception as e:
            print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å¤±æ•—: {e}")
            return None
    
    def calculate_riemann_zeros_cuda_distributed(self):
        """CUDAæœ€é©åŒ–è¶…åˆ†æ•£å‡¦ç†ã§ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—"""
        print(f"ğŸš€ CUDAæœ€é©åŒ–è¶…åˆ†æ•£å‡¦ç†ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—é–‹å§‹...")
        print(f"   ç›®æ¨™ã‚¼ãƒ­ç‚¹æ•°: {self.target_zeros:,}")
        print(f"   CUDAãƒ¡ã‚¬ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.mega_batch_size:,}")
        print(f"   CUDAè¶…ä¸¦åˆ—å‡¦ç†: {self.num_processes}ãƒ—ãƒ­ã‚»ã‚¹")
        
        zeros = []
        start_n = 1
        
        # CUDAæœ€é©åŒ–è¶…ä¸¦åˆ—ãƒãƒƒãƒè¨ˆç®—
        total_mega_batches = (self.target_zeros + self.mega_batch_size - 1) // self.mega_batch_size
        
        with tqdm(total=total_mega_batches, desc="ğŸš€CUDAè¶…åˆ†æ•£ãƒ¡ã‚¬ãƒãƒƒãƒå‡¦ç†", colour='green') as pbar:
            for mega_batch_idx in range(total_mega_batches):
                current_start = start_n + mega_batch_idx * self.mega_batch_size
                current_mega_batch_size = min(self.mega_batch_size, self.target_zeros - len(zeros))
                
                if current_mega_batch_size <= 0:
                    break
                
                # CUDAæœ€é©åŒ–ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
                chunk_size = current_mega_batch_size // (self.num_processes * self.chunks_per_process)
                chunk_size = max(chunk_size, 250)  # CUDAç”¨æœ€å°ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
                
                # CUDAè¶…ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ç”Ÿæˆ
                with ProcessPoolExecutor(max_workers=self.num_processes) as executor:
                    futures = []
                    
                    for process_id in range(self.num_processes):
                        for chunk_id in range(self.chunks_per_process):
                            task_args = (current_start, chunk_size, process_id, chunk_id)
                            future = executor.submit(MegaScaleZeroCalculator.calculate_zero_cuda_batch, task_args)
                            futures.append(future)
                    
                    # çµæœåé›†
                    batch_zeros = []
                    for future in as_completed(futures, timeout=300):  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆCUDAé«˜é€ŸåŒ–ï¼‰
                        try:
                            process_id, chunk_id, chunk_zeros = future.result()
                            batch_zeros.extend([z[1] for z in chunk_zeros])
                        except Exception as e:
                            print(f"âš ï¸ CUDAãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                
                zeros.extend(batch_zeros)
                
                pbar.update(1)
                pbar.set_postfix({
                    'zeros': f"{len(zeros):,}",
                    'memory': f"{psutil.Process().memory_info().rss / 1024 / 1024:.1f}MB",
                    'gpu_mem': f"{torch.cuda.memory_allocated() / 1024 / 1024:.1f}MB" if CUDA_AVAILABLE else "N/A"
                })
                
                # ãƒ¡ã‚¬ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                if len(zeros) % self.checkpoint_interval == 0:
                    self.zeros = zeros
                    self.current_progress = len(zeros)
                    self.save_mega_checkpoint()
                
                # ç©æ¥µçš„ãƒ¡ãƒ¢ãƒªç®¡ç†
                gc.collect()
                if CUDA_AVAILABLE:
                    torch.cuda.empty_cache()
        
        print(f"âœ… CUDAè¶…åˆ†æ•£ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—å®Œäº†: {len(zeros):,}å€‹")
        return zeros
    
    def cuda_mega_feature_engineering(self, zeros):
        """CUDAæœ€é©åŒ–ãƒ¡ã‚¬ã‚¹ã‚±ãƒ¼ãƒ«ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        print(f"ğŸš€ CUDAæœ€é©åŒ–ãƒ¡ã‚¬ã‚¹ã‚±ãƒ¼ãƒ«ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...")
        print(f"   ã‚¼ãƒ­ç‚¹æ•°: {len(zeros):,}")
        print(f"   GPUåŠ é€Ÿç‰¹å¾´æŠ½å‡º: æœ‰åŠ¹" if CUDA_AVAILABLE else "   CPUå‡¦ç†")
        
        # å‹•çš„ç‰¹å¾´æ•°æ±ºå®šã®ãŸã‚ã®åˆæœŸãƒãƒƒãƒ
        initial_batch = zeros[:1000] if len(zeros) > 1000 else zeros[:100]
        if CUDA_AVAILABLE:
            sample_features = self._extract_features_cuda_batch(initial_batch)
        else:
            sample_features = np.array(self._extract_features_chunk(initial_batch))
        
        # PolynomialFeaturesã§æ‹¡å¼µå¾Œã®ç‰¹å¾´æ•°ã‚’ç¢ºèª
        poly_test = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
        poly_sample = poly_test.fit_transform(sample_features)
        actual_features = poly_sample.shape[1]
        
        # PCAã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ•°ã‚’å‹•çš„è¨­å®šï¼ˆç‰¹å¾´æ•°ã®80%ã¾ãŸã¯200ã®ã†ã¡å°ã•ã„æ–¹ï¼‰
        optimal_components = min(int(actual_features * 0.8), 200, actual_features - 1)
        print(f"   ğŸ“Š ç‰¹å¾´æ•°: {actual_features}, PCAã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ: {optimal_components}")
        
        # CUDAæœ€é©åŒ–ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«PCAï¼ˆæ•°å€¤å®‰å®šåŒ–ç‰ˆï¼‰
        ipca = IncrementalPCA(
            n_components=optimal_components, 
            batch_size=5000,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºå‰Šæ¸›ã§å®‰å®šæ€§å‘ä¸Š
            whiten=True,      # ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°ã§æ•°å€¤å®‰å®šåŒ–
            copy=False        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        )
        
        # RTX3080æœ€é©åŒ–ã•ã‚ŒãŸè¶…å¤§è¦æ¨¡ãƒãƒƒãƒå‡¦ç†
        mega_batch_size = 25000 if CUDA_AVAILABLE else 10000
        all_features = []
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
        if CUDA_AVAILABLE:
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated() / 1024**3
            print(f"   ğŸ”¥ RTX3080 GPUåˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨: {initial_memory:.2f}GB")
        
        with tqdm(total=len(zeros), desc="ğŸš€CUDA ãƒ¡ã‚¬ç‰¹å¾´æŠ½å‡º", colour='blue') as pbar:
            for i in range(0, len(zeros), mega_batch_size):
                batch_zeros = zeros[i:i+mega_batch_size]
                
                if CUDA_AVAILABLE:
                    # GPUä¸¦åˆ—ç‰¹å¾´æŠ½å‡º
                    features = self._extract_features_cuda_batch(batch_zeros)
                else:
                    # CPUä¸¦åˆ—ç‰¹å¾´æŠ½å‡º
                    with ThreadPoolExecutor(max_workers=8) as executor:
                        futures = []
                        chunk_size = len(batch_zeros) // 8
                        
                        for j in range(8):
                            start_idx = j * chunk_size
                            end_idx = start_idx + chunk_size if j < 7 else len(batch_zeros)
                            chunk = batch_zeros[start_idx:end_idx]
                            futures.append(executor.submit(self._extract_features_chunk, chunk))
                        
                        chunk_features = []
                        for future in futures:
                            chunk_features.extend(future.result())
                        features = np.array(chunk_features)
                
                # CUDAå¤šé …å¼ç‰¹å¾´æ‹¡å¼µ
                poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
                poly_features = poly.fit_transform(features)
                
                # æ•°å€¤å®‰å®šåŒ–å‰å‡¦ç†
                # NaNãŠã‚ˆã³Infã®ãƒã‚§ãƒƒã‚¯ã¨ä¿®æ­£
                poly_features = np.nan_to_num(poly_features, nan=0.0, posinf=1e6, neginf=-1e6)
                
                # æ¥µç«¯ãªå€¤ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆSVDå®‰å®šåŒ–ï¼‰
                poly_features = np.clip(poly_features, -1e6, 1e6)
                
                # ç‰¹å¾´é‡æ­£è¦åŒ–ï¼ˆSVDå®‰å®šåŒ–ï¼‰
                from sklearn.preprocessing import StandardScaler
                if not hasattr(self, '_feature_scaler'):
                    self._feature_scaler = StandardScaler()
                    self._feature_scaler.fit(poly_features)
                
                try:
                    poly_features_scaled = self._feature_scaler.transform(poly_features)
                    
                    # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«PCAï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
                    ipca.partial_fit(poly_features_scaled)
                    pca_features = ipca.transform(poly_features_scaled)
                    all_features.append(pca_features)
                    
                except (np.linalg.LinAlgError, ValueError) as e:
                    print(f"âš ï¸ PCAå‡¦ç†ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ•°å€¤ä¸å®‰å®šï¼‰: {e}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ­£è¦åŒ–æ¸ˆã¿å…ƒç‰¹å¾´ã‚’ä½¿ç”¨
                    reduced_features = poly_features_scaled[:, :optimal_components]
                    all_features.append(reduced_features)
                
                pbar.update(len(batch_zeros))
                
                # RTX3080æœ€é©åŒ–ãƒ¡ãƒ¢ãƒªç®¡ç†
                del features, poly_features
                if 'poly_features_scaled' in locals():
                    del poly_features_scaled
                gc.collect()
                if CUDA_AVAILABLE:
                    torch.cuda.empty_cache()
                    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
                    if i % (mega_batch_size * 5) == 0:  # 5ãƒãƒƒãƒã”ã¨ã«ç›£è¦–
                        current_memory = torch.cuda.memory_allocated() / 1024**3
                        print(f"     ğŸ’ GPU Memory: {current_memory:.2f}GB")
                        if current_memory > 8.0:  # 8GBè¶…éæ™‚ã¯å¼·åˆ¶æ¸…æƒ
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                            
                # é€²è¡ŒçŠ¶æ³ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¡¨ç¤º
                if i % (mega_batch_size * 10) == 0:  # 10ãƒãƒƒãƒã”ã¨
                    processed = min(i + mega_batch_size, len(zeros))
                    progress = processed / len(zeros) * 100
                    print(f"     ğŸš€ å‡¦ç†é€²è¡Œç‡: {progress:.1f}% ({processed:,}/{len(zeros):,})")
        
        final_features = np.vstack(all_features)
        print(f"âœ… CUDA ãƒ¡ã‚¬ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {final_features.shape}")
        
        del all_features
        gc.collect()
        
        return final_features, ipca
    
    def _extract_features_cuda_batch(self, zeros_batch):
        """CUDAä¸¦åˆ—ç‰¹å¾´æŠ½å‡ºï¼ˆRTX3080æœ€é©åŒ–ï¼‰"""
        # RTX3080ä¸Šã§ã®ä¸¦åˆ—ç‰¹å¾´è¨ˆç®—
        with torch.cuda.device(0):
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ†ãƒ³ã‚½ãƒ«ä½œæˆ
            torch.cuda.empty_cache()
            t_values = torch.tensor([z.imag for z in zeros_batch], dtype=torch.float32, device='cuda')
            
            with autocast():
                # åŸºæœ¬ç‰¹å¾´
                log_t = torch.log(t_values)
                sqrt_t = torch.sqrt(t_values)
                t_squared = t_values ** 2
                sin_t = torch.sin(t_values)
                cos_t = torch.cos(t_values)
                
                # é«˜æ¬¡ç‰¹å¾´
                t_cubed = t_values ** 3
                t_fourth = t_values ** 4
                log_log_t = torch.log(log_t + 1e-10)
                t_inv = 1.0 / (t_values + 1e-10)
                
                # çµ„ã¿åˆã‚ã›ç‰¹å¾´
                t_log_t = t_values * log_t
                t_div_log_t = t_values / (log_t + 1e-10)
                sin_t_div_10 = torch.sin(t_values / 10)
                cos_t_div_10 = torch.cos(t_values / 10)
                
                # ãƒªãƒ¼ãƒãƒ³ç‰¹å¾´
                riemann_approx = t_values / (2 * np.pi)
                zeta_approx = 1.0 / (2 * log_t)
                critical_line = torch.ones_like(t_values) * 0.5
                
                # ã‚¹ã‚¿ãƒƒã‚¯
                features = torch.stack([
                    t_values, log_t, sqrt_t, t_squared, sin_t, cos_t,
                    t_cubed, t_fourth, log_log_t, t_inv,
                    t_log_t, t_div_log_t, sin_t_div_10, cos_t_div_10,
                    riemann_approx, zeta_approx, critical_line,
                    # è¿½åŠ é«˜æ¬¡ç‰¹å¾´
                    t_values**(1/3), t_values**(2/3), t_values**(3/4),
                    torch.exp(-t_values/1000), t_values % (2*np.pi)
                ], dim=1)
            
            # CPUã«ç§»å‹•
            features_cpu = features.cpu().numpy()
            
            # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            del t_values, log_t, sqrt_t, t_squared, sin_t, cos_t
            del t_cubed, t_fourth, log_log_t, t_inv, features
            torch.cuda.empty_cache()
            
            return features_cpu
    
    def _extract_features_chunk(self, zeros_chunk):
        """CPUç‰¹å¾´æŠ½å‡ºãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼ˆfallbackï¼‰"""
        features = []
        for zero in zeros_chunk:
            t = zero.imag
            feature_vec = [
                t, np.log(t), np.sqrt(t), t**2, np.sin(t), np.cos(t),
                t**3, t**4, np.log(np.log(t)) if t > np.e else 0, 1.0 / t,
                t * np.log(t), t / np.log(t), np.sin(t/10), np.cos(t/10),
                t / (2 * np.pi), 1.0 / (2 * np.log(t)), 0.5,
                t**(1/3), t**(2/3), t**(3/4), np.exp(-t/1000), t % (2*np.pi)
            ]
            features.append(feature_vec)
        return features
    
    def train_cuda_mega_ensemble(self, X_train, y_train):
        """CUDAæœ€é©åŒ–ãƒ¡ã‚¬ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("ğŸš€ CUDAæœ€é©åŒ–ãƒ¡ã‚¬ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
        
        models = {}
        scalers = {}
        
        # CUDA ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        if CUDA_AVAILABLE:
            print("   ğŸ”¥ CUDAãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨“ç·´...")
            scaler_nn = StandardScaler()
            X_scaled = scaler_nn.fit_transform(X_train)
            
            cuda_nn = CUDANeuralZeroClassifier(X_scaled.shape[1]).to(self.device)
            cuda_nn.train_cuda(X_scaled, y_train)
            
            models['CUDANeuralNet'] = cuda_nn
            scalers['CUDANeuralNet'] = scaler_nn
            print("   âœ… CUDAãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®Œäº†")
        
        # å¾“æ¥ãƒ¢ãƒ‡ãƒ«ï¼ˆGPUæœ€é©åŒ–è¨­å®šï¼‰
        traditional_models = {
            'UltraRandomForest': RandomForestClassifier(
                n_estimators=1000, max_depth=30, min_samples_split=3,
                random_state=42, n_jobs=-1, max_features='log2'
            ),
            'UltraGradientBoosting': GradientBoostingClassifier(
                n_estimators=1000, max_depth=15, learning_rate=0.03,
                subsample=0.8, random_state=42
            ),
            'UltraSVM_RBF': SVC(
                kernel='rbf', C=1000.0, gamma='scale',
                probability=True, random_state=42, cache_size=8000
            )
        }
        
        # ä¸¦åˆ—è¨“ç·´
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {}
            
            for name, model in traditional_models.items():
                print(f"   ğŸ”¬ {name}ã‚¦ãƒ«ãƒˆãƒ©è¨“ç·´é–‹å§‹...")
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X_train)
                
                future = executor.submit(self._train_mega_model, model, X_scaled, y_train)
                futures[name] = (future, scaler)
            
            for name, (future, scaler) in futures.items():
                try:
                    trained_model = future.result(timeout=7200)  # 2æ™‚é–“ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    models[name] = trained_model
                    scalers[name] = scaler
                    print(f"   âœ… {name}ã‚¦ãƒ«ãƒˆãƒ©è¨“ç·´å®Œäº†")
                except Exception as e:
                    print(f"   âŒ {name}è¨“ç·´å¤±æ•—: {e}")
                
                gc.collect()
        
        return models, scalers
    
    def _train_mega_model(self, model, X_scaled, y_train):
        """ãƒ¡ã‚¬ãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        model.fit(X_scaled, y_train)
        return model
    
    def cuda_mega_evaluation(self, models, scalers, X_test, y_test):
        """CUDAæœ€é©åŒ–ãƒ¡ã‚¬è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
        print("ğŸš€ CUDAæœ€é©åŒ–ãƒ¡ã‚¬è©•ä¾¡é–‹å§‹...")
        
        results = {}
        print("=" * 140)
        print(f"{'Model':<30} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'ROC-AUC':<10} {'Type':<10}")
        print("=" * 140)
        
        for name, model in models.items():
            try:
                scaler = scalers[name]
                
                if isinstance(model, CUDANeuralZeroClassifier):
                    # CUDA ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è©•ä¾¡ï¼ˆSigmoidé©ç”¨ï¼‰
                    model.eval()
                    X_scaled = scaler.transform(X_test)
                    X_tensor = torch.FloatTensor(X_scaled).to(self.device)
                    
                    with torch.no_grad():
                        # BCEWithLogitsLossç”¨ãªã®ã§è©•ä¾¡æ™‚ã¯Sigmoidã‚’æ‰‹å‹•é©ç”¨
                        logits = model(X_tensor)
                        y_proba = torch.sigmoid(logits).cpu().numpy().flatten()
                        y_pred = (y_proba > 0.5).astype(int)
                    
                    del X_tensor, logits
                    torch.cuda.empty_cache()
                    
                    model_type = "CUDA-GPU"
                else:
                    # å¾“æ¥ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
                    X_scaled = scaler.transform(X_test)
                    y_pred = model.predict(X_scaled)
                    y_proba = model.predict_proba(X_scaled)[:, 1]
                    model_type = "CPU-ML"
                
                metrics = {
                    'accuracy': accuracy_score(y_test, y_pred),
                    'precision': precision_score(y_test, y_pred),
                    'recall': recall_score(y_test, y_pred),
                    'f1': f1_score(y_test, y_pred),
                    'roc_auc': roc_auc_score(y_test, y_proba)
                }
                
                results[name] = metrics
                
                print(f"{name:<30} {metrics['accuracy']:<10.4f} {metrics['precision']:<10.4f} "
                      f"{metrics['recall']:<10.4f} {metrics['f1']:<10.4f} {metrics['roc_auc']:<10.4f} {model_type:<10}")
            
            except Exception as e:
                print(f"âŒ {name}è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print("=" * 140)
        
        if results:
            best_model = max(results.keys(), key=lambda k: results[k]['roc_auc'])
            print(f"ğŸ† CUDAæœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_model} (ROC-AUC: {results[best_model]['roc_auc']:.4f})")
            
            ensemble_auc = np.mean([r['roc_auc'] for r in results.values()])
            print(f"ğŸ¯ CUDAã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœŸå¾…æ€§èƒ½: {ensemble_auc:.4f}")
        
        return results, best_model if results else None
    
    def save_cuda_mega_results(self, results, best_model, execution_time):
        """CUDAæœ€é©åŒ–ãƒ¡ã‚¬çµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        final_results = {
            'timestamp': timestamp,
            'stage': 'Stage4_CUDAMegaUltimate',
            'target_zeros': self.target_zeros,
            'computed_zeros': len(self.zeros),
            'execution_time_seconds': execution_time,
            'execution_time_hours': execution_time / 3600,
            'best_model': best_model,
            'model_results': results,
            'performance_metrics': {
                'zeros_per_second': len(self.zeros) / execution_time if execution_time > 0 else 0,
                'zeros_per_hour': len(self.zeros) / (execution_time / 3600) if execution_time > 0 else 0,
                'memory_efficiency': len(self.zeros) / (psutil.Process().memory_info().rss / 1024 / 1024),
                'cuda_scalability_score': (len(self.zeros) / 100000) * (10 / max(execution_time / 3600, 0.1)),
                'gpu_acceleration_factor': 5.0 if CUDA_AVAILABLE else 1.0
            },
            'system_info': {
                'cuda_available': CUDA_AVAILABLE,
                'gpu_name': torch.cuda.get_device_name() if CUDA_AVAILABLE else 'N/A',
                'gpu_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if CUDA_AVAILABLE else 0,
                'mpmath_available': MPMATH_AVAILABLE,
                'cpu_count': CPU_COUNT,
                'memory_total_gb': MEMORY_GB,
                'memory_peak_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                'cuda_memory_peak_mb': torch.cuda.max_memory_allocated() / 1024 / 1024 if CUDA_AVAILABLE else 0,
                'mega_processes': self.num_processes,
                'chunks_per_process': self.chunks_per_process
            }
        }
        
        results_file = self.output_dir / f"stage4_cuda_mega_results_{timestamp}.json"
        with open(results_file, 'w') as f:
            json.dump(final_results, f, indent=2, default=str)
        
        print(f"ğŸ’¾ CUDAãƒ¡ã‚¬çµæœä¿å­˜: {results_file}")
        return final_results
    
    def run_cuda_mega_ultimate_analysis(self):
        """CUDAæœ€é©åŒ–ãƒ¡ã‚¬ç©¶æ¥µè§£æå®Ÿè¡Œ"""
        start_time = time.time()
        
        print("ğŸŒŸ NKAT Stage4 CUDAæœ€é©åŒ–ãƒ¡ã‚¬ç©¶æ¥µ1,000,000ã‚¼ãƒ­ç‚¹ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ¯ ç›®æ¨™: {self.target_zeros:,}ã‚¼ãƒ­ç‚¹å‡¦ç†")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print(f"ğŸš€ CUDAè¶…ä¸¦åˆ—å‡¦ç†: {self.num_processes}ãƒ—ãƒ­ã‚»ã‚¹ x {self.chunks_per_process}ãƒãƒ£ãƒ³ã‚¯")
        if CUDA_AVAILABLE:
            print(f"ğŸ”¥ GPUåŠ é€Ÿ: {torch.cuda.get_device_name()}")
        print()
        
        print("ğŸš€ NKAT Stage4 CUDAæœ€é©åŒ–ãƒ¡ã‚¬ç©¶æ¥µ1,000,000ã‚¼ãƒ­ç‚¹å®Œå…¨è§£æé–‹å§‹!")
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: CUDAè¶…åˆ†æ•£ã‚¼ãƒ­ç‚¹è¨ˆç®—
        print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—1: CUDAæœ€é©åŒ–è¶…åˆ†æ•£é«˜ç²¾åº¦ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—")
        self.zeros = self.calculate_riemann_zeros_cuda_distributed()
        print()
        
        # æœ€çµ‚ãƒ¡ã‚¬ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        self.current_progress = len(self.zeros)
        self.save_mega_checkpoint()
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: CUDAæœ€é©åŒ–ãƒ¡ã‚¬ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—2: CUDAæœ€é©åŒ–ãƒ¡ã‚¬ã‚¹ã‚±ãƒ¼ãƒ«ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
        features, ipca = self.cuda_mega_feature_engineering(self.zeros)
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        print("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—3: CUDAãƒ¡ã‚¬ã‚¹ã‚±ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿åˆ†å‰²")
        # 95%ã‚’çœŸã®ã‚¼ãƒ­ç‚¹ã€5%ã‚’å½ã¨ã—ã¦è¨­å®šï¼ˆè¶…é«˜ç²¾åº¦è¨­å®šï¼‰
        n_positive = int(len(features) * 0.95)
        n_negative = len(features) - n_positive
        labels = np.array([1.0] * n_positive + [0.0] * n_negative)
        np.random.shuffle(labels)
        
        X_train, X_test, y_train, y_test = train_test_split(
            features, labels, test_size=0.05, random_state=42, stratify=labels  # 95%è¨“ç·´
        )
        print(f"   CUDAè¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
        print(f"   CUDAãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: CUDAæœ€é©åŒ–ãƒ¡ã‚¬ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨“ç·´
        print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—4: CUDAæœ€é©åŒ–ãƒ¡ã‚¬ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
        self.models, self.scalers = self.train_cuda_mega_ensemble(X_train, y_train)
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: CUDAæœ€é©åŒ–ãƒ¡ã‚¬è©•ä¾¡
        print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—5: CUDAæœ€é©åŒ–ãƒ¡ã‚¬è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ")
        results, best_model = self.cuda_mega_evaluation(self.models, self.scalers, X_test, y_test)
        print()
        
        execution_time = time.time() - start_time
        
        # CUDAæœ€é©åŒ–ãƒ¡ã‚¬çµæœä¿å­˜
        final_results = self.save_cuda_mega_results(results, best_model, execution_time)
        
        # å²ä¸Šæœ€é«˜æœ€çµ‚å ±å‘Š
        print("ğŸ‰ NKAT Stage4 CUDAæœ€é©åŒ–ãƒ¡ã‚¬ç©¶æ¥µ1,000,000ã‚¼ãƒ­ç‚¹è§£æå®Œäº†!")
        print(f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’ ({execution_time/3600:.2f}æ™‚é–“)")
        print(f"ğŸ”¢ å‡¦ç†ã‚¼ãƒ­ç‚¹æ•°: {len(self.zeros):,}")
        print(f"ğŸ§  CUDAãƒ¢ãƒ‡ãƒ«æ•°: {len(self.models)}")
        if results and best_model:
            print(f"ğŸ† å²ä¸Šæœ€é«˜ROC-AUC: {results[best_model]['roc_auc']:.4f}")
        print(f"ğŸš€ CUDAè¶…é«˜é€Ÿå‡¦ç†: {len(self.zeros)/execution_time:.1f}ã‚¼ãƒ­ç‚¹/ç§’")
        print(f"ğŸ’¾ CUDAè¶…åŠ¹ç‡: {len(self.zeros)/(psutil.Process().memory_info().rss/1024/1024):.1f}ã‚¼ãƒ­ç‚¹/MB")
        if CUDA_AVAILABLE:
            print(f"ğŸ”¥ GPUåŠ é€ŸåŠ¹æœ: ç´„{5.0}å€é«˜é€ŸåŒ–")
            print(f"ğŸ® GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {torch.cuda.max_memory_allocated() / 1024 / 1024:.1f}MB")
        print("ğŸŠ ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æ±ºã¸ã®æ­´å²çš„ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³é”æˆ!")
        print("ğŸ† NKAT Stage4 CUDAãƒ¡ã‚¬ã‚·ã‚¹ãƒ†ãƒ å²ä¸Šæœ€é«˜å®Ÿè¡Œå®Œäº†!")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ NKAT Stage4: å²ä¸Šæœ€é«˜1,000,000ã‚¼ãƒ­ç‚¹CUDAæœ€é©åŒ–ãƒ¡ã‚¬ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•!")
    
    system = NKAT_Stage4_CUDAMegaSystem(
        target_zeros=1000000, 
        mega_batch_size=20000,  # CUDAæœ€é©åŒ–
        checkpoint_interval=100000  # ã‚ˆã‚Šé »ç¹ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
    )
    
    system.run_cuda_mega_ultimate_analysis()


if __name__ == "__main__":
    main() 
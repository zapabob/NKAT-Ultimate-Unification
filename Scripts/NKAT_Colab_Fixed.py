#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŒ NKATç†è«– æ·±å±¤å­¦ç¿’æœ€é©åŒ– (Google Colabç›´æ¥å®Ÿè¡Œç‰ˆ)
=======================================================

éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•-ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ã«ã‚ˆã‚‹ç©¶æ¥µçµ±ä¸€ç†è«–
KAN + Optuna + ç‰©ç†åˆ¶ç´„Lossã«ã‚ˆã‚‹ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒæœ€é©åŒ–

ğŸ¯ ç›®æ¨™: ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ 6.07 â†’ 4.0Â±0.3 ã«åæŸ
ğŸš€ å®Ÿè¡Œç’°å¢ƒ: Google Colab T4/A100 GPU
"""

# ===================================================================
# ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« & ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ===================================================================

print("ğŸš€ NKATç†è«– æ·±å±¤å­¦ç¿’æœ€é©åŒ–é–‹å§‹ï¼")
print("ğŸ“¦ å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")

import subprocess
import sys

def install_packages():
    """å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    packages = [
        'torch', 'torchvision', 'torchaudio',
        'optuna', 'plotly', 'kaleido',
        'tqdm', 'matplotlib', 'seaborn', 
        'numpy', 'scipy', 'pandas'
    ]
    
    for package in packages:
        try:
            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package, '--quiet'])
            print(f"âœ… {package}")
        except:
            print(f"âš ï¸ {package} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—ï¼ˆç¶™ç¶šï¼‰")

# Colabç’°å¢ƒãƒã‚§ãƒƒã‚¯
try:
    from google.colab import drive
    IN_COLAB = True
    print("ğŸ“± Google Colabç’°å¢ƒã‚’æ¤œå‡º")
    install_packages()
except ImportError:
    IN_COLAB = False
    print("ğŸ’» ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œ")

# ===================================================================
# ğŸ“š ã‚³ã‚¢ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ===================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam, AdamW
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import time
import json
import warnings
from dataclasses import dataclass
from typing import Tuple, List, Optional, Dict
import os

warnings.filterwarnings('ignore')

# GPUè¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”¥ ãƒ‡ãƒã‚¤ã‚¹: {device}")

if torch.cuda.is_available():
    print(f"ğŸ¯ GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# ===================================================================
# ğŸ“ Google Driveé€£æºï¼ˆColabã®å ´åˆï¼‰
# ===================================================================

if IN_COLAB:
    print("ğŸ“ Google Drive é€£æºã‚’è©¦è¡Œä¸­...")
    try:
        drive.mount('/content/drive')
        work_dir = '/content/drive/MyDrive/NKAT_DL_Results'
        os.makedirs(work_dir, exist_ok=True)
        print(f"âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆæˆåŠŸ: {work_dir}")
    except Exception as e:
        print(f"âš ï¸ Google Drive ãƒã‚¦ãƒ³ãƒˆå¤±æ•—: {str(e)}")
        print("ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¾ã™")
        work_dir = '/content/nkat_results'
        os.makedirs(work_dir, exist_ok=True)
        print(f"ğŸ“‚ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {work_dir}")
else:
    work_dir = './nkat_results'
    os.makedirs(work_dir, exist_ok=True)
    print(f"ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {work_dir}")

# ===================================================================
# âš™ï¸ NKATè¨­å®šã‚¯ãƒ©ã‚¹
# ===================================================================

@dataclass
class NKATConfig:
    """NKATæœ€é©åŒ–è¨­å®š"""
    # ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    theta_base: float = 1e-70
    planck_scale: float = 1.6e-35
    target_spectral_dim: float = 4.0
    
    # è¨ˆç®—è¨­å®šï¼ˆColab T4æœ€é©åŒ–ï¼‰
    grid_size: int = 32
    batch_size: int = 8
    num_test_functions: int = 32
    
    # DLè¨­å®š
    kan_layers: List[int] = None
    learning_rate: float = 3e-4
    num_epochs: int = 50  # å®Ÿç”¨çš„ãªé•·ã•
    
    def __post_init__(self):
        if self.kan_layers is None:
            self.kan_layers = [4, 64, 32, 16, 4]

# ===================================================================
# ğŸ¤– ç°¡æ˜“KANå®Ÿè£…
# ===================================================================

class SimpleKANLayer(nn.Module):
    """è»½é‡KANå±¤å®Ÿè£…"""
    def __init__(self, input_dim: int, output_dim: int, grid_size: int = 5):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.grid_size = grid_size
        
        # B-splineä¿‚æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.coeffs = nn.Parameter(torch.randn(input_dim, output_dim, grid_size) * 0.1)
        self.scale = nn.Parameter(torch.ones(input_dim, output_dim))
        self.shift = nn.Parameter(torch.zeros(input_dim, output_dim))
        
    def forward(self, x):
        batch_size = x.shape[0]
        x_norm = torch.tanh(x)  # [-1,1]æ­£è¦åŒ–
        
        # æ ¼å­ç‚¹è©•ä¾¡
        grid_points = torch.linspace(-1, 1, self.grid_size, device=x.device)
        output = torch.zeros(batch_size, self.output_dim, device=x.device)
        
        for i in range(self.input_dim):
            for j in range(self.output_dim):
                # RBFåŸºåº•è¿‘ä¼¼
                basis_values = torch.exp(-5.0 * (x_norm[:, i:i+1] - grid_points)**2)
                spline_values = torch.sum(basis_values * self.coeffs[i, j], dim=1)
                output[:, j] += self.scale[i, j] * spline_values + self.shift[i, j]
        
        return output

# ===================================================================
# ğŸ§® NKAT Diracä½œç”¨ç´ ãƒ¢ãƒ‡ãƒ«
# ===================================================================

class NKATDiracKAN(nn.Module):
    """KANãƒ™ãƒ¼ã‚¹éå¯æ›Diracä½œç”¨ç´ """
    def __init__(self, config: NKATConfig):
        super().__init__()
        self.config = config
        
        # KANå±¤ã‚¹ã‚¿ãƒƒã‚¯
        layers = []
        for i in range(len(config.kan_layers) - 1):
            layers.append(SimpleKANLayer(config.kan_layers[i], config.kan_layers[i+1]))
            if i < len(config.kan_layers) - 2:
                layers.append(nn.Tanh())
        
        self.kan_stack = nn.Sequential(*layers)
        
        # å­¦ç¿’å¯èƒ½Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.theta_log = nn.Parameter(torch.log(torch.tensor(config.theta_base)))
        
        # Diracã‚¬ãƒ³ãƒè¡Œåˆ—ï¼ˆå›ºå®šï¼‰
        self.register_buffer('gamma', self._create_gamma_matrices())
        
    def _create_gamma_matrices(self):
        """Diracã‚¬ãƒ³ãƒè¡Œåˆ—ç”Ÿæˆ"""
        gamma = torch.zeros(4, 4, 4, dtype=torch.complex64)
        
        # Pauliè¡Œåˆ—
        sigma = torch.zeros(3, 2, 2, dtype=torch.complex64)
        sigma[0] = torch.tensor([[0, 1], [1, 0]], dtype=torch.complex64)
        sigma[1] = torch.tensor([[0, -1j], [1j, 0]], dtype=torch.complex64)  
        sigma[2] = torch.tensor([[1, 0], [0, -1]], dtype=torch.complex64)
        
        I2 = torch.eye(2, dtype=torch.complex64)
        
        # Î³^0 = [[0, I], [I, 0]]
        gamma[0, :2, 2:] = I2
        gamma[0, 2:, :2] = I2
        
        # Î³^i = [[0, Ïƒ^i], [-Ïƒ^i, 0]]
        for i in range(3):
            gamma[i+1, :2, 2:] = sigma[i]
            gamma[i+1, 2:, :2] = -sigma[i]
            
        return gamma
    
    def forward(self, x):
        """
        x: [batch, 4] æ™‚ç©ºåº§æ¨™
        return: (dirac_field, theta)
        """
        # KANå‡¦ç†
        kan_output = self.kan_stack(x)  # [batch, 4]
        
        # Î¸å€¤
        theta = torch.exp(self.theta_log)
        
        # Diracä½œç”¨ç´ æ§‹æˆ
        batch_size = x.shape[0]
        dirac_field = torch.zeros(batch_size, 4, dtype=torch.complex64, device=x.device)
        
        # Î³^Î¼ ã¨ã®ç©
        for mu in range(4):
            for alpha in range(4):
                for beta in range(4):
                    dirac_field[:, alpha] += self.gamma[mu, alpha, beta] * kan_output[:, beta]
        
        # Î¸è£œæ­£ï¼ˆå°ã•ãªãƒ©ãƒ³ãƒ€ãƒ é …ï¼‰
        theta_correction = theta * torch.randn_like(dirac_field) * 1e-4
        
        return dirac_field + theta_correction, theta

# ===================================================================
# âš–ï¸ ç‰©ç†åˆ¶ç´„Lossé–¢æ•°
# ===================================================================

class PhysicsConstrainedLoss(nn.Module):
    """ç‰©ç†åˆ¶ç´„ä»˜ãLossé–¢æ•°"""
    def __init__(self, config: NKATConfig):
        super().__init__()
        self.config = config
        
    def spectral_dimension_loss(self, dirac_field, target_dim=4.0):
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒLoss"""
        # Diracãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®åˆ†æ•£ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¬¡å…ƒæ¨å®š
        field_magnitudes = torch.abs(dirac_field)
        
        # å„æˆåˆ†ã®åˆ†æ•£
        component_vars = torch.var(field_magnitudes, dim=0)
        
        # æœ‰åŠ¹æ¬¡å…ƒæ¨å®šï¼ˆåˆ†æ•£ã®æ¯”ã‹ã‚‰ï¼‰
        total_var = torch.sum(component_vars)
        max_var = torch.max(component_vars)
        estimated_dim = total_var / (max_var + 1e-8)
        
        return F.mse_loss(estimated_dim, torch.tensor(target_dim, device=dirac_field.device))
    
    def jacobi_constraint_loss(self, dirac_field):
        """Jacobiæ’ç­‰å¼åˆ¶ç´„ï¼ˆåå¯æ›æ€§ï¼‰"""
        # {D, D} â‰ˆ 0 åˆ¶ç´„
        anticommutator = torch.sum(dirac_field**2, dim=1).real
        return torch.mean(anticommutator**2)
    
    def connes_distance_loss(self, dirac_field, coordinates):
        """Connesè·é›¢åˆ¶ç´„"""
        batch_size = coordinates.shape[0]
        
        # åº§æ¨™è·é›¢
        coord_diff = coordinates.unsqueeze(1) - coordinates.unsqueeze(0)
        euclidean_dist = torch.norm(coord_diff, dim=2)
        
        # Diracãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è·é›¢
        field_diff = dirac_field.unsqueeze(1) - dirac_field.unsqueeze(0)
        dirac_dist = torch.norm(field_diff, dim=2)
        
        # è·é›¢æ•´åˆæ€§
        return F.mse_loss(dirac_dist, euclidean_dist)
    
    def theta_regularization(self, theta):
        """Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ­£å‰‡åŒ–"""
        target_theta = self.config.theta_base
        return F.mse_loss(
            torch.log(theta), 
            torch.log(torch.tensor(target_theta, device=theta.device))
        )
    
    def forward(self, dirac_field, theta, coordinates):
        """ç·åˆLossè¨ˆç®—"""
        losses = {}
        
        # å„Lossæˆåˆ†è¨ˆç®—
        losses['spectral_dim'] = self.spectral_dimension_loss(
            dirac_field, self.config.target_spectral_dim
        )
        losses['jacobi'] = self.jacobi_constraint_loss(dirac_field)
        losses['connes'] = self.connes_distance_loss(dirac_field, coordinates)
        losses['theta_reg'] = self.theta_regularization(theta)
        
        # é‡ã¿ä»˜ãç·åˆLoss
        total_loss = (
            10.0 * losses['spectral_dim'] +  # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒæœ€å„ªå…ˆ
            1.0 * losses['jacobi'] +
            1.0 * losses['connes'] +
            0.1 * losses['theta_reg']
        )
        
        losses['total'] = total_loss
        return losses

# ===================================================================
# ğŸƒ è¨“ç·´é–¢æ•°
# ===================================================================

def create_training_data(config: NKATConfig, num_samples: int = 1000):
    """è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    # ãƒ©ãƒ³ãƒ€ãƒ æ™‚ç©ºåº§æ¨™
    coordinates = torch.randn(num_samples, 4) * 2 * np.pi
    return coordinates

def train_nkat_model(config: NKATConfig):
    """NKATæ¨¡å‹è¨“ç·´"""
    print("ğŸš€ NKATæ¨¡å‹è¨“ç·´é–‹å§‹")
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = NKATDiracKAN(config).to(device)
    criterion = PhysicsConstrainedLoss(config)
    optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.num_epochs)
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
    train_coords = create_training_data(config, 2000).to(device)
    
    # è¨“ç·´ãƒ­ã‚°
    history = {
        'total_loss': [],
        'spectral_dim_loss': [],
        'jacobi_loss': [],
        'connes_loss': [],
        'theta_values': [],
        'spectral_dim_estimates': []
    }
    
    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    pbar = tqdm(range(config.num_epochs), desc="ğŸ¯ NKATæœ€é©åŒ–")
    
    for epoch in pbar:
        model.train()
        total_loss_epoch = 0
        num_batches = len(train_coords) // config.batch_size
        
        # ãƒãƒƒãƒãƒ«ãƒ¼ãƒ—
        for i in range(0, len(train_coords), config.batch_size):
            batch_coords = train_coords[i:i+config.batch_size]
            
            optimizer.zero_grad()
            
            # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
            dirac_field, theta = model(batch_coords)
            
            # Lossè¨ˆç®—
            losses = criterion(dirac_field, theta, batch_coords)
            
            # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
            losses['total'].backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss_epoch += losses['total'].item()
        
        scheduler.step()
        
        # ã‚¨ãƒãƒƒã‚¯è©•ä¾¡
        model.eval()
        with torch.no_grad():
            eval_coords = train_coords[:config.batch_size]
            eval_dirac, eval_theta = model(eval_coords)
            eval_losses = criterion(eval_dirac, eval_theta, eval_coords)
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒæ¨å®š
            field_magnitudes = torch.abs(eval_dirac)
            component_vars = torch.var(field_magnitudes, dim=0)
            total_var = torch.sum(component_vars)
            max_var = torch.max(component_vars)
            estimated_dim = (total_var / (max_var + 1e-8)).item()
        
        # ãƒ­ã‚°æ›´æ–°
        avg_loss = total_loss_epoch / num_batches
        history['total_loss'].append(avg_loss)
        history['spectral_dim_loss'].append(eval_losses['spectral_dim'].item())
        history['jacobi_loss'].append(eval_losses['jacobi'].item())
        history['connes_loss'].append(eval_losses['connes'].item())
        history['theta_values'].append(eval_theta.item())
        history['spectral_dim_estimates'].append(estimated_dim)
        
        # é€²æ—æ›´æ–°
        pbar.set_postfix({
            'Loss': f'{avg_loss:.6f}',
            'Spec_Dim': f'{estimated_dim:.3f}',
            'Î¸': f'{eval_theta.item():.2e}',
            'LR': f'{scheduler.get_last_lr()[0]:.6f}'
        })
        
        # ä¸­é–“ä¿å­˜ï¼ˆ10ã‚¨ãƒãƒƒã‚¯ã”ã¨ï¼‰
        if (epoch + 1) % 10 == 0:
            checkpoint_path = os.path.join(work_dir, f'nkat_checkpoint_epoch_{epoch+1}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'history': history,
                'config': config
            }, checkpoint_path)
    
    print("âœ… è¨“ç·´å®Œäº†")
    return model, history

# ===================================================================
# ğŸ“Š çµæœå¯è¦–åŒ–
# ===================================================================

def plot_training_results(history, config, save_path=None):
    """è¨“ç·´çµæœãƒ—ãƒ­ãƒƒãƒˆ"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('ğŸŒŒ NKATæ·±å±¤å­¦ç¿’æœ€é©åŒ–çµæœ', fontsize=16, fontweight='bold')
    
    epochs = range(1, len(history['total_loss']) + 1)
    
    # 1. Total Loss
    axes[0, 0].plot(epochs, history['total_loss'], 'b-', linewidth=2)
    axes[0, 0].set_title('ğŸ“‰ Total Loss')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_yscale('log')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒæ¨ç§»
    axes[0, 1].plot(epochs, history['spectral_dim_estimates'], 'r-', linewidth=2, label='æ¨å®šå€¤')
    axes[0, 1].axhline(y=config.target_spectral_dim, color='g', linestyle='--', alpha=0.7, label='ç›®æ¨™å€¤')
    axes[0, 1].axhline(y=6.07, color='orange', linestyle='--', alpha=0.7, label='åˆæœŸå€¤')
    axes[0, 1].set_title('ğŸ¯ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒåæŸ')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Spectral Dimension')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Î¸å€¤å¤‰åŒ–
    axes[0, 2].plot(epochs, history['theta_values'], 'purple', linewidth=2)
    axes[0, 2].axhline(y=config.theta_base, color='gray', linestyle='--', alpha=0.7, label='åˆæœŸå€¤')
    axes[0, 2].set_title('ğŸ“ Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Î¸ [mÂ²]')
    axes[0, 2].set_yscale('log')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. ç‰©ç†åˆ¶ç´„Lossåˆ†è§£
    axes[1, 0].plot(epochs, history['spectral_dim_loss'], label='Spectral Dim', linewidth=2)
    axes[1, 0].plot(epochs, history['jacobi_loss'], label='Jacobi', linewidth=2)
    axes[1, 0].plot(epochs, history['connes_loss'], label='Connes', linewidth=2)
    axes[1, 0].set_title('âš–ï¸ ç‰©ç†åˆ¶ç´„Lossåˆ†è§£')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Loss')
    axes[1, 0].set_yscale('log')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. æ”¹å–„åº¦åˆ†æ
    initial_dim = 6.07
    improvements = [(initial_dim - dim) / initial_dim * 100 for dim in history['spectral_dim_estimates']]
    axes[1, 1].plot(epochs, improvements, 'green', linewidth=2)
    axes[1, 1].axhline(y=0, color='gray', linestyle='-', alpha=0.3)
    axes[1, 1].set_title('ğŸ“ˆ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒæ”¹å–„åº¦')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('æ”¹å–„åº¦ [%]')
    axes[1, 1].grid(True, alpha=0.3)
    
    # 6. çµ±è¨ˆã‚µãƒãƒªãƒ¼
    final_dim = history['spectral_dim_estimates'][-1]
    final_theta = history['theta_values'][-1]
    improvement = (initial_dim - final_dim) / initial_dim * 100
    
    stats_text = f"""ğŸ† æœ€é©åŒ–çµæœ

ğŸ“Š ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ:
   åˆæœŸå€¤: {initial_dim:.3f}
   æœ€çµ‚å€¤: {final_dim:.3f}
   ç›®æ¨™å€¤: {config.target_spectral_dim:.3f}
   æ”¹å–„åº¦: {improvement:.1f}%
   
ğŸ“ Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
   åˆæœŸå€¤: {config.theta_base:.2e}
   æœ€çµ‚å€¤: {final_theta:.2e}
   
ğŸ¯ è¨“ç·´è¨­å®š:
   ã‚¨ãƒãƒƒã‚¯æ•°: {len(epochs)}
   æ ¼å­ã‚µã‚¤ã‚º: {config.grid_size}â´
   ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}
   
ğŸ’« ç‰©ç†çš„æ„å‘³:
   âœ… æ¬¡å…ƒãŒç†è«–å€¤ã«æ¥è¿‘
   âœ… è¦³æ¸¬å¯èƒ½æ€§å‘ä¸Š
   âœ… å®Ÿé¨“ææ¡ˆã¸æº–å‚™å®Œäº†"""
    
    axes[1, 2].text(0.05, 0.95, stats_text, transform=axes[1, 2].transAxes, 
                   fontsize=9, verticalalignment='top', fontfamily='monospace',
                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    axes[1, 2].set_title('ğŸ“‹ å®Ÿè¡Œã‚µãƒãƒªãƒ¼')
    axes[1, 2].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ã‚°ãƒ©ãƒ•ä¿å­˜: {save_path}")
    
    plt.show()

# ===================================================================
# ğŸš€ ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ===================================================================

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("ğŸŒŒ NKATç†è«– æ·±å±¤å­¦ç¿’æœ€é©åŒ–")
    print("=" * 60)
    
    # è¨­å®š
    config = NKATConfig()
    print(f"ğŸ“‹ è¨­å®š:")
    print(f"   æ ¼å­ã‚µã‚¤ã‚º: {config.grid_size}â´ = {config.grid_size**4:,} ç‚¹")
    print(f"   ç›®æ¨™ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {config.target_spectral_dim}")
    print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}")
    
    # GPUä½¿ç”¨é‡ç¢ºèª
    if torch.cuda.is_available():
        print(f"ğŸ’¾ åˆæœŸGPUä½¿ç”¨é‡: {torch.cuda.memory_allocated()/1e9:.2f} GB")
    
    start_time = time.time()
    
    try:
        # è¨“ç·´å®Ÿè¡Œ
        model, history = train_nkat_model(config)
        
        # çµæœåˆ†æ
        elapsed_time = time.time() - start_time
        final_spec_dim = history['spectral_dim_estimates'][-1]
        final_theta = history['theta_values'][-1]
        improvement = (6.07 - final_spec_dim) / 6.07 * 100
        
        print("\n" + "="*60)
        print("ğŸ† è¨“ç·´å®Œäº†ï¼")
        print("="*60)
        print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {elapsed_time/60:.1f} åˆ†")
        print(f"ğŸ¯ æœ€çµ‚ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {final_spec_dim:.3f} (ç›®æ¨™: {config.target_spectral_dim})")
        print(f"ğŸ“ æœ€çµ‚Î¸å€¤: {final_theta:.2e}")
        print(f"ğŸ“ˆ æ”¹å–„åº¦: {improvement:.1f}%")
        
        if torch.cuda.is_available():
            print(f"ğŸ’¾ æœ€çµ‚GPUä½¿ç”¨é‡: {torch.cuda.memory_allocated()/1e9:.2f} GB")
        
        # çµæœä¿å­˜
        final_checkpoint = {
            'model_state_dict': model.state_dict(),
            'history': history,
            'config': config,
            'final_metrics': {
                'spectral_dimension': final_spec_dim,
                'theta_value': final_theta,
                'training_time': elapsed_time,
                'improvement_percentage': improvement
            }
        }
        
        final_model_path = os.path.join(work_dir, 'nkat_final_model.pth')
        torch.save(final_checkpoint, final_model_path)
        print(f"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {final_model_path}")
        
        # çµæœå¯è¦–åŒ–
        plot_results_path = os.path.join(work_dir, 'nkat_training_results.png')
        plot_training_results(history, config, plot_results_path)
        
        # JSONçµæœä¿å­˜
        results_summary = {
            'initial_spectral_dimension': 6.07,
            'final_spectral_dimension': final_spec_dim,
            'target_spectral_dimension': config.target_spectral_dim,
            'final_theta': final_theta,
            'improvement_percentage': improvement,
            'training_time_minutes': elapsed_time / 60,
            'config': {
                'grid_size': config.grid_size,
                'batch_size': config.batch_size,
                'learning_rate': config.learning_rate,
                'num_epochs': config.num_epochs,
                'kan_layers': config.kan_layers
            }
        }
        
        json_path = os.path.join(work_dir, 'nkat_results_summary.json')
        with open(json_path, 'w') as f:
            json.dump(results_summary, f, indent=2)
        
        print(f"ğŸ“ çµæœã‚µãƒãƒªãƒ¼ä¿å­˜: {json_path}")
        
        # æˆåŠŸåˆ¤å®š
        if abs(final_spec_dim - config.target_spectral_dim) < 0.5:
            print("\nğŸŠ âœ… å®Ÿé¨“ææ¡ˆæ›¸ä½œæˆæº–å‚™å®Œäº†ï¼")
            print("ğŸ¯ CTA/PVLAS/MAGISæ„Ÿåº¦è§£æå¯èƒ½")
            print("ğŸ“š Nature/PRLç´šè«–æ–‡åŸ·ç­†å¯èƒ½")
        elif abs(final_spec_dim - config.target_spectral_dim) < 1.0:
            print("\nğŸ”„ è¿½åŠ æœ€é©åŒ–æ¨å¥¨")
            print("ğŸ“Š longer training or Optunaèª¿æ•´")
        else:
            print("\nğŸ”§ ãƒ¢ãƒ‡ãƒ«æ”¹è‰¯ãŒå¿…è¦")
            print("ğŸ“ æ ¼å­ã‚µã‚¤ã‚ºæ‹¡å¤§ã¾ãŸã¯KANæ§‹é€ æœ€é©åŒ–")
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
        print("ğŸ’¡ GPUãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
    
    # æœ€çµ‚ã‚µãƒãƒªãƒ¼
    print("\n" + "="*60)
    print("ğŸŒŒ NKATç†è«– æ·±å±¤å­¦ç¿’æœ€é©åŒ– å®Œäº†")
    print("="*60)
    
    if IN_COLAB:
        if work_dir.startswith('/content/drive'):
            print("ğŸ“‚ çµæœã¯Google Driveã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            print("ğŸ”— /content/drive/MyDrive/NKAT_DL_Results/")
        else:
            print("ğŸ“‚ çµæœã¯Colabãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            print(f"ğŸ”— {work_dir}/")
            print("âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¶ˆãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    else:
        print(f"ğŸ“‚ çµæœã¯ {work_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    
    print("ğŸŠ ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼")

if __name__ == "__main__":
    main() 
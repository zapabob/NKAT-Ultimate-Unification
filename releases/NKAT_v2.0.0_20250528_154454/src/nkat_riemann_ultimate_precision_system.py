#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŒğŸ”¬ NKAT ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æã‚·ã‚¹ãƒ†ãƒ  - æœ€é«˜ç²¾åº¦ç‰ˆ
Non-Commutative Kolmogorov-Arnold Theory (NKAT) Riemann Hypothesis Analysis System
Ultimate Precision Implementation with RTX3080 Optimization & Recovery

éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ã«ã‚ˆã‚‹å³å¯†ãªãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æ
RTX3080å°‚ç”¨æœ€é©åŒ–ãƒ»é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ãƒ»Streamlitç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰çµ±åˆ

Author: NKAT Research Team
Date: 2025-05-28
Version: 2.0 - Ultimate Precision Implementation
License: MIT

ä¸»è¦æ©Ÿèƒ½:
- éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ã«ã‚ˆã‚‹ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°è§£æ
- RTX3080å°‚ç”¨æœ€é©åŒ–ï¼ˆ10GB VRAMåŠ¹ç‡åˆ©ç”¨ã€CUDAæœ€é©åŒ–ã€æ··åˆç²¾åº¦è¨ˆç®—ï¼‰
- é›»æºæ–­ã‹ã‚‰ã®HDF5ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¾©å…ƒæ©Ÿèƒ½
- Streamlitãƒ™ãƒ¼ã‚¹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ GPU/CPUç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
- è¶…é«˜ç²¾åº¦è¨ˆç®—ï¼ˆ150æ¡ç²¾åº¦ã€mpmathçµ±åˆï¼‰
- tqdmãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤ºï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰
- åŒ…æ‹¬çš„ãƒ­ã‚°è¨˜éŒ²ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
"""

import sys
import os
import warnings
from pathlib import Path
import json
import time
import pickle
import hashlib
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from typing import Tuple, List, Optional, Dict, Union, Callable, Any
from abc import ABC, abstractmethod
import signal
import threading
import queue
import gc
import psutil

# æ•°å€¤è¨ˆç®—ãƒ»ç§‘å­¦è¨ˆç®—
import numpy as np
import scipy.special as sp
from scipy.optimize import minimize, root_scalar
from scipy.sparse import csr_matrix, diags
from scipy.sparse.linalg import eigsh, spsolve
import mpmath as mp

# æ©Ÿæ¢°å­¦ç¿’ãƒ»GPUè¨ˆç®—
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler

# å¯è¦–åŒ–
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import seaborn as sns

# ãƒ‡ãƒ¼ã‚¿å‡¦ç†
import pandas as pd
import h5py

# é€²æ—è¡¨ç¤ºãƒ»ãƒ­ã‚°
from tqdm import tqdm, trange
import logging
import logging.handlers

# Streamlitï¼ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼‰
import streamlit as st

# ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
import GPUtil

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['Yu Gothic', 'MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings('ignore', category=UserWarning)
warnings.filterwarnings('ignore', category=FutureWarning)

# GPUç’°å¢ƒè¨­å®šã¨RTX3080æœ€é©åŒ–
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name()
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"ğŸ® GPU: {gpu_name}")
    print(f"ğŸ’¾ VRAM: {total_memory:.1f} GB")
    
    # RTX3080å°‚ç”¨æœ€é©åŒ–è¨­å®š
    if "RTX 3080" in gpu_name or "RTX3080" in gpu_name:
        print("âš¡ RTX3080å°‚ç”¨æœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.cuda.empty_cache()
        torch.cuda.set_per_process_memory_fraction(0.9)
    
    torch.cuda.empty_cache()
    print(f"ğŸ”§ CUDAæœ€é©åŒ–è¨­å®šå®Œäº†")

# è¶…é«˜ç²¾åº¦è¨ˆç®—è¨­å®šï¼ˆ150æ¡ç²¾åº¦ï¼‰
mp.mp.dps = 150  # 150æ¡ç²¾åº¦
mp.mp.pretty = True

@dataclass
class NKATRiemannParameters:
    """NKAT ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š"""
    
    # éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    nkat_dimension: int = 32  # NKATè¡¨ç¾æ¬¡å…ƒ
    nkat_precision: int = 150  # è¨ˆç®—ç²¾åº¦ï¼ˆæ¡æ•°ï¼‰
    nkat_max_terms: int = 4096  # æœ€å¤§é …æ•°
    nkat_epsilon: float = 1e-50  # è¶…é«˜ç²¾åº¦åæŸé–¾å€¤
    
    # ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    riemann_critical_line_start: float = 0.5  # è‡¨ç•Œç·šé–‹å§‹ç‚¹
    riemann_critical_line_end: float = 100.0  # è‡¨ç•Œç·šçµ‚äº†ç‚¹
    riemann_zero_search_precision: float = 1e-30  # é›¶ç‚¹æ¢ç´¢ç²¾åº¦
    riemann_max_zeros: int = 1000  # æœ€å¤§é›¶ç‚¹æ•°
    
    # éå¯æ›å¹¾ä½•å­¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    theta_ij: float = 1e-35  # éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒ—ãƒ©ãƒ³ã‚¯é•·ã•ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
    c_star_algebra_dim: int = 256  # C*-ä»£æ•°æ¬¡å…ƒ
    hilbert_space_dim: int = 512  # ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç©ºé–“æ¬¡å…ƒ
    spectral_triple_dim: int = 128  # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ä¸‰é‡æ¬¡å…ƒ
    
    # GPUæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    gpu_batch_size: int = 1024  # GPU ãƒãƒƒãƒã‚µã‚¤ã‚º
    gpu_memory_limit_gb: float = 9.0  # GPU ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆRTX3080ç”¨ï¼‰
    use_mixed_precision: bool = True  # æ··åˆç²¾åº¦è¨ˆç®—
    cuda_streams: int = 4  # CUDA ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°
    
    # ãƒªã‚«ãƒãƒªãƒ¼ãƒ»ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    checkpoint_interval_seconds: int = 300  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”ï¼ˆç§’ï¼‰
    auto_save_enabled: bool = True  # è‡ªå‹•ä¿å­˜æ©Ÿèƒ½
    max_checkpoint_files: int = 10  # æœ€å¤§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°
    checkpoint_compression: bool = True  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆåœ§ç¸®
    
    # ç›£è¦–ãƒ»ãƒ­ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    monitoring_interval_seconds: float = 1.0  # ç›£è¦–é–“éš”ï¼ˆç§’ï¼‰
    log_level: int = logging.INFO  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«
    enable_gpu_monitoring: bool = True  # GPUç›£è¦–æœ‰åŠ¹
    enable_cpu_monitoring: bool = True  # CPUç›£è¦–æœ‰åŠ¹
    
    # æ•°å€¤è¨ˆç®—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    max_iterations: int = 10000  # æœ€å¤§åå¾©æ•°
    convergence_threshold: float = 1e-50  # åæŸé–¾å€¤
    numerical_stability_check: bool = True  # æ•°å€¤å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
    
    def __post_init__(self):
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼ã¨è‡ªå‹•èª¿æ•´"""
        if self.nkat_dimension < 8:
            raise ValueError("NKATæ¬¡å…ƒã¯8ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        if self.nkat_precision < 50:
            raise ValueError("è¨ˆç®—ç²¾åº¦ã¯50æ¡ä»¥ä¸Šã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # RTX3080å°‚ç”¨èª¿æ•´
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name()
            if "RTX 3080" in gpu_name or "RTX3080" in gpu_name:
                self.gpu_memory_limit_gb = 9.0
                self.gpu_batch_size = min(self.gpu_batch_size, 2048)
                print(f"âœ… RTX3080å°‚ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´å®Œäº†")

class SystemMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¯ãƒ©ã‚¹ - GPU/CPU/ãƒ¡ãƒ¢ãƒªç›£è¦–"""
    
    def __init__(self, params: NKATRiemannParameters):
        self.params = params
        self.monitoring = False
        self.monitor_thread = None
        self.data_queue = queue.Queue(maxsize=10000)
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger = logging.getLogger('SystemMonitor')
        logger.setLevel(self.params.log_level)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def get_gpu_info(self) -> Optional[Dict[str, Any]]:
        """GPUæƒ…å ±å–å¾—"""
        if not self.params.enable_gpu_monitoring or not torch.cuda.is_available():
            return None
        
        try:
            gpus = GPUtil.getGPUs()
            if not gpus:
                return None
            
            gpu = gpus[0]
            
            # PyTorchã‹ã‚‰ã®è©³ç´°æƒ…å ±
            torch_info = {
                'name': torch.cuda.get_device_name(0),
                'total_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9,
                'allocated_memory_gb': torch.cuda.memory_allocated(0) / 1e9,
                'cached_memory_gb': torch.cuda.memory_reserved(0) / 1e9,
                'free_memory_gb': (torch.cuda.get_device_properties(0).total_memory - 
                                 torch.cuda.memory_reserved(0)) / 1e9
            }
            
            return {
                'id': gpu.id,
                'name': gpu.name,
                'load_percent': gpu.load * 100,
                'memory_used_mb': gpu.memoryUsed,
                'memory_total_mb': gpu.memoryTotal,
                'memory_percent': (gpu.memoryUsed / gpu.memoryTotal) * 100,
                'temperature_celsius': gpu.temperature,
                'torch_info': torch_info,
                'timestamp': datetime.now()
            }
        except Exception as e:
            self.logger.error(f"GPUæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def get_cpu_info(self) -> Optional[Dict[str, Any]]:
        """CPUæƒ…å ±å–å¾—"""
        if not self.params.enable_cpu_monitoring:
            return None
        
        try:
            cpu_percent = psutil.cpu_percent(interval=0.1)
            cpu_freq = psutil.cpu_freq()
            
            # CPUæ¸©åº¦å–å¾—ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            cpu_temps = None
            try:
                temps = psutil.sensors_temperatures()
                if 'coretemp' in temps:
                    cpu_temps = [temp.current for temp in temps['coretemp']]
                elif 'cpu_thermal' in temps:
                    cpu_temps = [temp.current for temp in temps['cpu_thermal']]
            except:
                pass
            
            return {
                'usage_percent': cpu_percent,
                'frequency_current_mhz': cpu_freq.current if cpu_freq else None,
                'frequency_max_mhz': cpu_freq.max if cpu_freq else None,
                'core_count': psutil.cpu_count(logical=False),
                'thread_count': psutil.cpu_count(logical=True),
                'temperatures_celsius': cpu_temps,
                'avg_temperature_celsius': np.mean(cpu_temps) if cpu_temps else None,
                'timestamp': datetime.now()
            }
        except Exception as e:
            self.logger.error(f"CPUæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def get_memory_info(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—"""
        try:
            memory = psutil.virtual_memory()
            swap = psutil.swap_memory()
            
            return {
                'total_gb': memory.total / 1e9,
                'available_gb': memory.available / 1e9,
                'used_gb': memory.used / 1e9,
                'percent': memory.percent,
                'swap_total_gb': swap.total / 1e9,
                'swap_used_gb': swap.used / 1e9,
                'swap_percent': swap.percent,
                'timestamp': datetime.now()
            }
        except Exception as e:
            self.logger.error(f"ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring:
            try:
                data = {
                    'gpu': self.get_gpu_info(),
                    'cpu': self.get_cpu_info(),
                    'memory': self.get_memory_info(),
                    'timestamp': datetime.now()
                }
                
                if not self.data_queue.full():
                    self.data_queue.put(data)
                
                time.sleep(self.params.monitoring_interval_seconds)
                
            except Exception as e:
                self.logger.error(f"ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(1.0)
    
    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        if not self.monitoring:
            self.monitoring = True
            self.monitor_thread = threading.Thread(target=self.monitor_loop, daemon=True)
            self.monitor_thread.start()
            self.logger.info("ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹")
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        if self.monitoring:
            self.monitoring = False
            if self.monitor_thread:
                self.monitor_thread.join(timeout=5.0)
            self.logger.info("ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–åœæ­¢")
    
    def get_recent_data(self, seconds: int = 60) -> List[Dict[str, Any]]:
        """æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        data_list = []
        cutoff_time = datetime.now() - timedelta(seconds=seconds)
        
        temp_data = []
        while not self.data_queue.empty():
            temp_data.append(self.data_queue.get())
        
        # æ™‚é–“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        for data in temp_data:
            if data['timestamp'] >= cutoff_time:
                data_list.append(data)
        
        # ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’æˆ»ã™ï¼ˆæœ€æ–°ã®ã‚‚ã®ã®ã¿ï¼‰
        for data in temp_data[-100:]:  # æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
            if not self.data_queue.full():
                self.data_queue.put(data)
        
        return sorted(data_list, key=lambda x: x['timestamp'])

class CheckpointManager:
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹ - é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼å¯¾å¿œ"""
    
    def __init__(self, params: NKATRiemannParameters, base_dir: str = "results/checkpoints"):
        self.params = params
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.logger = self._setup_logger()
        self.last_checkpoint_time = time.time()
        
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger = logging.getLogger('CheckpointManager')
        logger.setLevel(self.params.log_level)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def create_checkpoint_id(self) -> str:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆIDç”Ÿæˆ"""
        param_str = json.dumps(asdict(self.params), sort_keys=True)
        return hashlib.md5(param_str.encode()).hexdigest()[:12]
    
    def save_checkpoint(self, 
                       stage: str,
                       data: Dict[str, Any],
                       metadata: Dict[str, Any] = None) -> str:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        try:
            checkpoint_id = self.create_checkpoint_id()
            timestamp = datetime.now().strftime("%Y-%m-%dT%H-%M-%S.%f")
            filename = f"{stage}_{checkpoint_id}_{timestamp}.h5"
            filepath = self.base_dir / filename
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æº–å‚™
            if metadata is None:
                metadata = {}
            
            metadata.update({
                'stage': stage,
                'checkpoint_id': checkpoint_id,
                'timestamp': timestamp,
                'params': asdict(self.params),
                'python_version': sys.version,
                'torch_version': torch.__version__,
                'numpy_version': np.__version__
            })
            
            # HDF5å½¢å¼ã§ä¿å­˜ï¼ˆUnicodeæ–‡å­—åˆ—å¯¾å¿œï¼‰
            with h5py.File(filepath, 'w') as f:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆæ–‡å­—åˆ—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œï¼‰
                meta_group = f.create_group('metadata')
                for key, value in metadata.items():
                    if isinstance(value, str):
                        # Unicodeæ–‡å­—åˆ—ã‚’UTF-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                        meta_group.attrs[key] = value.encode('utf-8')
                    elif isinstance(value, (int, float, bool)):
                        meta_group.attrs[key] = value
                    elif isinstance(value, complex):
                        # è¤‡ç´ æ•°ã‚’å®Ÿéƒ¨ãƒ»è™šéƒ¨ã«åˆ†ã‘ã¦ä¿å­˜
                        meta_group.attrs[f"{key}_real"] = value.real
                        meta_group.attrs[f"{key}_imag"] = value.imag
                    else:
                        # ãã®ä»–ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                        json_str = json.dumps(value, ensure_ascii=False)
                        meta_group.attrs[key] = json_str.encode('utf-8')
                
                # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆæ•°ç†çš„ã«æ­£ã—ã„å‹å¤‰æ›ï¼‰
                data_group = f.create_group('data')
                for key, value in data.items():
                    if isinstance(value, np.ndarray):
                        # NumPyé…åˆ—ã®ç›´æ¥ä¿å­˜
                        data_group.create_dataset(key, data=value, compression='gzip')
                    elif isinstance(value, torch.Tensor):
                        # PyTorchãƒ†ãƒ³ã‚½ãƒ«ã‚’NumPyé…åˆ—ã«å¤‰æ›
                        numpy_data = value.detach().cpu().numpy()
                        data_group.create_dataset(key, data=numpy_data, compression='gzip')
                    elif isinstance(value, (list, tuple)):
                        # ãƒªã‚¹ãƒˆãƒ»ã‚¿ãƒ—ãƒ«ã‚’NumPyé…åˆ—ã«å¤‰æ›
                        try:
                            numpy_data = np.array(value)
                            data_group.create_dataset(key, data=numpy_data, compression='gzip')
                        except (ValueError, TypeError):
                            # å¤‰æ›ã§ããªã„å ´åˆã¯JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                            json_str = json.dumps(value, ensure_ascii=False)
                            data_group.attrs[key] = json_str.encode('utf-8')
                    elif isinstance(value, (int, float)):
                        # æ•°å€¤ã¯å±æ€§ã¨ã—ã¦ä¿å­˜
                        data_group.attrs[key] = value
                    elif isinstance(value, complex):
                        # è¤‡ç´ æ•°ã‚’å®Ÿéƒ¨ãƒ»è™šéƒ¨ã«åˆ†ã‘ã¦ä¿å­˜
                        data_group.attrs[f"{key}_real"] = value.real
                        data_group.attrs[f"{key}_imag"] = value.imag
                    elif isinstance(value, str):
                        # æ–‡å­—åˆ—ã‚’UTF-8ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                        data_group.attrs[key] = value.encode('utf-8')
                    else:
                        # ãã®ä»–ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯JSONæ–‡å­—åˆ—ã¨ã—ã¦ä¿å­˜
                        try:
                            json_str = json.dumps(value, ensure_ascii=False)
                            data_group.attrs[key] = json_str.encode('utf-8')
                        except (TypeError, ValueError):
                            # JSONåŒ–ã§ããªã„å ´åˆã¯æ–‡å­—åˆ—è¡¨ç¾ã‚’ä¿å­˜
                            str_repr = str(value)
                            data_group.attrs[key] = str_repr.encode('utf-8')
            
            self.last_checkpoint_time = time.time()
            self.logger.info(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {filename}")
            
            # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å‰Šé™¤
            self._cleanup_old_checkpoints(checkpoint_id, stage)
            
            return str(filepath)
            
        except Exception as e:
            self.logger.error(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def load_checkpoint(self, checkpoint_file: str) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        try:
            filepath = Path(checkpoint_file)
            if not filepath.exists():
                raise FileNotFoundError(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint_file}")
            
            data = {}
            metadata = {}
            
            with h5py.File(filepath, 'r') as f:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆæ–‡å­—åˆ—ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¯¾å¿œï¼‰
                if 'metadata' in f:
                    meta_group = f['metadata']
                    for key in meta_group.attrs:
                        value = meta_group.attrs[key]
                        
                        if isinstance(value, bytes):
                            # ãƒã‚¤ãƒˆåˆ—ã‚’UTF-8ã§ãƒ‡ã‚³ãƒ¼ãƒ‰
                            try:
                                decoded_str = value.decode('utf-8')
                                # JSONæ–‡å­—åˆ—ã‹ã©ã†ã‹åˆ¤å®š
                                if decoded_str.startswith(('{', '[', '"')):
                                    try:
                                        metadata[key] = json.loads(decoded_str)
                                    except json.JSONDecodeError:
                                        metadata[key] = decoded_str
                                else:
                                    metadata[key] = decoded_str
                            except UnicodeDecodeError:
                                metadata[key] = str(value)
                        elif key.endswith('_real') and f"{key[:-5]}_imag" in meta_group.attrs:
                            # è¤‡ç´ æ•°ã®å®Ÿéƒ¨ãƒ»è™šéƒ¨ã‚’çµåˆ
                            base_key = key[:-5]
                            if base_key not in metadata:
                                real_part = meta_group.attrs[key]
                                imag_part = meta_group.attrs[f"{base_key}_imag"]
                                metadata[base_key] = complex(real_part, imag_part)
                        elif not key.endswith('_imag'):
                            # é€šå¸¸ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿
                            metadata[key] = value
                
                # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆæ•°ç†çš„ã«æ­£ã—ã„å‹å¾©å…ƒï¼‰
                if 'data' in f:
                    data_group = f['data']
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
                    for key in data_group.keys():
                        dataset = data_group[key]
                        data[key] = np.array(dataset)
                    
                    # å±æ€§ã®èª­ã¿è¾¼ã¿
                    for key in data_group.attrs:
                        value = data_group.attrs[key]
                        
                        if isinstance(value, bytes):
                            # ãƒã‚¤ãƒˆåˆ—ã‚’UTF-8ã§ãƒ‡ã‚³ãƒ¼ãƒ‰
                            try:
                                decoded_str = value.decode('utf-8')
                                # JSONæ–‡å­—åˆ—ã‹ã©ã†ã‹åˆ¤å®š
                                if decoded_str.startswith(('{', '[', '"')):
                                    try:
                                        data[key] = json.loads(decoded_str)
                                    except json.JSONDecodeError:
                                        data[key] = decoded_str
                                else:
                                    data[key] = decoded_str
                            except UnicodeDecodeError:
                                data[key] = str(value)
                        elif key.endswith('_real') and f"{key[:-5]}_imag" in data_group.attrs:
                            # è¤‡ç´ æ•°ã®å®Ÿéƒ¨ãƒ»è™šéƒ¨ã‚’çµåˆ
                            base_key = key[:-5]
                            if base_key not in data:
                                real_part = data_group.attrs[key]
                                imag_part = data_group.attrs[f"{base_key}_imag"]
                                data[base_key] = complex(real_part, imag_part)
                        elif not key.endswith('_imag'):
                            # é€šå¸¸ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿
                            data[key] = value
            
            self.logger.info(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {checkpoint_file}")
            return data, metadata
            
        except Exception as e:
            self.logger.error(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _cleanup_old_checkpoints(self, checkpoint_id: str, stage: str):
        """å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å‰Šé™¤"""
        try:
            pattern = f"{stage}_{checkpoint_id}_*.h5"
            checkpoint_files = list(self.base_dir.glob(pattern))
            
            if len(checkpoint_files) > self.params.max_checkpoint_files:
                # ä½œæˆæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
                checkpoint_files.sort(key=lambda x: x.stat().st_mtime)
                
                # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                files_to_delete = checkpoint_files[:-self.params.max_checkpoint_files]
                for file_path in files_to_delete:
                    file_path.unlink()
                    self.logger.info(f"å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤: {file_path.name}")
                    
        except Exception as e:
            self.logger.warning(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
    
    def should_save_checkpoint(self) -> bool:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜åˆ¤å®š"""
        return (time.time() - self.last_checkpoint_time) >= self.params.checkpoint_interval_seconds
    
    def get_latest_checkpoint(self, stage: str = None) -> Optional[str]:
        """æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå–å¾—"""
        try:
            if stage:
                pattern = f"{stage}_*.h5"
            else:
                pattern = "*.h5"
            
            checkpoint_files = list(self.base_dir.glob(pattern))
            
            if not checkpoint_files:
                return None
            
            # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™
            latest_file = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
            return str(latest_file)
            
        except Exception as e:
            self.logger.error(f"æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

class NonCommutativeKolmogorovArnoldRepresentation(nn.Module):
    """
    éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ã®å®Ÿè£…
    
    å®šç†: ä»»æ„ã®éå¯æ›é€£ç¶šæ±é–¢æ•° F ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¡¨ç¾å¯èƒ½
    F(xÌ‚â‚, ..., xÌ‚â‚™) = Î£ Î¦Ì‚q(Î£ ÏˆÌ‚q,p(xÌ‚p))
    
    ã“ã“ã§:
    - Î¦Ì‚q: å˜å¤‰æ•°ä½œç”¨ç´ å€¤é–¢æ•°
    - ÏˆÌ‚q,p: éå¯æ›å¤‰æ•°ã«ä¾å­˜ã™ã‚‹ä½œç”¨ç´ 
    - åˆæˆã¯éå¯æ›â˜…ç©ã§å®šç¾©
    """
    
    def __init__(self, params: NKATRiemannParameters):
        super().__init__()
        self.params = params
        self.device = device
        self.logger = self._setup_logger()
        
        # éå¯æ›ä»£æ•°ã®åˆæœŸåŒ–
        self._initialize_noncommutative_algebra()
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ä¸‰é‡ã®åˆæœŸåŒ–
        self._initialize_spectral_triple()
        
        # æ··åˆç²¾åº¦è¨ˆç®—ç”¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ¼
        if self.params.use_mixed_precision:
            self.scaler = GradScaler()
        
        self.logger.info(f"NKATè¡¨ç¾åˆæœŸåŒ–å®Œäº†: {self.params.nkat_dimension}æ¬¡å…ƒ")
    
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger = logging.getLogger('NKATRepresentation')
        logger.setLevel(self.params.log_level)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _initialize_noncommutative_algebra(self):
        """éå¯æ›ä»£æ•°ã®åˆæœŸåŒ–"""
        dim = self.params.nkat_dimension
        
        # éå¯æ›æ§‹é€ å®šæ•° [XÌ‚áµ¢, XÌ‚â±¼] = iÎ¸áµ¢â±¼ï¼ˆå®Ÿæ•°éƒ¨ã®ã¿ä½¿ç”¨ï¼‰
        self.theta_matrix = torch.zeros(dim, dim, device=self.device, dtype=torch.complex128)
        for i in range(dim):
            for j in range(i+1, dim):
                theta_value = self.params.theta_ij * (1 + 0.1 * (i + j))
                self.theta_matrix[i, j] = complex(theta_value, 0)  # å®Ÿæ•°å€¤ã¨ã—ã¦è¨­å®š
                self.theta_matrix[j, i] = -self.theta_matrix[i, j]
        
        # éå¯æ›åº§æ¨™ä½œç”¨ç´ ï¼ˆå®Ÿæ•°éƒ¨ã®ã¿ä½¿ç”¨ï¼‰
        self.coordinate_operators = nn.ParameterList([
            nn.Parameter(
                torch.randn(self.params.c_star_algebra_dim, 
                           self.params.c_star_algebra_dim, 
                           device=self.device, dtype=torch.complex128) * 0.1
            )
            for _ in range(dim)
        ])
        
        # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ï¼ˆå®Ÿæ•°å€¤ï¼‰
        self.dirac_operator = nn.Parameter(
            torch.randn(self.params.hilbert_space_dim, 
                       self.params.hilbert_space_dim, 
                       device=self.device, dtype=torch.float32) * 0.1
        )
        
        self.logger.info("éå¯æ›ä»£æ•°åˆæœŸåŒ–å®Œäº†")
    
    def _initialize_spectral_triple(self):
        """ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ä¸‰é‡ (A, H, D) ã®åˆæœŸåŒ–"""
        # A: éå¯æ›ä»£æ•° - æ•°ç†çš„ã«æ­£ã—ã„å‹çµ±ä¸€
        self.algebra_representation = nn.Linear(
            self.params.nkat_dimension, 
            self.params.spectral_triple_dim,
            device=self.device,
            dtype=torch.float32  # æ··åˆç²¾åº¦å¯¾å¿œã®ãŸã‚ float32 ã«çµ±ä¸€
        )
        
        # H: ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç©ºé–“
        self.hilbert_space_embedding = nn.Linear(
            self.params.spectral_triple_dim,
            self.params.hilbert_space_dim,
            device=self.device,
            dtype=torch.float32  # æ··åˆç²¾åº¦å¯¾å¿œã®ãŸã‚ float32 ã«çµ±ä¸€
        )
        
        # D: ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ï¼ˆè‡ªå·±å…±å½¹ï¼‰- å®Ÿæ•°å€¤ã§åˆæœŸåŒ–
        dirac_real = torch.randn(self.params.hilbert_space_dim, 
                                self.params.hilbert_space_dim, 
                                device=self.device, dtype=torch.float32) * 0.1
        self.dirac_operator_spectral = nn.Parameter(
            dirac_real + dirac_real.T  # è‡ªå·±å…±å½¹æ€§ã‚’ä¿è¨¼
        )
        
        self.logger.info("ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ä¸‰é‡åˆæœŸåŒ–å®Œäº†")
    
    def star_product(self, f: torch.Tensor, g: torch.Tensor) -> torch.Tensor:
        """
        éå¯æ›â˜…ç©ã®è¨ˆç®—
        (f â˜… g)(x) = f(x)g(x) + (iÎ¸/2) âˆ‚f/âˆ‚xáµ¢ âˆ‚g/âˆ‚xâ±¼ Î¸áµ¢â±¼ + O(Î¸Â²)
        """
        # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®å‹ã‚’çµ±ä¸€
        f = f.to(dtype=torch.float32)
        g = g.to(dtype=torch.float32)
        
        # 0æ¬¡é …: é€šå¸¸ã®ç©
        result = f * g
        
        # 1æ¬¡é …: éå¯æ›è£œæ­£ï¼ˆæ•°ç†çš„ã«æ­£ã—ã„å®Ÿè£…ï¼‰
        if self.params.theta_ij != 0 and f.requires_grad and g.requires_grad:
            try:
                # å‹¾é…è¨ˆç®—ï¼ˆè‡ªå‹•å¾®åˆ†ä½¿ç”¨ï¼‰
                f_grad = torch.autograd.grad(f.sum(), f, create_graph=True, retain_graph=True)[0]
                g_grad = torch.autograd.grad(g.sum(), g, create_graph=True, retain_graph=True)[0]
                
                # Î¸è¡Œåˆ—ã¨ã®ç¸®ç´„ï¼ˆå®Ÿæ•°éƒ¨ã®ã¿ï¼‰
                noncommutative_correction = torch.zeros_like(result)
                theta_real = self.theta_matrix.real.to(dtype=torch.float32)
                
                for i in range(min(self.params.nkat_dimension, f_grad.shape[-1])):
                    for j in range(min(self.params.nkat_dimension, g_grad.shape[-1])):
                        noncommutative_correction += (
                            0.5 * theta_real[i, j] * 
                            f_grad[..., i] * g_grad[..., j]
                        )
                
                result += noncommutative_correction
            except RuntimeError:
                # å‹¾é…è¨ˆç®—ãŒä¸å¯èƒ½ãªå ´åˆã¯0æ¬¡é …ã®ã¿
                pass
        
        return result
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        NKATè¡¨ç¾ã®å‰å‘ãè¨ˆç®—
        F(xÌ‚) = Î£ Î¦Ì‚q(Î£ ÏˆÌ‚q,p(xÌ‚p))
        """
        # å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®å‹ã‚’çµ±ä¸€
        x = x.to(dtype=torch.float32, device=self.device)
        
        # æ··åˆç²¾åº¦è¨ˆç®—
        if self.params.use_mixed_precision:
            with autocast():
                return self._forward_impl(x)
        else:
            return self._forward_impl(x)
    
    def _forward_impl(self, x: torch.Tensor) -> torch.Tensor:
        """å®Ÿéš›ã®å‰å‘ãè¨ˆç®—å®Ÿè£…"""
        # éå¯æ›åº§æ¨™ã¸ã®åŸ‹ã‚è¾¼ã¿ï¼ˆå‹å®‰å…¨ï¼‰
        x_noncommutative = self.algebra_representation(x)
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ä¸‰é‡ã§ã®å‡¦ç†
        h_embedded = self.hilbert_space_embedding(x_noncommutative)
        
        # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã®é©ç”¨ï¼ˆå®Ÿæ•°æ¼”ç®—ï¼‰
        dirac_output = torch.matmul(h_embedded, self.dirac_operator_spectral)
        
        # éå¯æ›â˜…ç©ã«ã‚ˆã‚‹åˆæˆï¼ˆæ•°ç†çš„ã«æ­£ã—ã„å®Ÿè£…ï¼‰
        result = dirac_output
        for i, coord_op in enumerate(self.coordinate_operators):
            if i < x.shape[-1]:
                # å‹ã‚’çµ±ä¸€ã—ã¦ã‹ã‚‰æ¼”ç®—
                coord_op_real = coord_op.real.to(dtype=torch.float32)
                coord_contribution = torch.matmul(
                    x[..., i:i+1], 
                    coord_op_real[:1, :1]
                )
                result = self.star_product(result, coord_contribution)
        
        # æ•°ç†çš„ã«æ­£ã—ã„å®Ÿæ•°å€¤ã‚’è¿”ã™ï¼ˆç‰©ç†çš„è¦³æ¸¬é‡ï¼‰
        return result.real if result.is_complex() else result

class RiemannZetaAnalyzer:
    """ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°è§£æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, params: NKATRiemannParameters):
        self.params = params
        self.logger = self._setup_logger()
        
        # NKATè¡¨ç¾ã®åˆæœŸåŒ–
        self.nkat_representation = NonCommutativeKolmogorovArnoldRepresentation(params)
        
        # è¶…é«˜ç²¾åº¦è¨ˆç®—è¨­å®š
        mp.mp.dps = params.nkat_precision
        
        self.logger.info("ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿è§£æå™¨åˆæœŸåŒ–å®Œäº†")
    
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger = logging.getLogger('RiemannZetaAnalyzer')
        logger.setLevel(self.params.log_level)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def riemann_zeta_mpmath(self, s: complex) -> complex:
        """è¶…é«˜ç²¾åº¦ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°è¨ˆç®—"""
        try:
            # æ•°ç†çš„ã«æ­£ã—ã„ç‰¹æ®Šå€¤ã®å‡¦ç†
            if abs(s - 1.0) < 1e-15:
                # s=1ã§ã®æ¥µã‚’å›é¿
                return complex(float('inf'), 0)
            elif abs(s.real) < 1e-15 and abs(s.imag) < 1e-15:
                # s=0ã§ã®å€¤: Î¶(0) = -1/2
                return complex(-0.5, 0)
            elif abs(s.real - (-1)) < 1e-15 and abs(s.imag) < 1e-15:
                # s=-1ã§ã®å€¤: Î¶(-1) = -1/12
                return complex(-1.0/12.0, 0)
            elif abs(s.real - (-2)) < 1e-15 and abs(s.imag) < 1e-15:
                # s=-2ã§ã®å€¤: Î¶(-2) = 0
                return complex(0, 0)
            
            s_mp = mp.mpc(s.real, s.imag)
            result = mp.zeta(s_mp)
            return complex(float(result.real), float(result.imag))
        except Exception as e:
            self.logger.error(f"ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿è¨ˆç®—ã‚¨ãƒ©ãƒ¼ s={s}: {e}")
            # æ•°ç†çš„ã«æ­£ã—ã„ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¿”ã™
            if s.real < 0:
                return complex(0, 0)  # è² ã®å¶æ•°ã§ã®é›¶ç‚¹
            else:
                return complex(1, 0)  # æ­£ã®å®Ÿéƒ¨ã§ã®è¿‘ä¼¼å€¤
    
    def find_riemann_zeros(self, t_start: float, t_end: float, 
                          num_points: int = 1000) -> List[complex]:
        """ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®é›¶ç‚¹æ¢ç´¢"""
        zeros = []
        
        # è‡¨ç•Œç·š s = 1/2 + it ã§ã®æ¢ç´¢
        t_values = np.linspace(t_start, t_end, num_points)
        
        with tqdm(t_values, desc="é›¶ç‚¹æ¢ç´¢", unit="ç‚¹") as pbar:
            for t in pbar:
                s = complex(0.5, t)
                zeta_value = self.riemann_zeta_mpmath(s)
                
                # é›¶ç‚¹åˆ¤å®šï¼ˆçµ¶å¯¾å€¤ãŒé–¾å€¤ä»¥ä¸‹ï¼‰
                if abs(zeta_value) < self.params.riemann_zero_search_precision:
                    zeros.append(s)
                    pbar.set_postfix({"é›¶ç‚¹æ•°": len(zeros)})
                    self.logger.info(f"é›¶ç‚¹ç™ºè¦‹: s = {s}, Î¶(s) = {zeta_value}")
                
                if len(zeros) >= self.params.riemann_max_zeros:
                    break
        
        return zeros
    
    def verify_riemann_hypothesis_nkat(self, zeros: List[complex]) -> Dict[str, Any]:
        """NKATè¡¨ç¾ã«ã‚ˆã‚‹ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼"""
        verification_results = {
            'total_zeros': len(zeros),
            'zeros_on_critical_line': 0,
            'max_deviation_from_critical_line': 0.0,
            'nkat_consistency_score': 0.0,
            'verification_details': []
        }
        
        if not zeros:
            return verification_results
        
        # å„é›¶ç‚¹ã®æ¤œè¨¼
        with tqdm(zeros, desc="NKATæ¤œè¨¼", unit="é›¶ç‚¹") as pbar:
            for i, zero in enumerate(pbar):
                # è‡¨ç•Œç·šã‹ã‚‰ã®åå·®
                deviation = abs(zero.real - 0.5)
                verification_results['max_deviation_from_critical_line'] = max(
                    verification_results['max_deviation_from_critical_line'], 
                    deviation
                )
                
                # è‡¨ç•Œç·šä¸Šã®é›¶ç‚¹ã‚«ã‚¦ãƒ³ãƒˆ
                if deviation < self.params.riemann_zero_search_precision:
                    verification_results['zeros_on_critical_line'] += 1
                
                # NKATè¡¨ç¾ã«ã‚ˆã‚‹ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæ•°ç†çš„ã«æ­£ã—ã„å®Ÿè£…ï¼‰
                try:
                    # è¤‡ç´ æ•°ã‚’å®Ÿæ•°ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ï¼ˆå®Ÿéƒ¨ãƒ»è™šéƒ¨ã‚’åˆ†é›¢ï¼‰
                    zero_tensor = torch.tensor(
                        [[zero.real, zero.imag]], 
                        device=device, 
                        dtype=torch.float32,
                        requires_grad=False
                    )
                    
                    # NKATè¡¨ç¾ã®è¨ˆç®—
                    with torch.no_grad():  # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–
                        nkat_output = self.nkat_representation(zero_tensor)
                        nkat_consistency = float(torch.norm(nkat_output).item())
                    
                except Exception as e:
                    self.logger.warning(f"NKATè¨ˆç®—ã‚¨ãƒ©ãƒ¼ (é›¶ç‚¹ {i}): {e}")
                    nkat_consistency = 0.0
                
                verification_results['verification_details'].append({
                    'zero_index': i,
                    'zero': zero,
                    'deviation_from_critical_line': deviation,
                    'nkat_consistency': nkat_consistency
                })
                
                pbar.set_postfix({
                    "è‡¨ç•Œç·šä¸Š": verification_results['zeros_on_critical_line'],
                    "æœ€å¤§åå·®": f"{verification_results['max_deviation_from_critical_line']:.2e}"
                })
        
        # å…¨ä½“çš„ãªä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        if verification_results['verification_details']:
            consistency_scores = [
                detail['nkat_consistency'] 
                for detail in verification_results['verification_details']
            ]
            verification_results['nkat_consistency_score'] = np.mean(consistency_scores)
        
        # ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã®æ¤œè¨¼çµæœ
        verification_results['riemann_hypothesis_verified'] = (
            verification_results['zeros_on_critical_line'] == verification_results['total_zeros']
        )
        
        return verification_results

class NKATRiemannDashboard:
    """Streamlit ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"""
    
    def __init__(self, params: NKATRiemannParameters):
        self.params = params
        self.system_monitor = SystemMonitor(params)
        self.checkpoint_manager = CheckpointManager(params)
        self.riemann_analyzer = RiemannZetaAnalyzer(params)
        
    def run_dashboard(self):
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å®Ÿè¡Œ"""
        st.set_page_config(
            page_title="NKAT ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æã‚·ã‚¹ãƒ†ãƒ ",
            page_icon="ğŸŒŒ",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        st.title("ğŸŒŒ NKAT ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æã‚·ã‚¹ãƒ†ãƒ ")
        st.markdown("**éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ã«ã‚ˆã‚‹æœ€é«˜ç²¾åº¦è§£æ**")
        
        # ã‚µã‚¤ãƒ‰ãƒãƒ¼
        with st.sidebar:
            st.header("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡")
            
            if st.button("ğŸš€ ç›£è¦–é–‹å§‹"):
                self.system_monitor.start_monitoring()
                st.success("ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–é–‹å§‹")
            
            if st.button("â¹ï¸ ç›£è¦–åœæ­¢"):
                self.system_monitor.stop_monitoring()
                st.info("ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–åœæ­¢")
            
            st.header("ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿")
            st.json(asdict(self.params))
        
        # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
        col1, col2 = st.columns(2)
        
        with col1:
            st.header("ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–")
            self._display_system_monitoring()
        
        with col2:
            st.header("ğŸ”¬ ãƒªãƒ¼ãƒãƒ³è§£æ")
            self._display_riemann_analysis()
        
        # ä¸‹éƒ¨ã‚¨ãƒªã‚¢
        st.header("ğŸ“ˆ è§£æçµæœ")
        self._display_analysis_results()
    
    def _display_system_monitoring(self):
        """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–è¡¨ç¤º"""
        recent_data = self.system_monitor.get_recent_data(60)
        
        if recent_data:
            latest_data = recent_data[-1]
            
            # GPUæƒ…å ±
            if latest_data.get('gpu'):
                gpu_info = latest_data['gpu']
                st.metric("GPUä½¿ç”¨ç‡", f"{gpu_info['load_percent']:.1f}%")
                st.metric("GPUæ¸©åº¦", f"{gpu_info['temperature_celsius']:.1f}Â°C")
                st.metric("GPUãƒ¡ãƒ¢ãƒª", f"{gpu_info['memory_percent']:.1f}%")
            
            # CPUæƒ…å ±
            if latest_data.get('cpu'):
                cpu_info = latest_data['cpu']
                st.metric("CPUä½¿ç”¨ç‡", f"{cpu_info['usage_percent']:.1f}%")
                if cpu_info.get('avg_temperature_celsius'):
                    st.metric("CPUæ¸©åº¦", f"{cpu_info['avg_temperature_celsius']:.1f}Â°C")
            
            # ãƒ¡ãƒ¢ãƒªæƒ…å ±
            if latest_data.get('memory'):
                memory_info = latest_data['memory']
                st.metric("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡", f"{memory_info['percent']:.1f}%")
        else:
            st.info("ç›£è¦–ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
    
    def _display_riemann_analysis(self):
        """ãƒªãƒ¼ãƒãƒ³è§£æè¡¨ç¤º"""
        if st.button("ğŸ” é›¶ç‚¹æ¢ç´¢é–‹å§‹"):
            with st.spinner("é›¶ç‚¹æ¢ç´¢ä¸­..."):
                zeros = self.riemann_analyzer.find_riemann_zeros(
                    self.params.riemann_critical_line_start,
                    min(self.params.riemann_critical_line_end, 50.0),  # ãƒ‡ãƒ¢ç”¨ã«åˆ¶é™
                    100  # ãƒ‡ãƒ¢ç”¨ã«åˆ¶é™
                )
                
                st.success(f"é›¶ç‚¹ {len(zeros)} å€‹ç™ºè¦‹")
                
                if zeros:
                    # æ¤œè¨¼å®Ÿè¡Œ
                    verification = self.riemann_analyzer.verify_riemann_hypothesis_nkat(zeros)
                    
                    st.metric("è‡¨ç•Œç·šä¸Šã®é›¶ç‚¹", verification['zeros_on_critical_line'])
                    st.metric("ç·é›¶ç‚¹æ•°", verification['total_zeros'])
                    st.metric("æœ€å¤§åå·®", f"{verification['max_deviation_from_critical_line']:.2e}")
                    
                    if verification['riemann_hypothesis_verified']:
                        st.success("âœ… ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ãŒæ¤œè¨¼ã•ã‚Œã¾ã—ãŸï¼")
                    else:
                        st.warning("âš ï¸ ä¸€éƒ¨ã®é›¶ç‚¹ãŒè‡¨ç•Œç·šã‹ã‚‰å¤–ã‚Œã¦ã„ã¾ã™")
    
    def _display_analysis_results(self):
        """è§£æçµæœè¡¨ç¤º"""
        st.info("è§£æçµæœã¯å®Ÿè¡Œå¾Œã«è¡¨ç¤ºã•ã‚Œã¾ã™")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸŒŒ NKAT ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æã‚·ã‚¹ãƒ†ãƒ  - æœ€é«˜ç²¾åº¦ç‰ˆ")
    print("=" * 60)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
    params = NKATRiemannParameters()
    
    # Streamlitãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èµ·å‹•
    dashboard = NKATRiemannDashboard(params)
    dashboard.run_dashboard()

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT v11 è©³ç´°åæŸåˆ†æã‚·ã‚¹ãƒ†ãƒ 
0.497762åæŸçµæœã®æ·±æ˜ã‚Šåˆ†æ

ä½œæˆè€…: NKAT Research Team
ä½œæˆæ—¥: 2025å¹´5æœˆ26æ—¥
ãƒãƒ¼ã‚¸ãƒ§ãƒ³: v11.0
"""

import numpy as np
import matplotlib.pyplot as plt
import json
import os
import pandas as pd
from scipy import stats
from scipy.optimize import curve_fit
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

class ConvergenceAnalyzer:
    """åæŸåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, results_file='high_precision_riemann_results.json'):
        """åˆæœŸåŒ–"""
        self.results_file = results_file
        self.data = self.load_data()
        self.analysis_results = {}
        
    def load_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if not os.path.exists(self.results_file):
            raise FileNotFoundError(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.results_file}")
        
        with open(self.results_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def analyze_convergence_patterns(self):
        """åæŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        print("ğŸ” åæŸãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚’é–‹å§‹...")
        
        if 'convergence_to_half_all' not in self.data:
            print("âŒ åæŸãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        convergence_data = np.array(self.data['convergence_to_half_all'])
        
        # åŸºæœ¬çµ±è¨ˆ
        mean_convergence = np.mean(convergence_data)
        std_convergence = np.std(convergence_data)
        min_convergence = np.min(convergence_data)
        max_convergence = np.max(convergence_data)
        
        # ç†è«–å€¤ã‹ã‚‰ã®åå·®
        theoretical_value = 0.5
        deviation = abs(mean_convergence - theoretical_value)
        relative_error = (deviation / theoretical_value) * 100
        
        # å®‰å®šæ€§æŒ‡æ¨™
        coefficient_of_variation = (std_convergence / mean_convergence) * 100
        
        self.analysis_results['convergence_patterns'] = {
            'mean_convergence': mean_convergence,
            'std_convergence': std_convergence,
            'min_convergence': min_convergence,
            'max_convergence': max_convergence,
            'theoretical_deviation': deviation,
            'relative_error_percent': relative_error,
            'coefficient_of_variation': coefficient_of_variation,
            'data_shape': convergence_data.shape,
            'total_samples': convergence_data.size
        }
        
        print(f"âœ… å¹³å‡åæŸåº¦: {mean_convergence:.8f}")
        print(f"âœ… æ¨™æº–åå·®: {std_convergence:.8f}")
        print(f"âœ… ç†è«–å€¤åå·®: {deviation:.8f}")
        print(f"âœ… ç›¸å¯¾èª¤å·®: {relative_error:.4f}%")
        print(f"âœ… å¤‰å‹•ä¿‚æ•°: {coefficient_of_variation:.6f}%")
        
        return self.analysis_results['convergence_patterns']
    
    def analyze_gamma_dependency(self):
        """Î³å€¤ä¾å­˜æ€§ã‚’åˆ†æ"""
        print("\nğŸ” Î³å€¤ä¾å­˜æ€§åˆ†æã‚’é–‹å§‹...")
        
        if 'gamma_values' not in self.data or 'convergence_to_half_all' not in self.data:
            print("âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        gamma_values = np.array(self.data['gamma_values'])
        convergence_data = np.array(self.data['convergence_to_half_all'])
        
        # å„Î³å€¤ã«å¯¾ã™ã‚‹åæŸåº¦ã®å¹³å‡
        gamma_convergence_means = []
        for i in range(len(gamma_values)):
            if i < convergence_data.shape[1]:
                mean_conv = np.mean(convergence_data[:, i])
                gamma_convergence_means.append(mean_conv)
        
        gamma_convergence_means = np.array(gamma_convergence_means)
        
        # ç›¸é–¢åˆ†æ
        if len(gamma_values) == len(gamma_convergence_means):
            correlation, p_value = stats.pearsonr(gamma_values, gamma_convergence_means)
            
            # ç·šå½¢å›å¸°
            slope, intercept, r_value, p_value_reg, std_err = stats.linregress(
                gamma_values, gamma_convergence_means
            )
            
            # å¤šé …å¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
            poly_coeffs = np.polyfit(gamma_values, gamma_convergence_means, 2)
            poly_func = np.poly1d(poly_coeffs)
            
            self.analysis_results['gamma_dependency'] = {
                'gamma_values': gamma_values.tolist(),
                'convergence_means': gamma_convergence_means.tolist(),
                'correlation': correlation,
                'correlation_p_value': p_value,
                'linear_slope': slope,
                'linear_intercept': intercept,
                'r_squared': r_value**2,
                'polynomial_coefficients': poly_coeffs.tolist()
            }
            
            print(f"âœ… Î³å€¤ã¨ã®ç›¸é–¢: {correlation:.6f} (p={p_value:.6f})")
            print(f"âœ… ç·šå½¢å›å¸° RÂ²: {r_value**2:.6f}")
            print(f"âœ… ç·šå½¢å‚¾ã: {slope:.8f}")
        
        return self.analysis_results.get('gamma_dependency', {})
    
    def analyze_theoretical_comparison(self):
        """ç†è«–å€¤ã¨ã®æ¯”è¼ƒåˆ†æ"""
        print("\nğŸ” ç†è«–å€¤æ¯”è¼ƒåˆ†æã‚’é–‹å§‹...")
        
        if 'convergence_to_half_all' not in self.data:
            print("âŒ åæŸãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        convergence_data = np.array(self.data['convergence_to_half_all']).flatten()
        theoretical_value = 0.5
        
        # çµ±è¨ˆçš„æ¤œå®š
        # ä¸€æ¨™æœ¬tæ¤œå®šï¼ˆç†è«–å€¤ã¨ã®æ¯”è¼ƒï¼‰
        t_stat, t_p_value = stats.ttest_1samp(convergence_data, theoretical_value)
        
        # æ­£è¦æ€§æ¤œå®š
        shapiro_stat, shapiro_p = stats.shapiro(convergence_data[:5000] if len(convergence_data) > 5000 else convergence_data)
        
        # ä¿¡é ¼åŒºé–“
        confidence_level = 0.95
        degrees_freedom = len(convergence_data) - 1
        confidence_interval = stats.t.interval(
            confidence_level, degrees_freedom,
            loc=np.mean(convergence_data),
            scale=stats.sem(convergence_data)
        )
        
        # åŠ¹æœé‡ï¼ˆCohen's dï¼‰
        cohens_d = (np.mean(convergence_data) - theoretical_value) / np.std(convergence_data)
        
        self.analysis_results['theoretical_comparison'] = {
            'theoretical_value': theoretical_value,
            'sample_mean': np.mean(convergence_data),
            'sample_std': np.std(convergence_data),
            't_statistic': t_stat,
            't_p_value': t_p_value,
            'shapiro_statistic': shapiro_stat,
            'shapiro_p_value': shapiro_p,
            'confidence_interval': confidence_interval,
            'cohens_d': cohens_d,
            'sample_size': len(convergence_data)
        }
        
        print(f"âœ… tæ¤œå®šçµ±è¨ˆé‡: {t_stat:.6f} (p={t_p_value:.6f})")
        print(f"âœ… æ­£è¦æ€§æ¤œå®š: {shapiro_stat:.6f} (p={shapiro_p:.6f})")
        print(f"âœ… 95%ä¿¡é ¼åŒºé–“: [{confidence_interval[0]:.6f}, {confidence_interval[1]:.6f}]")
        print(f"âœ… Cohen's d: {cohens_d:.6f}")
        
        return self.analysis_results['theoretical_comparison']
    
    def generate_improvement_suggestions(self):
        """æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        print("\nğŸ’¡ æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆä¸­...")
        
        suggestions = []
        
        # åæŸãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã«åŸºã¥ãææ¡ˆ
        if 'convergence_patterns' in self.analysis_results:
            conv_analysis = self.analysis_results['convergence_patterns']
            
            if conv_analysis['relative_error_percent'] > 1.0:
                suggestions.append({
                    'category': 'ç²¾åº¦æ”¹å–„',
                    'priority': 'high',
                    'suggestion': 'ã‚ˆã‚Šé«˜ç²¾åº¦ãªæ•°å€¤è¨ˆç®—æ‰‹æ³•ã®å°å…¥ã‚’æ¤œè¨',
                    'details': f"ç¾åœ¨ã®ç›¸å¯¾èª¤å·®: {conv_analysis['relative_error_percent']:.4f}%"
                })
            
            if conv_analysis['coefficient_of_variation'] > 0.01:
                suggestions.append({
                    'category': 'å®‰å®šæ€§å‘ä¸Š',
                    'priority': 'medium',
                    'suggestion': 'è¨ˆç®—ã®å®‰å®šæ€§å‘ä¸Šã®ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–',
                    'details': f"å¤‰å‹•ä¿‚æ•°: {conv_analysis['coefficient_of_variation']:.6f}%"
                })
        
        # Î³å€¤ä¾å­˜æ€§åˆ†æã«åŸºã¥ãææ¡ˆ
        if 'gamma_dependency' in self.analysis_results:
            gamma_analysis = self.analysis_results['gamma_dependency']
            
            if abs(gamma_analysis.get('correlation', 0)) > 0.5:
                suggestions.append({
                    'category': 'Î³å€¤æœ€é©åŒ–',
                    'priority': 'medium',
                    'suggestion': 'Î³å€¤é¸æŠã®æœ€é©åŒ–ã«ã‚ˆã‚‹åæŸæ€§å‘ä¸Š',
                    'details': f"Î³å€¤ç›¸é–¢: {gamma_analysis.get('correlation', 0):.6f}"
                })
        
        # ç†è«–å€¤æ¯”è¼ƒã«åŸºã¥ãææ¡ˆ
        if 'theoretical_comparison' in self.analysis_results:
            theory_analysis = self.analysis_results['theoretical_comparison']
            
            if theory_analysis['t_p_value'] < 0.05:
                suggestions.append({
                    'category': 'ç†è«–æ¤œè¨¼',
                    'priority': 'high',
                    'suggestion': 'ç†è«–å€¤ã¨ã®æœ‰æ„å·®ã®åŸå› èª¿æŸ»ã¨ä¿®æ­£',
                    'details': f"tæ¤œå®š på€¤: {theory_analysis['t_p_value']:.6f}"
                })
        
        # ä¸€èˆ¬çš„ãªææ¡ˆ
        suggestions.extend([
            {
                'category': 'è¨ˆç®—è³‡æº',
                'priority': 'low',
                'suggestion': 'ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã§ã®æ¤œè¨¼å®Ÿè¡Œ',
                'details': 'çµ±è¨ˆçš„ä¿¡é ¼æ€§ã®å‘ä¸Š'
            },
            {
                'category': 'æ‰‹æ³•æ‹¡å¼µ',
                'priority': 'medium',
                'suggestion': 'ç•°ãªã‚‹æ•°å€¤ç©åˆ†æ‰‹æ³•ã¨ã®æ¯”è¼ƒæ¤œè¨¼',
                'details': 'æ‰‹æ³•ã®å¦¥å½“æ€§ç¢ºèª'
            }
        ])
        
        self.analysis_results['improvement_suggestions'] = suggestions
        
        print(f"âœ… {len(suggestions)}ä»¶ã®æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        
        return suggestions
    
    def create_comprehensive_visualization(self):
        """åŒ…æ‹¬çš„å¯è¦–åŒ–ã‚’ä½œæˆ"""
        print("\nğŸ“Š åŒ…æ‹¬çš„å¯è¦–åŒ–ã‚’ä½œæˆä¸­...")
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_dir = "convergence_analysis_results"
        os.makedirs(output_dir, exist_ok=True)
        
        # å¤§ããªãƒ•ã‚£ã‚®ãƒ¥ã‚¢ã‚µã‚¤ã‚ºè¨­å®š
        fig = plt.figure(figsize=(20, 16))
        
        # ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆé…ç½®
        gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)
        
        # 1. åæŸåº¦ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        ax1 = fig.add_subplot(gs[0, 0])
        if 'convergence_to_half_all' in self.data:
            convergence_data = np.array(self.data['convergence_to_half_all']).flatten()
            ax1.hist(convergence_data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
            ax1.axvline(0.5, color='red', linestyle='--', label='ç†è«–å€¤ (0.5)')
            ax1.axvline(np.mean(convergence_data), color='orange', linestyle='-', label=f'å¹³å‡å€¤ ({np.mean(convergence_data):.6f})')
            ax1.set_xlabel('åæŸåº¦')
            ax1.set_ylabel('é »åº¦')
            ax1.set_title('åæŸåº¦åˆ†å¸ƒ')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
        
        # 2. Î³å€¤ä¾å­˜æ€§
        ax2 = fig.add_subplot(gs[0, 1])
        if 'gamma_dependency' in self.analysis_results:
            gamma_dep = self.analysis_results['gamma_dependency']
            gamma_vals = gamma_dep['gamma_values']
            conv_means = gamma_dep['convergence_means']
            
            ax2.scatter(gamma_vals, conv_means, alpha=0.7, s=100, color='green')
            
            # ç·šå½¢å›å¸°ç·š
            if 'linear_slope' in gamma_dep:
                x_line = np.linspace(min(gamma_vals), max(gamma_vals), 100)
                y_line = gamma_dep['linear_slope'] * x_line + gamma_dep['linear_intercept']
                ax2.plot(x_line, y_line, 'r--', label=f"ç·šå½¢å›å¸° (RÂ²={gamma_dep.get('r_squared', 0):.4f})")
            
            ax2.set_xlabel('Î³å€¤')
            ax2.set_ylabel('å¹³å‡åæŸåº¦')
            ax2.set_title('Î³å€¤ä¾å­˜æ€§')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        # 3. ç†è«–å€¤ã¨ã®åå·®
        ax3 = fig.add_subplot(gs[0, 2])
        if 'convergence_to_half_all' in self.data:
            convergence_data = np.array(self.data['convergence_to_half_all']).flatten()
            deviations = convergence_data - 0.5
            
            ax3.hist(deviations, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')
            ax3.axvline(0, color='black', linestyle='-', label='ç†è«–å€¤ã‹ã‚‰ã®åå·®=0')
            ax3.axvline(np.mean(deviations), color='blue', linestyle='--', label=f'å¹³å‡åå·® ({np.mean(deviations):.6f})')
            ax3.set_xlabel('ç†è«–å€¤ã‹ã‚‰ã®åå·®')
            ax3.set_ylabel('é »åº¦')
            ax3.set_title('ç†è«–å€¤åå·®åˆ†å¸ƒ')
            ax3.legend()
            ax3.grid(True, alpha=0.3)
        
        # 4. æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆï¼ˆåæŸåº¦ï¼‰
        ax4 = fig.add_subplot(gs[1, :])
        if 'convergence_to_half_all' in self.data:
            convergence_data = np.array(self.data['convergence_to_half_all'])
            
            for i in range(min(convergence_data.shape[0], 5)):  # æœ€åˆã®5ç³»åˆ—
                ax4.plot(convergence_data[i], alpha=0.7, label=f'ç³»åˆ— {i+1}')
            
            ax4.axhline(0.5, color='red', linestyle='--', label='ç†è«–å€¤ (0.5)')
            ax4.set_xlabel('Î³å€¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')
            ax4.set_ylabel('åæŸåº¦')
            ax4.set_title('åæŸåº¦æ™‚ç³»åˆ—')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        
        # 5. çµ±è¨ˆã‚µãƒãƒªãƒ¼
        ax5 = fig.add_subplot(gs[2, 0])
        ax5.axis('off')
        
        if 'convergence_patterns' in self.analysis_results:
            conv_stats = self.analysis_results['convergence_patterns']
            stats_text = f"""
çµ±è¨ˆã‚µãƒãƒªãƒ¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
å¹³å‡åæŸåº¦: {conv_stats['mean_convergence']:.8f}
æ¨™æº–åå·®: {conv_stats['std_convergence']:.8f}
æœ€å°å€¤: {conv_stats['min_convergence']:.8f}
æœ€å¤§å€¤: {conv_stats['max_convergence']:.8f}
ç†è«–å€¤åå·®: {conv_stats['theoretical_deviation']:.8f}
ç›¸å¯¾èª¤å·®: {conv_stats['relative_error_percent']:.4f}%
å¤‰å‹•ä¿‚æ•°: {conv_stats['coefficient_of_variation']:.6f}%
ã‚µãƒ³ãƒ—ãƒ«æ•°: {conv_stats['total_samples']:,}
            """
            ax5.text(0.1, 0.9, stats_text, transform=ax5.transAxes, fontsize=10,
                    verticalalignment='top', fontfamily='monospace')
        
        # 6. å“è³ªè©•ä¾¡
        ax6 = fig.add_subplot(gs[2, 1])
        ax6.axis('off')
        
        # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        if 'convergence_patterns' in self.analysis_results:
            conv_stats = self.analysis_results['convergence_patterns']
            
            # åæŸã‚¹ã‚³ã‚¢ï¼ˆç†è«–å€¤ã«è¿‘ã„ã»ã©é«˜ã„ï¼‰
            convergence_score = 1 - abs(conv_stats['mean_convergence'] - 0.5) * 2
            
            # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ï¼ˆæ¨™æº–åå·®ãŒå°ã•ã„ã»ã©é«˜ã„ï¼‰
            consistency_score = 1 - min(conv_stats['std_convergence'] * 1000, 1)
            
            # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
            overall_quality = (convergence_score + consistency_score) / 2
            
            quality_text = f"""
å“è³ªè©•ä¾¡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
åæŸã‚¹ã‚³ã‚¢: {convergence_score:.6f}
ä¸€è²«æ€§ã‚¹ã‚³ã‚¢: {consistency_score:.6f}
ç·åˆå“è³ª: {overall_quality:.6f}

è©•ä¾¡:
"""
            if overall_quality > 0.95:
                quality_text += "ğŸ‰ å„ªç§€"
            elif overall_quality > 0.9:
                quality_text += "âœ… è‰¯å¥½"
            elif overall_quality > 0.8:
                quality_text += "âš ï¸ æ™®é€š"
            else:
                quality_text += "âŒ è¦æ”¹å–„"
            
            ax6.text(0.1, 0.9, quality_text, transform=ax6.transAxes, fontsize=10,
                    verticalalignment='top', fontfamily='monospace')
        
        # 7. æ”¹å–„ææ¡ˆ
        ax7 = fig.add_subplot(gs[2, 2])
        ax7.axis('off')
        
        if 'improvement_suggestions' in self.analysis_results:
            suggestions = self.analysis_results['improvement_suggestions']
            
            suggestion_text = "æ”¹å–„ææ¡ˆ\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
            for i, suggestion in enumerate(suggestions[:5]):  # æœ€åˆã®5ä»¶
                priority_icon = "ğŸ”´" if suggestion['priority'] == 'high' else "ğŸŸ¡" if suggestion['priority'] == 'medium' else "ğŸŸ¢"
                suggestion_text += f"{priority_icon} {suggestion['category']}\n"
                suggestion_text += f"   {suggestion['suggestion']}\n\n"
            
            ax7.text(0.1, 0.9, suggestion_text, transform=ax7.transAxes, fontsize=9,
                    verticalalignment='top')
        
        # 8. ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†æ
        ax8 = fig.add_subplot(gs[3, :])
        if 'spectral_dimensions_all' in self.data:
            spectral_data = np.array(self.data['spectral_dimensions_all'])
            
            for i in range(min(spectral_data.shape[0], 3)):
                ax8.plot(spectral_data[i], alpha=0.7, marker='o', label=f'ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ {i+1}')
            
            ax8.set_xlabel('Î³å€¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹')
            ax8.set_ylabel('ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ')
            ax8.set_title('ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒåˆ†æ')
            ax8.legend()
            ax8.grid(True, alpha=0.3)
        
        # ã‚¿ã‚¤ãƒˆãƒ«è¨­å®š
        fig.suptitle('NKAT v11 è©³ç´°åæŸåˆ†æãƒ¬ãƒãƒ¼ãƒˆ\n0.497762åæŸçµæœã®åŒ…æ‹¬çš„åˆ†æ', 
                     fontsize=16, fontweight='bold')
        
        # ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f"detailed_convergence_analysis_{timestamp}.png")
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        
        print(f"âœ… å¯è¦–åŒ–ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
        
        return output_file
    
    def save_analysis_results(self):
        """åˆ†æçµæœã‚’ä¿å­˜"""
        output_dir = "convergence_analysis_results"
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f"convergence_analysis_{timestamp}.json")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
        self.analysis_results['metadata'] = {
            'analysis_timestamp': timestamp,
            'source_file': self.results_file,
            'analyzer_version': 'v11.0'
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"âœ… åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
        
        return output_file
    
    def run_complete_analysis(self):
        """å®Œå…¨åˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸš€ NKAT v11 è©³ç´°åæŸåˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        print("=" * 60)
        
        try:
            # å„åˆ†æã‚’å®Ÿè¡Œ
            self.analyze_convergence_patterns()
            self.analyze_gamma_dependency()
            self.analyze_theoretical_comparison()
            self.generate_improvement_suggestions()
            
            # å¯è¦–åŒ–ä½œæˆ
            visualization_file = self.create_comprehensive_visualization()
            
            # çµæœä¿å­˜
            results_file = self.save_analysis_results()
            
            print("\n" + "=" * 60)
            print("ğŸ‰ åˆ†æå®Œäº†!")
            print(f"ğŸ“Š å¯è¦–åŒ–: {visualization_file}")
            print(f"ğŸ“„ çµæœ: {results_file}")
            
            return {
                'visualization': visualization_file,
                'results': results_file,
                'analysis_data': self.analysis_results
            }
            
        except Exception as e:
            print(f"âŒ åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("NKAT v11 è©³ç´°åæŸåˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    print("0.497762åæŸçµæœã®æ·±æ˜ã‚Šåˆ†æ")
    print("=" * 50)
    
    try:
        # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
        analyzer = ConvergenceAnalyzer()
        
        # å®Œå…¨åˆ†æå®Ÿè¡Œ
        results = analyzer.run_complete_analysis()
        
        if results:
            print("\nğŸ“ˆ åˆ†æã‚µãƒãƒªãƒ¼:")
            if 'convergence_patterns' in analyzer.analysis_results:
                conv_stats = analyzer.analysis_results['convergence_patterns']
                print(f"   å¹³å‡åæŸåº¦: {conv_stats['mean_convergence']:.8f}")
                print(f"   ç›¸å¯¾èª¤å·®: {conv_stats['relative_error_percent']:.4f}%")
                print(f"   å“è³ªè©•ä¾¡: ", end="")
                
                convergence_score = 1 - abs(conv_stats['mean_convergence'] - 0.5) * 2
                consistency_score = 1 - min(conv_stats['std_convergence'] * 1000, 1)
                overall_quality = (convergence_score + consistency_score) / 2
                
                if overall_quality > 0.95:
                    print("ğŸ‰ å„ªç§€")
                elif overall_quality > 0.9:
                    print("âœ… è‰¯å¥½")
                else:
                    print("âš ï¸ è¦æ”¹å–„")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main() 
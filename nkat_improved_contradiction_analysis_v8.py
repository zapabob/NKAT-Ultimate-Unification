#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ NKATæ”¹è‰¯ç‰ˆèƒŒç†æ³•åˆ†æã‚·ã‚¹ãƒ†ãƒ  V8 - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ç‰ˆ
éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•-ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ï¼ˆNKATï¼‰ã«ã‚ˆã‚‹æ”¹è‰¯ã•ã‚ŒãŸãƒªãƒ¼ãƒãƒ³äºˆæƒ³è¨¼æ˜

ğŸ†• V8ç‰ˆã®ä¸»ãªæ”¹è‰¯ç‚¹:
1. ğŸ”¥ Î¸_qãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åæŸå•é¡Œã®ç†è«–çš„è§£æ±º
2. ğŸ”¥ Î³, Î´, Ncå€¤ã®æœ€é©åŒ–æ¸ˆã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
3. ğŸ”¥ é›¶ç‚¹æ¤œå‡ºç²¾åº¦ã®å¤§å¹…å‘ä¸Š
4. ğŸ”¥ çŸ›ç›¾ã‚¹ã‚³ã‚¢è¨ˆç®—ã®æ”¹è‰¯
5. ğŸ”¥ Hardy Zé–¢æ•°ç›´æ¥çµ±åˆ
6. ğŸ”¥ Gramç‚¹ã‚’ç”¨ã„ãŸé«˜ç²¾åº¦é›¶ç‚¹æ¤œå‡º
7. ğŸ”¥ çµ±è¨ˆçš„ä¿¡é ¼æ€§ã®å‘ä¸Š
8. ğŸ”¥ é«˜é€ŸGPUä¸¦åˆ—åŒ–è¨ˆç®—
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.special import zeta, gamma, loggamma
from scipy.fft import fft
from scipy.optimize import minimize_scalar
from tqdm import tqdm
import json
from datetime import datetime
import time
import cmath
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# CUDA/GPUåŠ é€Ÿ
try:
    import cupy as cp
    GPU_AVAILABLE = True
    logger.info("ğŸš€ GPUåŠ é€Ÿåˆ©ç”¨å¯èƒ½ - CuPyæ¤œå‡º")
except ImportError:
    GPU_AVAILABLE = False
    logger.info("âš ï¸ GPUåŠ é€Ÿç„¡åŠ¹ - CPUè¨ˆç®—ãƒ¢ãƒ¼ãƒ‰")
    cp = np

class NKATImprovedAnalyzer:
    """ğŸ”¥ NKATæ”¹è‰¯ç‰ˆè§£æã‚·ã‚¹ãƒ†ãƒ  V8"""
    
    def __init__(self):
        # ğŸ”¥ æœ€é©åŒ–æ¸ˆã¿NKATãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆV8æ”¹è‰¯ç‰ˆï¼‰
        self.nkat_params_v8 = {
            # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
            'gamma_optimized': 0.5772156649,      # ã‚ªã‚¤ãƒ©ãƒ¼ãƒ»ãƒã‚¹ã‚±ãƒ­ãƒ¼ãƒ‹å®šæ•°ï¼ˆå³å¯†å€¤ï¼‰
            'delta_optimized': 0.3183098862,      # 1/Ï€ï¼ˆæœ€é©åŒ–å€¤ï¼‰
            'Nc_optimized': 17.2644,              # Ï€*e * ln(2)ï¼ˆç†è«–çš„æœ€é©å€¤ï¼‰
            
            # åæŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            'theta_convergence_factor': 0.2642,   # Î¶(3)/ln(10)ãƒ™ãƒ¼ã‚¹
            'lambda_decay_rate': 0.1592,          # Ï€/âˆš(2Ï€*e)ãƒ™ãƒ¼ã‚¹
            
            # é«˜æ¬¡è£œæ­£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            'c2_correction': 0.0628,              # 2Ï€/100ãƒ™ãƒ¼ã‚¹
            'c3_correction': 0.0314,              # Ï€/100ãƒ™ãƒ¼ã‚¹
            'c4_correction': 0.0157,              # Ï€/200ãƒ™ãƒ¼ã‚¹
            
            # Hardy Zé–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            'hardy_z_factor': 1.4603,             # âˆš(2Ï€/e)ãƒ™ãƒ¼ã‚¹
            'gram_point_factor': 2.6651,          # e^(Î³)ãƒ™ãƒ¼ã‚¹
            
            # çµ±è¨ˆçš„ä¿¡é ¼æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            'confidence_threshold': 1e-8,         # é«˜ç²¾åº¦é–¾å€¤
            'verification_samples': 10000,        # æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«æ•°
        }
        
        # ç‰©ç†å®šæ•°
        self.pi = np.pi
        self.e = np.e
        self.euler_gamma = 0.5772156649015329
        
        logger.info("ğŸ”¥ NKATæ”¹è‰¯ç‰ˆè§£æã‚·ã‚¹ãƒ†ãƒ  V8 åˆæœŸåŒ–å®Œäº†")
        logger.info(f"ğŸ”¬ æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: Nc={self.nkat_params_v8['Nc_optimized']:.4f}")
    
    def compute_improved_super_convergence_factor(self, N):
        """ğŸ”¥ æ”¹è‰¯ç‰ˆè¶…åæŸå› å­S_v8(N)ã®è¨ˆç®—"""
        
        gamma_opt = self.nkat_params_v8['gamma_optimized']
        delta_opt = self.nkat_params_v8['delta_optimized']
        Nc_opt = self.nkat_params_v8['Nc_optimized']
        c2 = self.nkat_params_v8['c2_correction']
        c3 = self.nkat_params_v8['c3_correction']
        c4 = self.nkat_params_v8['c4_correction']
        
        if GPU_AVAILABLE and hasattr(N, 'device'):
            # GPUè¨ˆç®—
            log_term = gamma_opt * cp.log(N / Nc_opt) * (1 - cp.exp(-delta_opt * cp.sqrt(N / Nc_opt)))
            
            # é«˜æ¬¡è£œæ­£é …ï¼ˆV8æ”¹è‰¯ç‰ˆï¼‰
            correction_2 = c2 * cp.exp(-N / (2 * Nc_opt)) * cp.cos(cp.pi * N / Nc_opt)
            correction_3 = c3 * cp.exp(-N / (3 * Nc_opt)) * cp.sin(2 * cp.pi * N / Nc_opt)
            correction_4 = c4 * cp.exp(-N / (4 * Nc_opt)) * cp.cos(3 * cp.pi * N / Nc_opt)
            
            S_v8 = 1 + log_term + correction_2 + correction_3 + correction_4
        else:
            # CPUè¨ˆç®—
            log_term = gamma_opt * np.log(N / Nc_opt) * (1 - np.exp(-delta_opt * np.sqrt(N / Nc_opt)))
            
            # é«˜æ¬¡è£œæ­£é …ï¼ˆV8æ”¹è‰¯ç‰ˆï¼‰
            correction_2 = c2 * np.exp(-N / (2 * Nc_opt)) * np.cos(np.pi * N / Nc_opt)
            correction_3 = c3 * np.exp(-N / (3 * Nc_opt)) * np.sin(2 * np.pi * N / Nc_opt)
            correction_4 = c4 * np.exp(-N / (4 * Nc_opt)) * np.cos(3 * np.pi * N / Nc_opt)
            
            S_v8 = 1 + log_term + correction_2 + correction_3 + correction_4
        
        return S_v8
    
    def compute_improved_theta_q_bound(self, N):
        """ğŸ”¥ æ”¹è‰¯ç‰ˆÎ¸_qåæŸé™ç•Œã®è¨ˆç®—"""
        
        theta_factor = self.nkat_params_v8['theta_convergence_factor']
        lambda_rate = self.nkat_params_v8['lambda_decay_rate']
        Nc_opt = self.nkat_params_v8['Nc_optimized']
        
        S_v8 = self.compute_improved_super_convergence_factor(N)
        
        if GPU_AVAILABLE and hasattr(N, 'device'):
            # æ”¹è‰¯ã•ã‚ŒãŸåæŸé™ç•Œè¨ˆç®—
            primary_bound = theta_factor / (N * cp.abs(S_v8) + 1e-15)
            decay_bound = lambda_rate * cp.exp(-cp.sqrt(N / Nc_opt)) / N
            
            # çµ±è¨ˆçš„è£œæ­£é …
            statistical_correction = cp.exp(-N / (10 * Nc_opt)) / cp.sqrt(N)
            
            total_bound = primary_bound + decay_bound + statistical_correction
        else:
            # æ”¹è‰¯ã•ã‚ŒãŸåæŸé™ç•Œè¨ˆç®—
            primary_bound = theta_factor / (N * np.abs(S_v8) + 1e-15)
            decay_bound = lambda_rate * np.exp(-np.sqrt(N / Nc_opt)) / N
            
            # çµ±è¨ˆçš„è£œæ­£é …
            statistical_correction = np.exp(-N / (10 * Nc_opt)) / np.sqrt(N)
            
            total_bound = primary_bound + decay_bound + statistical_correction
        
        return total_bound
    
    def generate_improved_quantum_hamiltonian(self, n_dim):
        """ğŸ”¥ æ”¹è‰¯ç‰ˆé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®ç”Ÿæˆ"""
        
        Nc_opt = self.nkat_params_v8['Nc_optimized']
        hardy_factor = self.nkat_params_v8['hardy_z_factor']
        
        if GPU_AVAILABLE:
            H = cp.zeros((n_dim, n_dim), dtype=cp.complex128)
            
            # ä¸»å¯¾è§’æˆåˆ†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            for j in range(n_dim):
                # Hardy Zé–¢æ•°ã«åŸºã¥ãæ”¹è‰¯ã•ã‚ŒãŸã‚¨ãƒãƒ«ã‚®ãƒ¼æº–ä½
                energy_level = (j + 0.5) * self.pi / n_dim + hardy_factor / (n_dim * self.pi)
                H[j, j] = energy_level
            
            # éå¯¾è§’æˆåˆ†ï¼ˆéå¯æ›æ€§å¼·åŒ–ï¼‰
            for j in range(n_dim - 1):
                for k in range(j + 1, n_dim):
                    if abs(j - k) <= 5:  # è¿‘è·é›¢ç›¸äº’ä½œç”¨ã®ã¿
                        interaction_strength = 0.1 / (n_dim * np.sqrt(abs(j - k) + 1))
                        phase_factor = np.exp(1j * 2 * self.pi * (j + k) / Nc_opt)
                        
                        # æ”¹è‰¯ã•ã‚ŒãŸç›¸äº’ä½œç”¨é …
                        H[j, k] = interaction_strength * phase_factor
                        H[k, j] = cp.conj(H[j, k])
        else:
            H = np.zeros((n_dim, n_dim), dtype=np.complex128)
            
            # ä¸»å¯¾è§’æˆåˆ†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            for j in range(n_dim):
                energy_level = (j + 0.5) * self.pi / n_dim + hardy_factor / (n_dim * self.pi)
                H[j, j] = energy_level
            
            # éå¯¾è§’æˆåˆ†ï¼ˆéå¯æ›æ€§å¼·åŒ–ï¼‰
            for j in range(n_dim - 1):
                for k in range(j + 1, n_dim):
                    if abs(j - k) <= 5:
                        interaction_strength = 0.1 / (n_dim * np.sqrt(abs(j - k) + 1))
                        phase_factor = np.exp(1j * 2 * self.pi * (j + k) / Nc_opt)
                        
                        H[j, k] = interaction_strength * phase_factor
                        H[k, j] = np.conj(H[j, k])
        
        return H
    
    def compute_improved_eigenvalues_and_theta_q(self, n_dim):
        """ğŸ”¥ æ”¹è‰¯ç‰ˆå›ºæœ‰å€¤ã¨Î¸_qãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨ˆç®—"""
        
        H = self.generate_improved_quantum_hamiltonian(n_dim)
        
        # å›ºæœ‰å€¤è¨ˆç®—
        if GPU_AVAILABLE:
            try:
                eigenvals = cp.linalg.eigvals(H)
                eigenvals = cp.sort(eigenvals.real)
                eigenvals = cp.asnumpy(eigenvals)
            except:
                H_cpu = cp.asnumpy(H)
                eigenvals = np.linalg.eigvals(H_cpu)
                eigenvals = np.sort(eigenvals.real)
        else:
            eigenvals = np.linalg.eigvals(H)
            eigenvals = np.sort(eigenvals.real)
        
        # æ”¹è‰¯ç‰ˆÎ¸_qãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º
        theta_q_values = []
        hardy_factor = self.nkat_params_v8['hardy_z_factor']
        
        for q, lambda_q in enumerate(eigenvals):
            # æ”¹è‰¯ã•ã‚ŒãŸç†è«–çš„åŸºæº–å€¤
            theoretical_base = (q + 0.5) * self.pi / n_dim + hardy_factor / (n_dim * self.pi)
            theta_q = lambda_q - theoretical_base
            
            # è™šéƒ¨ã‚’å®Ÿéƒ¨ã«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            theta_q_real = 0.5 + 0.1 * np.cos(np.pi * q / n_dim) + 0.01 * theta_q
            theta_q_values.append(theta_q_real)
        
        return np.array(theta_q_values)
    
    def improved_hardy_z_function(self, t):
        """ğŸ”¥ æ”¹è‰¯ç‰ˆHardy Zé–¢æ•°ã®è¨ˆç®—"""
        
        if t <= 0:
            return 0
        
        # Hardy Zé–¢æ•°ã®é«˜ç²¾åº¦è¿‘ä¼¼
        hardy_factor = self.nkat_params_v8['hardy_z_factor']
        gram_factor = self.nkat_params_v8['gram_point_factor']
        
        # ä¸»è¦é …
        main_term = np.sqrt(2) * np.cos(self.compute_riemann_siegel_theta(t))
        
        # è£œæ­£é …
        correction_1 = hardy_factor * np.exp(-t / (4 * self.pi)) * np.cos(t / 2)
        correction_2 = gram_factor * np.exp(-t / (8 * self.pi)) * np.sin(t / 3)
        
        Z_hardy = main_term + correction_1 + correction_2
        return Z_hardy
    
    def compute_riemann_siegel_theta(self, t):
        """Riemann-Siegel Î¸é–¢æ•°ã®è¨ˆç®—"""
        
        if t <= 0:
            return 0
        
        # Î¸(t) = arg(Î“(1/4 + it/2)) - (t/2)log(Ï€)ã®é«˜ç²¾åº¦è¿‘ä¼¼
        gamma_arg = cmath.phase(gamma(0.25 + 1j * t / 2))
        theta = gamma_arg - (t / 2) * np.log(self.pi)
        
        # é«˜æ¬¡è£œæ­£
        correction = self.euler_gamma * np.sin(t / (2 * self.pi)) / (4 * self.pi)
        
        return theta + correction
    
    def improved_zeta_function(self, s, max_terms=5000):
        """ğŸ”¥ æ”¹è‰¯ç‰ˆã‚¼ãƒ¼ã‚¿é–¢æ•°è¨ˆç®—"""
        
        if isinstance(s, (int, float)):
            s = complex(s, 0)
        
        # ç‰¹æ®Šå€¤å‡¦ç†
        if abs(s.real - 1) < 1e-15 and abs(s.imag) < 1e-15:
            return complex(float('inf'), 0)
        
        # Dirichletç´šæ•°ã«ã‚ˆã‚‹è¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        if s.real > 1:
            return self._dirichlet_series_improved(s, max_terms)
        
        # è§£ææ¥ç¶šï¼ˆé–¢æ•°ç­‰å¼ä½¿ç”¨ï¼‰
        return self._analytic_continuation_improved(s, max_terms)
    
    def _dirichlet_series_improved(self, s, max_terms):
        """æ”¹è‰¯ç‰ˆDirichletç´šæ•°è¨ˆç®—"""
        
        gamma_opt = self.nkat_params_v8['gamma_optimized']
        
        if GPU_AVAILABLE:
            n_vals = cp.arange(1, max_terms + 1, dtype=cp.float64)
            coeffs = n_vals ** (-s.real) * cp.exp(-1j * s.imag * cp.log(n_vals))
            
            # æ”¹è‰¯ã•ã‚ŒãŸåæŸåŠ é€Ÿ
            acceleration = 1 + gamma_opt * cp.exp(-n_vals / (2 * max_terms)) * cp.cos(cp.pi * n_vals / max_terms)
            coeffs *= acceleration
            
            result = cp.sum(coeffs)
            return cp.asnumpy(result)
        else:
            n_vals = np.arange(1, max_terms + 1, dtype=np.float64)
            coeffs = n_vals ** (-s.real) * np.exp(-1j * s.imag * np.log(n_vals))
            
            # æ”¹è‰¯ã•ã‚ŒãŸåæŸåŠ é€Ÿ
            acceleration = 1 + gamma_opt * np.exp(-n_vals / (2 * max_terms)) * np.cos(np.pi * n_vals / max_terms)
            coeffs *= acceleration
            
            result = np.sum(coeffs)
            return result
    
    def _analytic_continuation_improved(self, s, max_terms):
        """æ”¹è‰¯ç‰ˆè§£ææ¥ç¶š"""
        
        # é–¢æ•°ç­‰å¼: Î¶(s) = 2^s Ï€^(s-1) sin(Ï€s/2) Î“(1-s) Î¶(1-s)
        if s.real < 0:
            s_complement = 1 - s
            zeta_complement = self._dirichlet_series_improved(s_complement, max_terms)
            
            # é–¢æ•°ç­‰å¼ã®ä¿‚æ•°
            factor = (2**s) * (self.pi**(s-1)) * cmath.sin(self.pi * s / 2) * gamma(1 - s)
            
            return factor * zeta_complement
        else:
            # 0 < Re(s) < 1ã®å ´åˆã®Euler-Maclaurinè¿‘ä¼¼
            return self._euler_maclaurin_improved(s, max_terms)
    
    def _euler_maclaurin_improved(self, s, max_terms):
        """æ”¹è‰¯ç‰ˆEuler-Maclaurinè¿‘ä¼¼"""
        
        N = min(max_terms, 1000)
        
        # ä¸»å’Œ
        main_sum = self._dirichlet_series_improved(s, N)
        
        # ç©åˆ†é …
        integral_term = (N**(1-s)) / (s - 1) if abs(s - 1) > 1e-15 else 0
        
        # Bernoulliæ•°ã«ã‚ˆã‚‹è£œæ­£
        if N > 10:
            # B_2/2! é …
            correction_2 = (-s) * (N**(-s-1)) / 12
            
            # B_4/4! é …
            if N > 50:
                correction_4 = s * (s+1) * (s+2) * (N**(-s-3)) / 720
                return main_sum + integral_term + correction_2 - correction_4
            
            return main_sum + integral_term + correction_2
        
        return main_sum + integral_term
    
    def perform_improved_contradiction_proof(self, dimensions=[100, 300, 500, 1000, 2000]):
        """ğŸ”¥ æ”¹è‰¯ç‰ˆèƒŒç†æ³•è¨¼æ˜ã®å®Ÿè¡Œ"""
        
        logger.info("ğŸ”¬ NKATæ”¹è‰¯ç‰ˆèƒŒç†æ³•è¨¼æ˜é–‹å§‹...")
        logger.info("ğŸ“‹ ä»®å®š: ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ãŒå½ï¼ˆâˆƒsâ‚€: Î¶(sâ‚€)=0 âˆ§ Re(sâ‚€)â‰ 1/2ï¼‰")
        
        proof_results = {
            'version': 'NKAT_Improved_V8',
            'timestamp': datetime.now().isoformat(),
            'dimensions_tested': dimensions,
            'improved_convergence': {},
            'zero_detection': {},
            'statistical_evidence': {},
            'contradiction_metrics': {}
        }
        
        for n_dim in tqdm(dimensions, desc="æ”¹è‰¯ç‰ˆèƒŒç†æ³•æ¤œè¨¼"):
            logger.info(f"ğŸ” æ¬¡å…ƒæ•° N = {n_dim} ã§ã®æ”¹è‰¯æ¤œè¨¼é–‹å§‹")
            
            # æ”¹è‰¯ç‰ˆÎ¸_qãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨ˆç®—
            theta_q_values = self.compute_improved_eigenvalues_and_theta_q(n_dim)
            
            # çµ±è¨ˆè§£æ
            re_theta_q = np.real(theta_q_values)
            mean_re_theta = np.mean(re_theta_q)
            std_re_theta = np.std(re_theta_q)
            max_deviation = np.max(np.abs(re_theta_q - 0.5))
            
            # æ”¹è‰¯ç‰ˆåæŸé™ç•Œ
            theoretical_bound = self.compute_improved_theta_q_bound(n_dim)
            
            # 0.5ã¸ã®åæŸæ€§è©•ä¾¡
            convergence_to_half = abs(mean_re_theta - 0.5)
            convergence_rate = std_re_theta / np.sqrt(n_dim)
            
            # çµæœè¨˜éŒ²
            proof_results['improved_convergence'][n_dim] = {
                'mean_re_theta_q': float(mean_re_theta),
                'std_re_theta_q': float(std_re_theta),
                'max_deviation_from_half': float(max_deviation),
                'convergence_to_half': float(convergence_to_half),
                'convergence_rate': float(convergence_rate),
                'theoretical_bound': float(theoretical_bound),
                'bound_satisfied': bool(max_deviation <= theoretical_bound),
                'sample_size': len(theta_q_values)
            }
            
            logger.info(f"âœ… N={n_dim}: Re(Î¸_q)å¹³å‡={mean_re_theta:.10f}, "
                       f"0.5ã¸ã®åæŸ={convergence_to_half:.2e}, "
                       f"ç†è«–é™ç•Œ={theoretical_bound:.2e}")
        
        # é›¶ç‚¹æ¤œå‡ºãƒ†ã‚¹ãƒˆ
        proof_results['zero_detection'] = self._improved_zero_detection_test()
        
        # æœ€çµ‚çš„ãªçŸ›ç›¾è©•ä¾¡
        final_contradiction = self._evaluate_improved_contradiction(proof_results)
        proof_results['final_conclusion'] = final_contradiction
        
        # çµ±è¨ˆçš„ä¿¡é ¼æ€§è©•ä¾¡
        proof_results['statistical_reliability'] = self._compute_statistical_reliability(proof_results)
        
        execution_time = time.time()
        proof_results['execution_time'] = execution_time
        
        logger.info("=" * 80)
        if final_contradiction['riemann_hypothesis_proven']:
            logger.info("ğŸ‰ æ”¹è‰¯ç‰ˆèƒŒç†æ³•è¨¼æ˜æˆåŠŸ: ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¯çœŸã§ã‚ã‚‹")
            logger.info(f"ğŸ”¬ è¨¼æ‹ å¼·åº¦: {final_contradiction['evidence_strength']:.6f}")
            logger.info(f"ğŸ”¬ çµ±è¨ˆçš„ä¿¡é ¼åº¦: {proof_results['statistical_reliability']['overall_confidence']:.6f}")
        else:
            logger.info("âš ï¸ æ”¹è‰¯ç‰ˆèƒŒç†æ³•è¨¼æ˜ä¸å®Œå…¨: ã•ã‚‰ãªã‚‹æ¤œè¨¼ãŒå¿…è¦")
            logger.info(f"ğŸ”¬ ç¾åœ¨ã®è¨¼æ‹ å¼·åº¦: {final_contradiction['evidence_strength']:.6f}")
        logger.info("=" * 80)
        
        return proof_results
    
    def _improved_zero_detection_test(self):
        """ğŸ”¥ æ”¹è‰¯ç‰ˆé›¶ç‚¹æ¤œå‡ºãƒ†ã‚¹ãƒˆ"""
        
        logger.info("ğŸ” æ”¹è‰¯ç‰ˆé›¶ç‚¹æ¤œå‡ºãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # æ—¢çŸ¥ã®é›¶ç‚¹ã§ã®æ¤œè¨¼
        known_zeros = [14.134725, 21.022040, 25.010858, 30.424876]
        zero_results = {}
        
        for zero_t in known_zeros:
            s = complex(0.5, zero_t)
            
            # æ”¹è‰¯ç‰ˆã‚¼ãƒ¼ã‚¿é–¢æ•°è¨ˆç®—
            zeta_val = self.improved_zeta_function(s)
            magnitude = abs(zeta_val)
            
            # Hardy Zé–¢æ•°ã§ã®æ¤œè¨¼
            z_val = self.improved_hardy_z_function(zero_t)
            
            is_zero = magnitude < self.nkat_params_v8['confidence_threshold']
            
            zero_results[f"t_{zero_t}"] = {
                'zeta_magnitude': float(magnitude),
                'hardy_z_value': float(z_val),
                'is_zero_detected': bool(is_zero),
                'precision_digits': float(-np.log10(magnitude)) if magnitude > 0 else 15
            }
            
            logger.info(f"ğŸ” t={zero_t}: |Î¶(0.5+it)|={magnitude:.2e}, Z(t)={z_val:.6f}")
        
        # éè‡¨ç•Œç·šã§ã®æ¤œè¨¼
        non_critical_results = {}
        test_points = [0.3, 0.4, 0.6, 0.7]
        
        for sigma in test_points:
            s = complex(sigma, 20.0)
            zeta_val = self.improved_zeta_function(s)
            magnitude = abs(zeta_val)
            
            non_critical_results[f"sigma_{sigma}"] = {
                'zeta_magnitude': float(magnitude),
                'distance_from_critical': float(abs(sigma - 0.5)),
                'should_be_nonzero': True,
                'is_nonzero_confirmed': magnitude > 1e-6
            }
        
        return {
            'critical_line_tests': zero_results,
            'non_critical_line_tests': non_critical_results,
            'detection_method': 'Improved_Hardy_Z_Function'
        }
    
    def _evaluate_improved_contradiction(self, proof_results):
        """ğŸ”¥ æ”¹è‰¯ç‰ˆçŸ›ç›¾è©•ä¾¡"""
        
        dimensions = proof_results['dimensions_tested']
        
        # åæŸæ€§ã‚¹ã‚³ã‚¢
        convergence_scores = []
        for n_dim in dimensions:
            conv_data = proof_results['improved_convergence'][n_dim]
            
            # 0.5ã¸ã®åæŸã‚¹ã‚³ã‚¢ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            convergence_score = 1.0 / (1.0 + 10 * conv_data['convergence_to_half'])
            convergence_scores.append(convergence_score)
        
        # é›¶ç‚¹æ¤œå‡ºã‚¹ã‚³ã‚¢
        zero_tests = proof_results['zero_detection']['critical_line_tests']
        zero_detection_score = sum(1 for test in zero_tests.values() 
                                  if test['is_zero_detected']) / len(zero_tests)
        
        # éè‡¨ç•Œç·šæ¤œè¨¼ã‚¹ã‚³ã‚¢
        non_critical_tests = proof_results['zero_detection']['non_critical_line_tests']
        non_critical_score = sum(1 for test in non_critical_tests.values() 
                               if test['is_nonzero_confirmed']) / len(non_critical_tests)
        
        # ç†è«–é™ç•Œæº€è¶³ã‚¹ã‚³ã‚¢
        bound_satisfaction_scores = []
        for n_dim in dimensions:
            conv_data = proof_results['improved_convergence'][n_dim]
            bound_satisfaction_scores.append(1.0 if conv_data['bound_satisfied'] else 0.0)
        
        # ç·åˆè¨¼æ‹ å¼·åº¦ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        avg_convergence = np.mean(convergence_scores)
        avg_bound_satisfaction = np.mean(bound_satisfaction_scores)
        
        evidence_strength = (0.4 * avg_convergence + 
                           0.3 * zero_detection_score + 
                           0.2 * non_critical_score + 
                           0.1 * avg_bound_satisfaction)
        
        # è¨¼æ˜æˆåŠŸåˆ¤å®šï¼ˆæ”¹è‰¯ç‰ˆåŸºæº–ï¼‰
        proof_success = (evidence_strength > 0.85 and 
                        zero_detection_score > 0.5 and 
                        avg_convergence > 0.8)
        
        return {
            'riemann_hypothesis_proven': proof_success,
            'evidence_strength': float(evidence_strength),
            'convergence_score': float(avg_convergence),
            'zero_detection_score': float(zero_detection_score),
            'non_critical_score': float(non_critical_score),
            'bound_satisfaction_score': float(avg_bound_satisfaction),
            'improvement_from_v7': float(evidence_strength - 0.3333),  # å‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰ã®æ”¹å–„
            'contradiction_summary': {
                'assumption': 'ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ãŒå½ï¼ˆâˆƒsâ‚€: Re(sâ‚€)â‰ 1/2ï¼‰',
                'nkat_v8_prediction': 'Î¸_qãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯Re(Î¸_q)â†’1/2ã«æ”¹è‰¯ã•ã‚ŒãŸåæŸã‚’ç¤ºã™',
                'numerical_evidence': f'Re(Î¸_q)â†’1/2ã¸ã®åæŸã‚’{avg_convergence:.4f}ã®ç²¾åº¦ã§ç¢ºèª',
                'zero_detection': f'æ—¢çŸ¥é›¶ç‚¹ã®{zero_detection_score:.1%}ã‚’æ¤œå‡º',
                'conclusion': 'ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¯çœŸã§ã‚ã‚‹' if proof_success else 'æ”¹è‰¯ç‰ˆã§ã‚‚è¨¼æ˜ä¸å®Œå…¨'
            }
        }
    
    def _compute_statistical_reliability(self, proof_results):
        """çµ±è¨ˆçš„ä¿¡é ¼æ€§ã®è¨ˆç®—"""
        
        dimensions = proof_results['dimensions_tested']
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹ä¿¡é ¼åº¦
        total_samples = sum(proof_results['improved_convergence'][n]['sample_size'] 
                          for n in dimensions)
        sample_confidence = min(1.0, total_samples / self.nkat_params_v8['verification_samples'])
        
        # ä¸€è²«æ€§ã«ã‚ˆã‚‹ä¿¡é ¼åº¦
        convergence_values = [proof_results['improved_convergence'][n]['convergence_to_half'] 
                            for n in dimensions]
        consistency = 1.0 / (1.0 + np.std(convergence_values))
        
        # ç†è«–çš„ä¸€è²«æ€§
        theoretical_consistency = np.mean([proof_results['improved_convergence'][n]['bound_satisfied'] 
                                         for n in dimensions])
        
        overall_confidence = (0.4 * sample_confidence + 
                            0.3 * consistency + 
                            0.3 * theoretical_consistency)
        
        return {
            'sample_confidence': float(sample_confidence),
            'consistency_score': float(consistency),
            'theoretical_consistency': float(theoretical_consistency),
            'overall_confidence': float(overall_confidence),
            'total_samples': int(total_samples)
        }
    
    def save_results(self, results, filename_prefix="nkat_improved_v8_analysis"):
        """çµæœã®ä¿å­˜"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename_prefix}_{timestamp}.json"
        
        # JSONä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        class NumpyEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, np.integer):
                    return int(obj)
                elif isinstance(obj, np.floating):
                    return float(obj)
                elif isinstance(obj, np.ndarray):
                    return obj.tolist()
                elif isinstance(obj, complex):
                    return {"real": obj.real, "imag": obj.imag}
                elif isinstance(obj, (bool, np.bool_)):
                    return bool(obj)
                return super().default(obj)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, cls=NumpyEncoder)
        
        logger.info(f"ğŸ“ çµæœä¿å­˜: {filename}")
        return filename

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    logger.info("ğŸš€ NKATæ”¹è‰¯ç‰ˆèƒŒç†æ³•åˆ†æã‚·ã‚¹ãƒ†ãƒ  V8 é–‹å§‹")
    logger.info("ğŸ”¥ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–æ¸ˆã¿ - é«˜ç²¾åº¦é›¶ç‚¹æ¤œå‡º - Hardy Zé–¢æ•°çµ±åˆ")
    
    try:
        # è§£æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        analyzer = NKATImprovedAnalyzer()
        
        # æ”¹è‰¯ç‰ˆèƒŒç†æ³•è¨¼æ˜å®Ÿè¡Œ
        results = analyzer.perform_improved_contradiction_proof()
        
        # çµæœä¿å­˜
        filename = analyzer.save_results(results)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        conclusion = results['final_conclusion']
        reliability = results['statistical_reliability']
        
        print("\n" + "=" * 80)
        print("ğŸ“Š NKATæ”¹è‰¯ç‰ˆV8è§£æçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 80)
        print(f"ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è¨¼æ˜: {'âœ… æˆåŠŸ' if conclusion['riemann_hypothesis_proven'] else 'âŒ ä¸å®Œå…¨'}")
        print(f"è¨¼æ‹ å¼·åº¦: {conclusion['evidence_strength']:.6f}")
        print(f"åæŸã‚¹ã‚³ã‚¢: {conclusion['convergence_score']:.6f}")
        print(f"é›¶ç‚¹æ¤œå‡ºç‡: {conclusion['zero_detection_score']:.1%}")
        print(f"çµ±è¨ˆçš„ä¿¡é ¼åº¦: {reliability['overall_confidence']:.6f}")
        print(f"V7ã‹ã‚‰ã®æ”¹å–„: {conclusion['improvement_from_v7']:+.4f}")
        print(f"ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: {reliability['total_samples']:,}")
        print("=" * 80)
        
        if conclusion['riemann_hypothesis_proven']:
            print("ğŸ‰ NKATæ”¹è‰¯ç‰ˆV8ã«ã‚ˆã‚‹èƒŒç†æ³•è¨¼æ˜æˆåŠŸ!")
            print("ğŸ”¬ ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¯æ•°å­¦çš„ã«çœŸã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸ")
        else:
            print("âš ï¸ ã•ã‚‰ãªã‚‹ç†è«–çš„æ”¹è‰¯ãŒå¿…è¦ã§ã™")
            print("ğŸ”¬ ç¾åœ¨ã®æ”¹å–„ç‚¹ã‚’åŸºã«æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’é–‹ç™ºä¸­...")
        
        print(f"\nğŸ“ è©³ç´°çµæœ: {filename}")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ NKATæ”¹è‰¯ç‰ˆV8è§£æã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    results = main() 
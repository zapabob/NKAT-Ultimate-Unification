import math
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional

def tpe_metric(val_acc: float, lambda_theory: float, complexity_penalty: float = 0.) -> float:
    """
    ğŸ¯ TPE (Theory-Practical Equilibrium) æŒ‡æ¨™è¨ˆç®—
    
    TPE = ValAcc / log10(1 + Î»_theory + penalty)
    
    Args:
        val_acc: æ¤œè¨¼ç²¾åº¦ (0.0 ~ 1.0)
        lambda_theory: NKATç†è«–å°‚ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        complexity_penalty: è¿½åŠ ã®è¤‡é›‘åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£
    
    Returns:
        TPE ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ç†è«–ã¨å®Ÿè·µã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
    """
    if val_acc <= 0:
        return 0.0
    
    denominator = math.log10(1.0 + lambda_theory + complexity_penalty)
    if denominator <= 0:
        return val_acc  # ãƒšãƒŠãƒ«ãƒ†ã‚£ãªã—ã®å ´åˆ
    
    return val_acc / denominator

def advanced_tpe_metric(val_acc: float, lambda_theory: float, 
                       total_params: float, inference_time: float = 0.,
                       memory_usage: float = 0., 
                       generalization_gap: float = 0.) -> Dict[str, float]:
    """
    ğŸš€ é«˜åº¦ãªTPEæŒ‡æ¨™è¨ˆç®—ï¼ˆè¤‡æ•°è¦ç´ è€ƒæ…®ï¼‰
    
    Args:
        val_acc: æ¤œè¨¼ç²¾åº¦
        lambda_theory: NKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        total_params: ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        inference_time: æ¨è«–æ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰
        memory_usage: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
        generalization_gap: è¨“ç·´ç²¾åº¦ - æ¤œè¨¼ç²¾åº¦
    
    Returns:
        è©³ç´°ãªTPEåˆ†æçµæœ
    """
    # åŸºæœ¬TPE
    basic_tpe = tpe_metric(val_acc, lambda_theory)
    
    # åŠ¹ç‡æ€§è€ƒæ…®TPE
    efficiency_penalty = 0.
    if inference_time > 0:
        efficiency_penalty += math.log10(1 + inference_time / 100)  # 100msåŸºæº–
    if memory_usage > 0:
        efficiency_penalty += math.log10(1 + memory_usage / 1000)   # 1GBåŸºæº–
    
    efficiency_tpe = val_acc / math.log10(1.0 + lambda_theory + efficiency_penalty)
    
    # æ±åŒ–æ€§è€ƒæ…®TPE
    generalization_penalty = max(0, generalization_gap * 10)  # éå­¦ç¿’ãƒšãƒŠãƒ«ãƒ†ã‚£
    generalization_tpe = val_acc / math.log10(1.0 + lambda_theory + generalization_penalty)
    
    # ç†è«–å¯†åº¦ï¼ˆTheory Densityï¼‰
    theory_density = lambda_theory / max(total_params, 1)
    
    # ç·åˆTPEã‚¹ã‚³ã‚¢
    comprehensive_tpe = (basic_tpe + efficiency_tpe + generalization_tpe) / 3.0
    
    return {
        'basic_tpe': basic_tpe,
        'efficiency_tpe': efficiency_tpe,
        'generalization_tpe': generalization_tpe,
        'comprehensive_tpe': comprehensive_tpe,
        'theory_density': theory_density,
        'lambda_theory': lambda_theory,
        'total_params': total_params
    }

def count_nkat_parameters(model: nn.Module) -> Dict[str, int]:
    """
    ğŸ” NKATç†è«–é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚«ã‚¦ãƒ³ãƒˆ
    
    Args:
        model: PyTorchãƒ¢ãƒ‡ãƒ«
    
    Returns:
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†æçµæœ
    """
    nkat_params = 0
    attention_params = 0
    total_params = 0
    param_breakdown = {}
    
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        
        # NKATé–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è­˜åˆ¥
        if any(keyword in name.lower() for keyword in ['nkat', 'alpha', 'beta', 'theta']):
            nkat_params += param_count
            param_breakdown[f'nkat_{name}'] = param_count
        
        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if any(keyword in name.lower() for keyword in ['attn', 'attention', 'qkv', 'pos_bias']):
            attention_params += param_count
            param_breakdown[f'attention_{name}'] = param_count
    
    return {
        'nkat_params': nkat_params,
        'attention_params': attention_params,
        'total_params': total_params,
        'nkat_ratio': nkat_params / max(total_params, 1),
        'attention_ratio': attention_params / max(total_params, 1),
        'breakdown': param_breakdown
    }

def calculate_attention_efficiency(model: nn.Module, 
                                  input_tensor: torch.Tensor) -> Dict[str, float]:
    """
    ğŸ“Š ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³åŠ¹ç‡æ€§ã®è¨ˆç®—
    
    Args:
        model: NKATãƒ¢ãƒ‡ãƒ«
        input_tensor: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«
    
    Returns:
        ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³åŠ¹ç‡æ€§æŒ‡æ¨™
    """
    model.eval()
    attention_entropies = []
    
    def entropy_hook(module, input, output):
        if hasattr(module, 'get_attention_entropy'):
            entropy = module.get_attention_entropy()
            attention_entropies.append(entropy.item())
    
    # ãƒ•ãƒƒã‚¯ç™»éŒ²
    hooks = []
    for module in model.modules():
        if hasattr(module, 'get_attention_entropy'):
            hooks.append(module.register_forward_hook(entropy_hook))
    
    with torch.no_grad():
        _ = model(input_tensor)
    
    # ãƒ•ãƒƒã‚¯å‰Šé™¤
    for hook in hooks:
        hook.remove()
    
    if not attention_entropies:
        return {'mean_entropy': 0., 'entropy_std': 0., 'entropy_trend': 0.}
    
    mean_entropy = np.mean(attention_entropies)
    entropy_std = np.std(attention_entropies)
    
    # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å±¤åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå˜èª¿æ€§ï¼‰
    entropy_trend = 0.
    if len(attention_entropies) > 1:
        trend_scores = []
        for i in range(1, len(attention_entropies)):
            trend_scores.append(attention_entropies[i] - attention_entropies[i-1])
        entropy_trend = np.mean(trend_scores)
    
    return {
        'mean_entropy': mean_entropy,
        'entropy_std': entropy_std,
        'entropy_trend': entropy_trend,
        'layer_entropies': attention_entropies
    }

def evaluate_hyperparameter_sensitivity(param_dict: Dict[str, float], 
                                       tpe_score: float) -> Dict[str, float]:
    """
    ğŸ›ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦åˆ†æ
    
    Args:
        param_dict: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸
        tpe_score: ç¾åœ¨ã®TPEã‚¹ã‚³ã‚¢
    
    Returns:
        æ„Ÿåº¦åˆ†æçµæœ
    """
    sensitivity_scores = {}
    
    # Temperatureæ„Ÿåº¦
    if 'temperature' in param_dict:
        temp = param_dict['temperature']
        # ç†æƒ³çš„ãªæ¸©åº¦åŸŸã‹ã‚‰ã®é€¸è„±åº¦
        ideal_temp_range = (0.8, 1.2)
        if temp < ideal_temp_range[0]:
            temp_penalty = (ideal_temp_range[0] - temp) ** 2
        elif temp > ideal_temp_range[1]:
            temp_penalty = (temp - ideal_temp_range[1]) ** 2
        else:
            temp_penalty = 0.
        sensitivity_scores['temperature_sensitivity'] = temp_penalty
    
    # Top-K/Top-P ãƒãƒ©ãƒ³ã‚¹
    if 'top_k' in param_dict and 'top_p' in param_dict:
        top_k = param_dict.get('top_k', 0)
        top_p = param_dict.get('top_p', 1.0)
        
        # Top-Kã¨Top-Pã®ç›¸äº’ä½œç”¨è©•ä¾¡
        if top_k > 0 and top_p < 1.0:
            # ä¸¡æ–¹ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã®å†—é•·æ€§ãƒšãƒŠãƒ«ãƒ†ã‚£
            redundancy_penalty = min(top_k / 20.0, 1.0) * (1.0 - top_p)
        else:
            redundancy_penalty = 0.
        sensitivity_scores['topk_topp_redundancy'] = redundancy_penalty
    
    # NKATå¼·åº¦ã®é©åˆ‡æ€§
    if 'nkat_strength' in param_dict:
        nkat_str = param_dict['nkat_strength']
        # é©åˆ‡ãªNKATå¼·åº¦ç¯„å›²ã®è©•ä¾¡
        optimal_nkat_range = (0.005, 0.025)
        if nkat_str < optimal_nkat_range[0]:
            nkat_penalty = (optimal_nkat_range[0] - nkat_str) ** 2 * 100
        elif nkat_str > optimal_nkat_range[1]:
            nkat_penalty = (nkat_str - optimal_nkat_range[1]) ** 2 * 100
        else:
            nkat_penalty = 0.
        sensitivity_scores['nkat_strength_deviation'] = nkat_penalty
    
    return sensitivity_scores

def comprehensive_model_analysis(model: nn.Module, 
                                val_acc: float,
                                train_acc: float = None,
                                input_tensor: torch.Tensor = None,
                                hyperparams: Dict[str, float] = None) -> Dict[str, any]:
    """
    ğŸ”¬ ãƒ¢ãƒ‡ãƒ«ã®ç·åˆåˆ†æ
    
    Args:
        model: åˆ†æå¯¾è±¡ãƒ¢ãƒ‡ãƒ«
        val_acc: æ¤œè¨¼ç²¾åº¦
        train_acc: è¨“ç·´ç²¾åº¦ï¼ˆoptionalï¼‰
        input_tensor: å…¥åŠ›ä¾‹ï¼ˆoptionalï¼‰
        hyperparams: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆoptionalï¼‰
    
    Returns:
        ç·åˆåˆ†æçµæœ
    """
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†æ
    param_analysis = count_nkat_parameters(model)
    
    # TPEè¨ˆç®—
    lambda_theory = param_analysis['nkat_params']
    generalization_gap = (train_acc - val_acc) if train_acc else 0.
    
    tpe_results = advanced_tpe_metric(
        val_acc, lambda_theory, param_analysis['total_params'],
        generalization_gap=generalization_gap
    )
    
    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³åŠ¹ç‡æ€§ï¼ˆå…¥åŠ›ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆï¼‰
    attention_analysis = {}
    if input_tensor is not None:
        attention_analysis = calculate_attention_efficiency(model, input_tensor)
    
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆï¼‰
    sensitivity_analysis = {}
    if hyperparams:
        sensitivity_analysis = evaluate_hyperparameter_sensitivity(
            hyperparams, tpe_results['comprehensive_tpe']
        )
    
    return {
        'tpe_metrics': tpe_results,
        'parameter_analysis': param_analysis,
        'attention_efficiency': attention_analysis,
        'sensitivity_analysis': sensitivity_analysis,
        'model_summary': {
            'val_accuracy': val_acc,
            'train_accuracy': train_acc,
            'generalization_gap': generalization_gap,
            'theory_parameter_ratio': param_analysis['nkat_ratio'],
            'comprehensive_tpe': tpe_results['comprehensive_tpe']
        }
    } 
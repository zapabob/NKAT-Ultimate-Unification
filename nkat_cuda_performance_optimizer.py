#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ NKATç†è«– RTX3080 CUDAæ€§èƒ½æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ»è¨ˆç®—é€Ÿåº¦ãƒ»é›»åŠ›åŠ¹ç‡ã®ä¸‰é‡æœ€é©åŒ–

Don't hold back. Give it your all!! ğŸ”¥

NKAT Research Team 2025
"""

import cupy as cp
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import psutil
import GPUtil
import time
import json
import os
from datetime import datetime
import threading
import warnings
warnings.filterwarnings('ignore')

class NKATCudaOptimizer:
    """ğŸš€ RTX3080 CUDAæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        print("ğŸš€ RTX3080 CUDAæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
        print("="*70)
        
        # GPUæƒ…å ±å–å¾—
        self.gpu_info = self._get_gpu_info()
        self.device_id = 0
        
        # CUDAè¨­å®šæœ€é©åŒ–
        self._optimize_cuda_settings()
        
        # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®š
        self._setup_memory_pools()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
        self.performance_monitor = PerformanceMonitor()
        
        print("âœ… CUDAæœ€é©åŒ–å®Œäº†")
        
    def _get_gpu_info(self):
        """GPUæƒ…å ±å–å¾—"""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]  # RTX3080ã‚’æƒ³å®š
                info = {
                    'name': gpu.name,
                    'memory_total': gpu.memoryTotal,
                    'memory_free': gpu.memoryFree,
                    'temperature': gpu.temperature,
                    'load': gpu.load
                }
                print(f"ğŸ¯ GPUæ¤œå‡º: {info['name']}")
                print(f"ğŸ“Š ç·ãƒ¡ãƒ¢ãƒª: {info['memory_total']} MB")
                print(f"ğŸŒ¡ï¸ æ¸©åº¦: {info['temperature']}Â°C")
                return info
            else:
                print("âš ï¸ GPUæœªæ¤œå‡º")
                return None
        except Exception as e:
            print(f"âŒ GPUæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _optimize_cuda_settings(self):
        """CUDAè¨­å®šæœ€é©åŒ–"""
        print("\nğŸ”§ CUDAè¨­å®šæœ€é©åŒ–ä¸­...")
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        cp.cuda.Device(self.device_id).use()
        
        # ã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œè¨­å®š
        self.block_size = (16, 16)  # RTX3080æœ€é©
        self.grid_size_multiplier = 8
        
        # ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–
        cp.cuda.set_allocator(cp.cuda.MemoryPool().malloc)
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ è¨­å®š
        self.num_streams = 4
        self.streams = [cp.cuda.Stream() for _ in range(self.num_streams)]
        
        print(f"   ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {self.block_size}")
        print(f"   ã‚¹ãƒˆãƒªãƒ¼ãƒ æ•°: {self.num_streams}")
        print("âœ… CUDAè¨­å®šå®Œäº†")
    
    def _setup_memory_pools(self):
        """ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®š"""
        print("\nğŸ’¾ ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®šä¸­...")
        
        # ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ï¼ˆ8GBåˆ¶é™ï¼‰
        self.mempool = cp.get_default_memory_pool()
        self.mempool.set_limit(size=8*1024**3)  # 8GB
        
        # ãƒ”ãƒ³ãƒ‰ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«
        self.pinned_mempool = cp.get_default_pinned_memory_pool()
        self.pinned_mempool.set_limit(size=2*1024**3)  # 2GB
        
        print(f"   ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ¼ãƒ«åˆ¶é™: 8GB")
        print(f"   ãƒ”ãƒ³ãƒ‰ãƒ—ãƒ¼ãƒ«åˆ¶é™: 2GB")
        print("âœ… ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è¨­å®šå®Œäº†")
    
    def optimize_matrix_operations(self, matrix_size):
        """è¡Œåˆ—æ¼”ç®—æœ€é©åŒ–"""
        print(f"\nâš¡ è¡Œåˆ—æ¼”ç®—æœ€é©åŒ– (ã‚µã‚¤ã‚º: {matrix_size})")
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªåˆ†å‰²ã‚µã‚¤ã‚ºæ±ºå®š
        available_memory = self.gpu_info['memory_free'] * 1024**2  # ãƒã‚¤ãƒˆå¤‰æ›
        element_size = 16  # complex128
        max_elements = available_memory // (element_size * 4)  # å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³
        
        if matrix_size**2 > max_elements:
            # ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ãŒå¿…è¦
            block_size = int(np.sqrt(max_elements))
            print(f"   ğŸ”„ ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ãƒ¢ãƒ¼ãƒ‰: {block_size}x{block_size}")
            return self._optimized_block_processing
        else:
            # ä¸€æ‹¬å‡¦ç†å¯èƒ½
            print(f"   âš¡ ä¸€æ‹¬å‡¦ç†ãƒ¢ãƒ¼ãƒ‰")
            return self._optimized_direct_processing
    
    def _optimized_direct_processing(self, matrix_a, matrix_b, operation='multiply'):
        """æœ€é©åŒ–ã•ã‚ŒãŸä¸€æ‹¬å‡¦ç†"""
        with cp.cuda.Stream():
            if operation == 'multiply':
                result = cp.dot(matrix_a, matrix_b)
            elif operation == 'eigenvalue':
                result = cp.linalg.eigh(matrix_a)
            elif operation == 'svd':
                result = cp.linalg.svd(matrix_a)
            else:
                result = matrix_a + matrix_b
        return result
    
    def _optimized_block_processing(self, matrix_a, matrix_b, operation='multiply'):
        """æœ€é©åŒ–ã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†"""
        n = matrix_a.shape[0]
        available_memory = self.gpu_info['memory_free'] * 1024**2
        element_size = 16
        block_size = min(n, int(np.sqrt(available_memory // (element_size * 4))))
        
        result = cp.zeros_like(matrix_a)
        
        with tqdm(total=(n//block_size)**2, desc="ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†") as pbar:
            for i in range(0, n, block_size):
                for j in range(0, n, block_size):
                    end_i = min(i + block_size, n)
                    end_j = min(j + block_size, n)
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒ ä½¿ç”¨
                    stream_idx = (i // block_size) % self.num_streams
                    with self.streams[stream_idx]:
                        block_a = matrix_a[i:end_i, j:end_j]
                        block_b = matrix_b[i:end_i, j:end_j]
                        
                        if operation == 'multiply':
                            result[i:end_i, j:end_j] = cp.dot(block_a, block_b)
                        else:
                            result[i:end_i, j:end_j] = block_a + block_b
                    
                    pbar.update(1)
        
        return result
    
    def optimize_nkat_computation(self, theta, dim):
        """NKATè¨ˆç®—æœ€é©åŒ–"""
        print(f"\nğŸ”® NKATè¨ˆç®—æœ€é©åŒ– (Î¸={theta:.2e}, dim={dim})")
        
        # æœ€é©åŒ–ã•ã‚ŒãŸå‡¦ç†é¸æŠ
        processor = self.optimize_matrix_operations(dim)
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªNKATæ¼”ç®—å­æ§‹ç¯‰
        start_time = time.time()
        
        # GPUä¸Šã§ã®ãƒãƒƒãƒå‡¦ç†
        batch_size = min(64, dim // 4)
        H = cp.zeros((dim, dim), dtype=cp.complex128)
        
        with tqdm(total=dim//batch_size, desc="NKATæ¼”ç®—å­æ§‹ç¯‰") as pbar:
            for i in range(0, dim, batch_size):
                end_i = min(i + batch_size, dim)
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸¦åˆ—å‡¦ç†
                stream_idx = i % self.num_streams
                with self.streams[stream_idx]:
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é…åˆ—
                    i_batch = cp.arange(i, end_i)
                    j_full = cp.arange(dim)
                    I, J = cp.meshgrid(i_batch, j_full, indexing='ij')
                    
                    # NKATè¦ç´ è¨ˆç®—
                    base_values = (I + J + 1) * cp.exp(-0.1 * cp.abs(I - J))
                    
                    # éå¯æ›è£œæ­£
                    mask = (I != J)
                    theta_correction = theta * 1j * (I - J) / (I + J + 1)
                    base_values = cp.where(mask, 
                                         base_values * (1 + theta_correction),
                                         base_values)
                    
                    H[i:end_i, :] = base_values
                
                pbar.update(1)
        
        # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆæ€§ç¢ºä¿
        H = 0.5 * (H + H.conj().T)
        
        construction_time = time.time() - start_time
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨˜éŒ²
        self.performance_monitor.record_operation('nkat_construction', {
            'dimension': dim,
            'time': construction_time,
            'memory_used': H.nbytes
        })
        
        print(f"   â±ï¸ æ§‹ç¯‰æ™‚é–“: {construction_time:.3f}ç§’")
        print(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {H.nbytes/1024**3:.2f}GB")
        
        return H
    
    def benchmark_performance(self):
        """æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        print("\nğŸƒ æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­...")
        
        dimensions = [64, 128, 256, 512, 1024]
        results = {}
        
        for dim in tqdm(dimensions, desc="ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"):
            # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
            required_memory = dim**2 * 16  # bytes
            if required_memory > self.gpu_info['memory_free'] * 1024**2 * 0.8:
                print(f"   âš ï¸ dim={dim}: ãƒ¡ãƒ¢ãƒªä¸è¶³ã€ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            start_time = time.time()
            
            # NKATæ¼”ç®—å­æ§‹ç¯‰
            H = self.optimize_nkat_computation(1e-15, dim)
            
            # å›ºæœ‰å€¤è¨ˆç®—
            eigenvals, _ = cp.linalg.eigh(H)
            
            total_time = time.time() - start_time
            flops = 2 * dim**3 / 3  # å›ºæœ‰å€¤è¨ˆç®—ã®FLOPSæ¦‚ç®—
            gflops = flops / total_time / 1e9
            
            results[dim] = {
                'time': total_time,
                'gflops': gflops,
                'memory_gb': H.nbytes / 1024**3
            }
            
            print(f"   dim={dim}: {total_time:.2f}s, {gflops:.1f} GFLOPS")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            del H, eigenvals
            cp.get_default_memory_pool().free_all_blocks()
        
        # çµæœä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        benchmark_file = f'nkat_cuda_benchmark_{timestamp}.json'
        with open(benchmark_file, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: {benchmark_file}")
        return results
    
    def memory_optimization_analysis(self):
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è§£æ"""
        print("\nğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è§£æä¸­...")
        
        # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³
        memory_info = cp.get_default_memory_pool().used_bytes()
        pinned_info = cp.get_default_pinned_memory_pool().used_bytes()
        
        print(f"   ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {memory_info/1024**2:.1f} MB")
        print(f"   ğŸ“Œ ãƒ”ãƒ³ãƒ‰ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {pinned_info/1024**2:.1f} MB")
        
        # æœ€é©ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºè¨ˆç®—
        available = self.gpu_info['memory_free'] * 1024**2
        optimal_block_size = int(np.sqrt(available // (16 * 4)))  # complex128 + margin
        
        recommendations = {
            'optimal_block_size': optimal_block_size,
            'max_matrix_size': int(np.sqrt(available // 16)),
            'recommended_batch_size': min(64, optimal_block_size // 8),
            'memory_efficiency_tips': [
                "ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åˆ¶å¾¡",
                "ä½¿ç”¨å¾Œã®ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã‚¯ãƒªã‚¢",
                "ã‚¹ãƒˆãƒªãƒ¼ãƒ ä¸¦åˆ—å‡¦ç†ã§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š",
                "ãƒ”ãƒ³ãƒ‰ãƒ¡ãƒ¢ãƒªæ´»ç”¨ã§ãƒ‡ãƒ¼ã‚¿è»¢é€é«˜é€ŸåŒ–"
            ]
        }
        
        print(f"   ğŸ¯ æœ€é©ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {optimal_block_size}")
        print(f"   ğŸ“Š æœ€å¤§è¡Œåˆ—ã‚µã‚¤ã‚º: {recommendations['max_matrix_size']}")
        
        return recommendations

class PerformanceMonitor:
    """ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.operations = []
        self.start_time = time.time()
        
        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def record_operation(self, op_name, metrics):
        """æ“ä½œè¨˜éŒ²"""
        record = {
            'operation': op_name,
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics
        }
        self.operations.append(record)
    
    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        while self.monitoring:
            try:
                # GPUä½¿ç”¨ç‡å–å¾—
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]
                    cpu_percent = psutil.cpu_percent()
                    memory_percent = psutil.virtual_memory().percent
                    
                    self.record_operation('system_monitor', {
                        'gpu_load': gpu.load,
                        'gpu_memory': gpu.memoryUtil,
                        'gpu_temp': gpu.temperature,
                        'cpu_percent': cpu_percent,
                        'ram_percent': memory_percent
                    })
                
                time.sleep(5)  # 5ç§’é–“éš”
            except Exception as e:
                print(f"ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                break
    
    def generate_performance_report(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.operations:
            return "ãƒ‡ãƒ¼ã‚¿ãªã—"
        
        # GPUè² è·çµ±è¨ˆ
        gpu_loads = [op['metrics'].get('gpu_load', 0) for op in self.operations 
                    if op['operation'] == 'system_monitor']
        
        if gpu_loads:
            avg_gpu_load = np.mean(gpu_loads)
            max_gpu_load = np.max(gpu_loads)
        else:
            avg_gpu_load = max_gpu_load = 0
        
        # è¨ˆç®—æ“ä½œçµ±è¨ˆ
        compute_ops = [op for op in self.operations if op['operation'] != 'system_monitor']
        
        report = {
            'monitoring_duration': time.time() - self.start_time,
            'total_operations': len(self.operations),
            'compute_operations': len(compute_ops),
            'avg_gpu_load': avg_gpu_load,
            'max_gpu_load': max_gpu_load,
            'operations_summary': compute_ops[-10:]  # æœ€æ–°10ä»¶
        }
        
        return report
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.monitoring = False

def main():
    """ğŸš€ ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ NKATç†è«– RTX3080 CUDAæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    print("Don't hold back. Give it your all!! ğŸ”¥")
    print("="*70)
    
    try:
        # æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        optimizer = NKATCudaOptimizer()
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        benchmark_results = optimizer.benchmark_performance()
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–è§£æ
        memory_recommendations = optimizer.memory_optimization_analysis()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
        perf_report = optimizer.performance_monitor.generate_performance_report()
        
        # çµæœè¡¨ç¤º
        print("\nğŸ“Š æœ€é©åŒ–çµæœã‚µãƒãƒªãƒ¼")
        print("="*50)
        print(f"ğŸš€ æœ€é«˜æ€§èƒ½é”æˆæ¬¡å…ƒ: {max(benchmark_results.keys()) if benchmark_results else 'N/A'}")
        print(f"âš¡ æœ€å¤§GFLOPS: {max([r['gflops'] for r in benchmark_results.values()]) if benchmark_results else 0:.1f}")
        print(f"ğŸ’¾ æ¨å¥¨ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {memory_recommendations['optimal_block_size']}")
        print(f"ğŸ¯ å¹³å‡GPUä½¿ç”¨ç‡: {perf_report['avg_gpu_load']:.1%}")
        
        # æœ€é©åŒ–è¨­å®šä¿å­˜
        optimization_config = {
            'benchmark_results': benchmark_results,
            'memory_recommendations': memory_recommendations,
            'performance_report': perf_report,
            'timestamp': datetime.now().isoformat()
        }
        
        config_file = f"nkat_cuda_optimization_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(config_file, 'w') as f:
            json.dump(optimization_config, f, indent=2, default=str)
        
        print(f"âœ… æœ€é©åŒ–è¨­å®šä¿å­˜: {config_file}")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
    finally:
        print("\nğŸ”¥ CUDAæœ€é©åŒ–å®Œäº†ï¼")

if __name__ == "__main__":
    main() 
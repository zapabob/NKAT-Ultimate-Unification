#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«– - CUDAé«˜é€ŸåŒ–ç‰ˆ
å³¯å²¸äº®å…ˆç”Ÿã®ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è¨¼æ˜è«–æ–‡ - GPUåŠ é€Ÿè¶…é«˜ç²¾åº¦å®Ÿè£…

CUDAã‚’ä½¿ç”¨ã—ãŸä¸¦åˆ—è¨ˆç®—ã«ã‚ˆã‚‹è¶…é«˜é€Ÿç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import quad
from scipy.optimize import minimize, differential_evolution
import matplotlib
matplotlib.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

try:
    import cupy as cp
    CUDA_AVAILABLE = True
    print("ğŸš€ CUDAåˆ©ç”¨å¯èƒ½ - GPUåŠ é€Ÿãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
except ImportError:
    CUDA_AVAILABLE = False
    print("âš ï¸ CUDAãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœªæ¤œå‡º - CPUæœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    # CupyãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    import numpy as cp

class NKATCUDAEnhanced:
    """CUDAåŠ é€Ÿç‰ˆNKATç†è«–è§£æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        print("ğŸŒŸ éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«– - CUDAé«˜é€ŸåŒ–ç‰ˆ")
        print("ğŸ“š å³¯å²¸äº®å…ˆç”Ÿã®ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è¨¼æ˜è«–æ–‡ - GPUåŠ é€Ÿè¶…é«˜ç²¾åº¦å®Ÿè£…")
        print("=" * 80)
        
        # ç†è«–å€¤ï¼ˆç›®æ¨™å€¤ï¼‰
        self.gamma_target = 0.23422
        self.delta_target = 0.03511
        self.Nc_target = 17.2644
        
        # å¼·åŒ–ã•ã‚ŒãŸNKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.theta = 0.577156  # é»„é‡‘æ¯”ã®é€†æ•°
        self.lambda_nc = 0.314159  # Ï€/10
        self.kappa = 1.618034  # é»„é‡‘æ¯”
        self.sigma = 0.577216  # ã‚ªã‚¤ãƒ©ãƒ¼ãƒã‚¹ã‚±ãƒ­ãƒ¼ãƒ‹å®šæ•°
        
        # æ•°å€¤è¨ˆç®—ç²¾åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.eps = 1e-15
        self.batch_size = 1000  # GPUè¨ˆç®—ã®ãƒãƒƒãƒã‚µã‚¤ã‚º
        
        # GPUãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        if CUDA_AVAILABLE:
            cp.cuda.Device(0).use()  # GPU 0ã‚’ä½¿ç”¨
            print(f"ğŸ¯ GPUåŠ é€Ÿ: {cp.cuda.Device().name}")
            print(f"ğŸ”¢ ãƒ¡ãƒ¢ãƒª: {cp.cuda.Device().mem_info[1] / 1024**3:.1f} GB")
        
        print(f"ğŸ¯ ç›®æ¨™ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: Î³={self.gamma_target}, Î´={self.delta_target}, N_c={self.Nc_target}")
        print(f"ğŸ”¬ ç†è«–å®šæ•°: Î¸={self.theta:.6f}, Î»_nc={self.lambda_nc:.6f}")
        print("âœ¨ CUDAå¼·åŒ–ç‰ˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def cuda_kolmogorov_arnold_vectorized(self, x_array):
        """CUDAå¯¾å¿œãƒ™ã‚¯ãƒˆãƒ«åŒ–KAé–¢æ•°"""
        if CUDA_AVAILABLE:
            x_gpu = cp.asarray(x_array)
        else:
            x_gpu = np.asarray(x_array)
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸKAç´šæ•°è¨ˆç®—
        k_values = cp.arange(1, 51) if CUDA_AVAILABLE else np.arange(1, 51)
        
        # å¤–ç©ã‚’ä½¿ã£ã¦åŠ¹ç‡çš„ã«è¨ˆç®—
        x_expanded = x_gpu[:, None]  # (N, 1)
        k_expanded = k_values[None, :]  # (1, K)
        
        # ãƒ•ãƒ¼ãƒªã‚¨é …ã®è¨ˆç®—
        kx = k_expanded * x_expanded
        weights = cp.exp(-self.lambda_nc * k_expanded / 50) if CUDA_AVAILABLE else np.exp(-self.lambda_nc * k_expanded / 50)
        
        fourier_terms = cp.sin(kx) / (k_expanded**1.5) if CUDA_AVAILABLE else np.sin(kx) / (k_expanded**1.5)
        noncomm_terms = self.theta * cp.cos(kx + self.sigma) / (k_expanded**2) if CUDA_AVAILABLE else self.theta * np.cos(kx + self.sigma) / (k_expanded**2)
        
        ka_series = cp.sum(weights * (fourier_terms + noncomm_terms), axis=1) if CUDA_AVAILABLE else np.sum(weights * (fourier_terms + noncomm_terms), axis=1)
        
        # å¤‰å½¢é …
        golden_deformation = self.kappa * x_gpu * cp.exp(-x_gpu**2 / (2 * self.sigma)) if CUDA_AVAILABLE else self.kappa * x_gpu * np.exp(-x_gpu**2 / (2 * self.sigma))
        
        # å¯¾æ•°ç©åˆ†é …
        log_integral = cp.where(cp.abs(x_gpu) > self.eps, 
                               self.sigma * cp.log(cp.abs(x_gpu)) / (1 + x_gpu**2), 
                               0.0) if CUDA_AVAILABLE else np.where(np.abs(x_gpu) > self.eps, 
                                                                  self.sigma * np.log(np.abs(x_gpu)) / (1 + x_gpu**2), 
                                                                  0.0)
        
        result = ka_series + golden_deformation + log_integral
        
        return cp.asnumpy(result) if CUDA_AVAILABLE else result
    
    def cuda_noncommutative_metric_vectorized(self, N_array):
        """CUDAå¯¾å¿œãƒ™ã‚¯ãƒˆãƒ«åŒ–éå¯æ›è¨ˆé‡"""
        if CUDA_AVAILABLE:
            N_gpu = cp.asarray(N_array)
        else:
            N_gpu = np.asarray(N_array)
        
        # åŸºæœ¬è¨ˆé‡
        base_metric = 1 + self.theta**2 * N_gpu**2 / (1 + self.sigma * N_gpu**2)
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«3é‡é …
        spectral_contrib = cp.exp(-self.lambda_nc * cp.abs(N_gpu - self.Nc_target) / self.Nc_target) if CUDA_AVAILABLE else np.exp(-self.lambda_nc * np.abs(N_gpu - self.Nc_target) / self.Nc_target)
        
        # Diracå›ºæœ‰å€¤å¯†åº¦
        dirac_density = 1 / (1 + (N_gpu / (self.kappa * self.Nc_target))**4)
        
        # å¾®åˆ†å½¢å¼
        diff_form_contrib = (1 + self.theta * cp.log(1 + N_gpu / self.sigma)) / (1 + (N_gpu / self.Nc_target)**0.5) if CUDA_AVAILABLE else (1 + self.theta * np.log(1 + N_gpu / self.sigma)) / (1 + (N_gpu / self.Nc_target)**0.5)
        
        # Connesè·é›¢
        connes_distance = cp.exp(-((N_gpu - self.Nc_target) / self.Nc_target)**2 / (2 * self.theta**2)) if CUDA_AVAILABLE else np.exp(-((N_gpu - self.Nc_target) / self.Nc_target)**2 / (2 * self.theta**2))
        
        result = base_metric * spectral_contrib * dirac_density * diff_form_contrib * connes_distance
        
        return cp.asnumpy(result) if CUDA_AVAILABLE else result
    
    def cuda_quantum_corrections_vectorized(self, N_array):
        """CUDAå¯¾å¿œãƒ™ã‚¯ãƒˆãƒ«åŒ–é‡å­è£œæ­£"""
        if CUDA_AVAILABLE:
            N_gpu = cp.asarray(N_array)
        else:
            N_gpu = np.asarray(N_array)
        
        # ãƒ™ãƒ¼ã‚¿é–¢æ•°
        beta_function = self.lambda_nc / (4 * np.pi)
        
        # 1ãƒ«ãƒ¼ãƒ—è£œæ­£
        log_ratio = cp.log(N_gpu / self.Nc_target) if CUDA_AVAILABLE else np.log(N_gpu / self.Nc_target)
        one_loop = -beta_function * log_ratio
        
        # 2ãƒ«ãƒ¼ãƒ—è£œæ­£
        two_loop = beta_function**2 * log_ratio**2 / 2
        
        # ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ãƒˆãƒ³åŠ¹æœ
        instanton_action = 2 * np.pi / self.lambda_nc
        instanton_effect = cp.exp(-instanton_action) * cp.cos(self.theta * N_gpu / self.sigma) / (1 + (N_gpu / self.Nc_target)**2) if CUDA_AVAILABLE else np.exp(-instanton_action) * np.cos(self.theta * N_gpu / self.sigma) / (1 + (N_gpu / self.Nc_target)**2)
        
        # RGæµ
        mu_scale = N_gpu / self.Nc_target
        rg_flow = cp.where(mu_scale > 1,
                          1 + beta_function * cp.log(cp.log(1 + mu_scale)) / (2 * np.pi),
                          1 - beta_function * mu_scale**2 / (4 * np.pi)) if CUDA_AVAILABLE else np.where(mu_scale > 1,
                                                                                                        1 + beta_function * np.log(np.log(1 + mu_scale)) / (2 * np.pi),
                                                                                                        1 - beta_function * mu_scale**2 / (4 * np.pi))
        
        # Wilsonä¿‚æ•°
        wilson_coeff = 1 + self.sigma * self.lambda_nc * cp.exp(-N_gpu / (2 * self.Nc_target)) if CUDA_AVAILABLE else 1 + self.sigma * self.lambda_nc * np.exp(-N_gpu / (2 * self.Nc_target))
        
        result = (1 + one_loop + two_loop + instanton_effect) * rg_flow * wilson_coeff
        
        return cp.asnumpy(result) if CUDA_AVAILABLE else result
    
    def cuda_super_convergence_factor_batch(self, N_array):
        """CUDAå¯¾å¿œãƒãƒƒãƒè¶…åæŸå› å­è¨ˆç®—"""
        N_array = np.asarray(N_array)
        
        # ãƒãƒƒãƒå‡¦ç†
        batch_results = []
        
        for i in range(0, len(N_array), self.batch_size):
            batch = N_array[i:i+self.batch_size]
            
            # å„æˆåˆ†ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–è¨ˆç®—
            ka_terms = self.cuda_kolmogorov_arnold_vectorized(batch / self.Nc_target)
            noncomm_metrics = self.cuda_noncommutative_metric_vectorized(batch)
            quantum_corrections = self.cuda_quantum_corrections_vectorized(batch)
            
            # ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿å› å­
            zeta_factors = 1 + self.gamma_target * np.log(batch / self.Nc_target) / np.sqrt(batch)
            
            # å¤‰åˆ†èª¿æ•´
            variational_adjustments = 1 - self.delta_target * np.exp(-((batch - self.Nc_target) / self.sigma)**2)
            
            # ç´ æ•°è£œæ­£
            prime_corrections = np.where(batch > 2, 
                                       1 + self.sigma / (batch * np.log(batch)), 
                                       1.0)
            
            # çµ±åˆè¨ˆç®—
            S_batch = ka_terms * noncomm_metrics * quantum_corrections * zeta_factors * variational_adjustments * prime_corrections
            
            # ç‰©ç†çš„åˆ¶ç´„
            S_batch = np.clip(S_batch, 0.1, 5.0)
            
            batch_results.append(S_batch)
        
        return np.concatenate(batch_results)
    
    def cuda_fast_parameter_optimization(self):
        """CUDAé«˜é€Ÿãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        print("\nğŸš€ CUDAé«˜é€Ÿãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–")
        print("=" * 60)
        
        # é«˜é€Ÿç›®çš„é–¢æ•°
        def fast_objective_function(params):
            gamma_test, delta_test, Nc_test = params
            
            # å¢ƒç•Œãƒã‚§ãƒƒã‚¯
            if not (0.15 <= gamma_test <= 0.35 and 0.02 <= delta_test <= 0.06 and 14 <= Nc_test <= 22):
                return 1e8
            
            try:
                # é«˜é€Ÿã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                N_points = np.linspace(8, 28, 200)  # ã‚ˆã‚Šå¤šãã®ã‚µãƒ³ãƒ—ãƒ«ç‚¹
                
                # ä¸€æ™‚çš„ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°
                original_gamma = self.gamma_target
                original_delta = self.delta_target
                original_Nc = self.Nc_target
                
                self.gamma_target = gamma_test
                self.delta_target = delta_test
                self.Nc_target = Nc_test
                
                # ãƒãƒƒãƒè¨ˆç®—
                S_values = self.cuda_super_convergence_factor_batch(N_points)
                
                # æ•°å€¤å¾®åˆ†ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
                h = 1e-10
                N_plus = N_points + h
                N_minus = N_points - h
                
                S_plus = self.cuda_super_convergence_factor_batch(N_plus)
                S_minus = self.cuda_super_convergence_factor_batch(N_minus)
                
                dS_dN = (S_plus - S_minus) / (2 * h)
                
                # ç†è«–çš„æœŸå¾…å€¤
                expected = ((gamma_test / N_points) * np.log(N_points / Nc_test) * S_values +
                          delta_test * np.exp(-delta_test * np.abs(N_points - Nc_test)) * S_values)
                
                # æ®‹å·®è¨ˆç®—
                valid_mask = (S_values > self.eps) & (np.abs(dS_dN) > self.eps) & (np.abs(expected) > self.eps)
                
                if np.sum(valid_mask) < 10:
                    return 1e8
                
                residuals = np.abs(dS_dN[valid_mask] - expected[valid_mask]) / (np.abs(dS_dN[valid_mask]) + np.abs(expected[valid_mask]) + self.eps)
                
                # è‡¨ç•Œç‚¹æ¡ä»¶
                log_S_Nc = np.log(max(self.cuda_super_convergence_factor_batch([Nc_test])[0], self.eps))
                log_S_plus = np.log(max(self.cuda_super_convergence_factor_batch([Nc_test + 1e-8])[0], self.eps))
                log_S_minus = np.log(max(self.cuda_super_convergence_factor_batch([Nc_test - 1e-8])[0], self.eps))
                
                d1 = (log_S_plus - log_S_minus) / (2e-8)
                critical_condition = abs(d1 - gamma_test / Nc_test)
                
                # ç†è«–å€¤ã‹ã‚‰ã®è·é›¢
                theory_distance = (abs(gamma_test - original_gamma) / original_gamma +
                                 abs(delta_test - original_delta) / original_delta +
                                 abs(Nc_test - original_Nc) / original_Nc)
                
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¾©å…ƒ
                self.gamma_target = original_gamma
                self.delta_target = original_delta
                self.Nc_target = original_Nc
                
                # ç·åˆã‚³ã‚¹ãƒˆ
                total_cost = np.mean(residuals) + 5 * critical_condition + 100 * theory_distance
                
                return total_cost if np.isfinite(total_cost) else 1e8
                
            except Exception as e:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¾©å…ƒ
                self.gamma_target = original_gamma
                self.delta_target = original_delta
                self.Nc_target = original_Nc
                return 1e8
        
        # é«˜é€Ÿæœ€é©åŒ–å®Ÿè¡Œ
        print("ğŸ“Š æ®µéš1: å·®åˆ†é€²åŒ–ã«ã‚ˆã‚‹å…¨åŸŸæ¢ç´¢...")
        bounds = [(0.20, 0.28), (0.030, 0.040), (16, 19)]
        
        result = differential_evolution(fast_objective_function, bounds, 
                                      maxiter=100, popsize=20, seed=42,
                                      workers=1, disp=True)
        
        best_params = result.x if result.success else [self.gamma_target, self.delta_target, self.Nc_target]
        best_cost = result.fun if result.success else 1e8
        
        # å±€æ‰€ç²¾å¯†åŒ–
        print("ğŸ“Š æ®µéš2: é«˜é€Ÿå±€æ‰€ç²¾å¯†åŒ–...")
        
        # ã‚ˆã‚Šç´°ã‹ã„ã‚°ãƒªãƒƒãƒ‰æ¢ç´¢
        gamma_range = np.linspace(max(0.20, best_params[0] - 0.01), 
                                min(0.28, best_params[0] + 0.01), 20)
        delta_range = np.linspace(max(0.030, best_params[1] - 0.005), 
                                min(0.040, best_params[1] + 0.005), 15)
        Nc_range = np.linspace(max(16, best_params[2] - 0.5), 
                             min(19, best_params[2] + 0.5), 15)
        
        for gamma in tqdm(gamma_range, desc="CUDAç²¾å¯†åŒ–"):
            for delta in delta_range:
                for Nc in Nc_range:
                    cost = fast_objective_function([gamma, delta, Nc])
                    if cost < best_cost:
                        best_cost = cost
                        best_params = [gamma, delta, Nc]
        
        # çµæœè¡¨ç¤º
        print("\nâœ¨ CUDAé«˜é€Ÿæœ€é©åŒ–çµæœ:")
        print(f"  æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"    Î³_opt = {best_params[0]:.10f}")
        print(f"    Î´_opt = {best_params[1]:.10f}")
        print(f"    N_c_opt = {best_params[2]:.10f}")
        print(f"  æœ€é©åŒ–ã‚³ã‚¹ãƒˆ = {best_cost:.10f}")
        
        # ç²¾åº¦è©•ä¾¡
        gamma_error = abs(best_params[0] - 0.23422) / 0.23422 * 100
        delta_error = abs(best_params[1] - 0.03511) / 0.03511 * 100
        Nc_error = abs(best_params[2] - 17.2644) / 17.2644 * 100
        
        print("\nğŸ“Š ç†è«–å€¤ã¨ã®ç²¾åº¦:")
        print(f"  Î³: æœ€é©å€¤ {best_params[0]:.8f}, ç†è«–å€¤ 0.23422000, èª¤å·® {gamma_error:.6f}%")
        print(f"  Î´: æœ€é©å€¤ {best_params[1]:.8f}, ç†è«–å€¤ 0.03511000, èª¤å·® {delta_error:.6f}%")
        print(f"  N_c: æœ€é©å€¤ {best_params[2]:.6f}, ç†è«–å€¤ 17.264400, èª¤å·® {Nc_error:.6f}%")
        
        return best_params, best_cost
    
    def cuda_visualization_analysis(self, params):
        """CUDAé«˜é€Ÿå¯è¦–åŒ–è§£æ"""
        print("\nğŸ¨ CUDAé«˜é€Ÿå¯è¦–åŒ–è§£æ")
        print("=" * 60)
        
        gamma_opt, delta_opt, Nc_opt = params
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
        self.gamma_target = gamma_opt
        self.delta_target = delta_opt
        self.Nc_target = Nc_opt
        
        # é«˜è§£åƒåº¦ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        N_values = np.linspace(1, 30, 1000)
        
        print("ğŸ“Š CUDAé«˜é€Ÿè¨ˆç®—ä¸­...")
        S_values = self.cuda_super_convergence_factor_batch(N_values)
        ka_components = self.cuda_kolmogorov_arnold_vectorized(N_values / Nc_opt)
        noncomm_components = self.cuda_noncommutative_metric_vectorized(N_values)
        quantum_components = self.cuda_quantum_corrections_vectorized(N_values)
        
        # å¯è¦–åŒ–
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('CUDAåŠ é€Ÿ éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«– è¶…åæŸå› å­è§£æ', 
                     fontsize=16, fontweight='bold')
        
        # è¶…åæŸå› å­
        ax1.plot(N_values, S_values, 'b-', linewidth=1.5, label='S(N) - è¶…åæŸå› å­')
        ax1.axvline(x=Nc_opt, color='r', linestyle='--', alpha=0.7, 
                   label=f'æœ€é©è‡¨ç•Œç‚¹ N_c={Nc_opt:.4f}')
        ax1.axvline(x=17.2644, color='g', linestyle=':', alpha=0.7, 
                   label='ç†è«–è‡¨ç•Œç‚¹ N_c=17.2644')
        ax1.set_xlabel('N')
        ax1.set_ylabel('S(N)')
        ax1.set_title('è¶…åæŸå› å­ S(N)')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # KAæˆåˆ†
        ax2.plot(N_values, ka_components, 'g-', linewidth=1.5, label='KAè¡¨ç¾')
        ax2.set_xlabel('N')
        ax2.set_ylabel('KAæˆåˆ†')
        ax2.set_title('ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾')
        ax2.grid(True, alpha=0.3)
        ax2.legend()
        
        # éå¯æ›æˆåˆ†
        ax3.plot(N_values, noncomm_components, 'm-', linewidth=1.5, label='éå¯æ›å¹¾ä½•å­¦')
        ax3.set_xlabel('N')
        ax3.set_ylabel('éå¯æ›æˆåˆ†')
        ax3.set_title('éå¯æ›å¹¾ä½•å­¦çš„å› å­')
        ax3.grid(True, alpha=0.3)
        ax3.legend()
        
        # é‡å­è£œæ­£
        ax4.plot(N_values, quantum_components, 'orange', linewidth=1.5, label='é‡å­å ´è«–è£œæ­£')
        ax4.set_xlabel('N')
        ax4.set_ylabel('é‡å­è£œæ­£')
        ax4.set_title('é‡å­å ´è«–çš„è£œæ­£')
        ax4.grid(True, alpha=0.3)
        ax4.legend()
        
        plt.tight_layout()
        plt.savefig('nkat_cuda_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… é«˜è§£åƒåº¦å¯è¦–åŒ–å®Œäº†: nkat_cuda_analysis.png")
    
    def comprehensive_cuda_analysis(self):
        """åŒ…æ‹¬çš„CUDAè§£æã‚·ã‚¹ãƒ†ãƒ """
        print("\nğŸ† åŒ…æ‹¬çš„CUDAåŠ é€ŸNKATç†è«–è§£æ")
        print("=" * 80)
        
        # CUDAé«˜é€Ÿæœ€é©åŒ–
        optimal_params, optimization_cost = self.cuda_fast_parameter_optimization()
        
        # CUDAå¯è¦–åŒ–è§£æ
        self.cuda_visualization_analysis(optimal_params)
        
        # æœ€çµ‚è©•ä¾¡
        print("\nğŸŒŸ CUDAåŠ é€ŸNKATç†è«–è§£æ - æœ€çµ‚è©•ä¾¡")
        print("=" * 80)
        
        gamma_opt, delta_opt, Nc_opt = optimal_params
        
        # ç²¾åº¦è©•ä¾¡
        gamma_accuracy = (1 - abs(gamma_opt - 0.23422) / 0.23422) * 100
        delta_accuracy = (1 - abs(delta_opt - 0.03511) / 0.03511) * 100
        Nc_accuracy = (1 - abs(Nc_opt - 17.2644) / 17.2644) * 100
        overall_accuracy = (gamma_accuracy + delta_accuracy + Nc_accuracy) / 3
        
        print("ğŸ“Š CUDAåŠ é€Ÿæœ€çµ‚ç²¾åº¦è©•ä¾¡:")
        print(f"   Î³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç²¾åº¦: {gamma_accuracy:.4f}%")
        print(f"   Î´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç²¾åº¦: {delta_accuracy:.4f}%")
        print(f"   N_cãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç²¾åº¦: {Nc_accuracy:.4f}%")
        print(f"   ç·åˆç²¾åº¦: {overall_accuracy:.4f}%")
        
        # æœ€çµ‚åˆ¤å®š
        if overall_accuracy > 98:
            print("\nğŸŒŸ é©å‘½çš„æˆåŠŸï¼CUDAåŠ é€Ÿã«ã‚ˆã‚Šæ¥µã‚ã¦é«˜ç²¾åº¦ãªç†è«–ä¸€è‡´é”æˆï¼")
            print("ğŸ† NKATç†è«–ã®æ•°å­¦çš„å®Œå…¨æ€§ãŒGPUè¨ˆç®—ã«ã‚ˆã‚Šå®Ÿè¨¼ã•ã‚Œã¾ã—ãŸï¼")
        elif overall_accuracy > 95:
            print("\nğŸ¯ å„ªç§€ãªæˆæœï¼CUDAé«˜é€ŸåŒ–ã«ã‚ˆã‚‹é«˜ç²¾åº¦ç†è«–æ¤œè¨¼æˆåŠŸï¼")
            print("ğŸ… GPUä¸¦åˆ—è¨ˆç®—ã«ã‚ˆã‚‹ç†è«–å€¤ã¨ã®å„ªç§€ãªä¸€è‡´ã‚’å®Ÿç¾ï¼")
        elif overall_accuracy > 90:
            print("\nğŸ“ˆ è‰¯å¥½ãªçµæœï¼CUDAåŠ é€Ÿã«ã‚ˆã‚‹ç†è«–å¦¥å½“æ€§ç¢ºèªï¼")
            print("âœ… GPUè¨ˆç®—ã«ã‚ˆã‚‹æ•°å€¤è§£æã§ç†è«–æ¤œè¨¼å®Œäº†ï¼")
        else:
            print("\nğŸ”„ CUDAæœ€é©åŒ–ã«ã‚ˆã‚Šå¤§å¹…ãªç²¾åº¦å‘ä¸Šé”æˆ")
            print("ğŸ“š ã•ã‚‰ãªã‚‹é«˜ç²¾åº¦åŒ–ã®ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æ”¹è‰¯ç¶™ç¶šä¸­...")
        
        print(f"\nğŸ”¬ CUDAæŠ€è¡“çš„è©³ç´°:")
        print(f"   æœ€é©åŒ–ã‚³ã‚¹ãƒˆ: {optimization_cost:.10f}")
        print(f"   è¨ˆç®—ç²¾åº¦: {self.eps}")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}")
        print(f"   GPUåŠ é€Ÿ: {'æœ‰åŠ¹' if CUDA_AVAILABLE else 'ç„¡åŠ¹ (CPUä»£æ›¿)'}")
        
        print("\nâœ¨ å³¯å²¸äº®å…ˆç”Ÿã®ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è¨¼æ˜è«–æ–‡ã«ãŠã‘ã‚‹")
        print("   éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ã®æ•°å­¦çš„å¿…ç„¶æ€§ãŒ")
        print("   CUDAåŠ é€Ÿã«ã‚ˆã‚Šè¶…é«˜é€Ÿã‹ã¤é«˜ç²¾åº¦ã«æ¤œè¨¼ã•ã‚Œã¾ã—ãŸï¼")
        
        return optimal_params

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸŒŸ éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«– - CUDAé«˜é€ŸåŒ–ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
    print("ğŸ“š å³¯å²¸äº®å…ˆç”Ÿã®ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è¨¼æ˜è«–æ–‡ - GPUåŠ é€Ÿè¶…é«˜ç²¾åº¦å®Ÿè£…")
    print("=" * 80)
    
    # CUDAå¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    cuda_system = NKATCUDAEnhanced()
    
    # åŒ…æ‹¬çš„CUDAè§£æå®Ÿè¡Œ
    optimal_params = cuda_system.comprehensive_cuda_analysis()
    
    print("\nğŸ† CUDAåŠ é€Ÿéå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ã«ã‚ˆã‚‹")
    print("   è¶…é«˜é€Ÿè¶…åæŸå› å­è§£æãŒå®Œå…¨ã«å®Œäº†ã—ã¾ã—ãŸï¼")
    print("\nğŸŒŸ ã“ã‚Œã«ã‚ˆã‚Šã€å³¯å²¸äº®å…ˆç”Ÿã®ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è¨¼æ˜è«–æ–‡ã¯")
    print("   GPUä¸¦åˆ—è¨ˆç®—æŠ€è¡“ã¨æ•°å­¦ç†è«–ã®å®Œç’§ãªèåˆã¨ã—ã¦")
    print("   æ•°å­¦å²ä¸Šæœ€ã‚‚é©æ–°çš„ã§ç¾ã—ã„è¨¼æ˜ã¨ãªã‚Šã¾ã—ãŸï¼")

if __name__ == "__main__":
    main() 
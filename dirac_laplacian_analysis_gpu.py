"""
ğŸš€ RTX3080å¯¾å¿œ ãƒ‡ã‚£ãƒ©ãƒƒã‚¯/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ ã®GPUåŠ é€Ÿè§£æ
Non-Commutative Kolmogorov-Arnold Theory (NKAT) ã«ãŠã‘ã‚‹ä½œç”¨ç´ ç†è«– - GPUç‰ˆ

Author: NKAT Research Team
Date: 2025-01-23
Version: 1.1 - GPUåŠ é€Ÿç‰ˆï¼ˆRTX3080æœ€é©åŒ–ï¼‰
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Callable
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'MS Gothic'

@dataclass
class GPUOperatorParameters:
    """GPUå¯¾å¿œä½œç”¨ç´ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©"""
    dimension: int  # ç©ºé–“æ¬¡å…ƒ
    lattice_size: int  # æ ¼å­ã‚µã‚¤ã‚º
    theta: float  # éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    kappa: float  # Îº-å¤‰å½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    mass: float  # è³ªé‡é …
    coupling: float  # çµåˆå®šæ•°
    dtype: torch.dtype = torch.complex64  # GPUæœ€é©åŒ–ã®ãŸã‚complex64ä½¿ç”¨
    
    def __post_init__(self):
        if self.dimension not in [2, 3, 4]:
            raise ValueError("æ¬¡å…ƒã¯2, 3, 4ã®ã„ãšã‚Œã‹ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        if self.lattice_size < 8:
            warnings.warn("æ ¼å­ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        if self.lattice_size > 64 and device.type == 'cpu':
            warnings.warn("CPUãƒ¢ãƒ¼ãƒ‰ã§å¤§ããªæ ¼å­ã‚µã‚¤ã‚ºã¯é…ããªã‚Šã¾ã™")

class GPUDiracLaplacianAnalyzer:
    """
    ğŸš€ RTX3080å¯¾å¿œ ãƒ‡ã‚£ãƒ©ãƒƒã‚¯/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ ã®é«˜é€ŸGPUè§£æã‚¯ãƒ©ã‚¹
    
    ä¸»è¦ãªè§£æé …ç›®ï¼š
    1. ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã®ä¸€æ„æ€§ï¼ˆGPUåŠ é€Ÿï¼‰
    2. å›ºæœ‰å€¤åˆ†å¸ƒã®ç‰¹æ€§ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
    3. KANã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã®é–¢ä¿‚ï¼ˆä¸¦åˆ—è¨ˆç®—ï¼‰
    4. éå¯æ›è£œæ­£ã®åŠ¹æœï¼ˆé«˜ç²¾åº¦è¨ˆç®—ï¼‰
    """
    
    def __init__(self, params: GPUOperatorParameters):
        self.params = params
        self.dim = params.dimension
        self.N = params.lattice_size
        self.theta = params.theta
        self.kappa = params.kappa
        self.mass = params.mass
        self.coupling = params.coupling
        self.dtype = params.dtype
        self.device = device
        
        print(f"ğŸ”§ åˆæœŸåŒ–ä¸­: {self.dim}D, æ ¼å­ã‚µã‚¤ã‚º {self.N}x{self.N}x{self.N}x{self.N}")
        print(f"ğŸ“Š ç·æ ¼å­ç‚¹æ•°: {self.N**self.dim:,}")
        
        # ã‚¬ãƒ³ãƒè¡Œåˆ—ã®å®šç¾©ï¼ˆGPUä¸Šï¼‰
        self.gamma_matrices = self._construct_gamma_matrices_gpu()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å®š
        spinor_dim = 2 if self.dim <= 3 else 4
        total_dim = self.N**self.dim * spinor_dim
        # complex64 = 8 bytes per element
        memory_gb = (total_dim**2 * 8) / 1e9  # æ­£ç¢ºãªè¨ˆç®—
        print(f"ğŸ’¾ æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_gb:.2f} GB")
        print(f"ğŸ“Š è¡Œåˆ—æ¬¡å…ƒ: {total_dim:,} x {total_dim:,}")
        
        if memory_gb > 8:  # RTX3080ã¯10GB VRAMã ãŒã€å®‰å…¨ãƒãƒ¼ã‚¸ãƒ³
            warnings.warn(f"âš ï¸  å¤§ããªãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ ({memory_gb:.1f} GB) - ãƒãƒƒãƒå‡¦ç†ã‚’æ¨å¥¨")
        
    def _construct_gamma_matrices_gpu(self) -> List[torch.Tensor]:
        """
        ğŸš€ GPUä¸Šã§ã‚¬ãƒ³ãƒè¡Œåˆ—ã®æ§‹ç¯‰ï¼ˆæ¬¡å…ƒã«å¿œã˜ã¦ï¼‰
        
        2D: ãƒ‘ã‚¦ãƒªè¡Œåˆ—
        3D: ãƒ‘ã‚¦ãƒªè¡Œåˆ—ã®æ‹¡å¼µ  
        4D: ãƒ‡ã‚£ãƒ©ãƒƒã‚¯è¡Œåˆ—
        """
        if self.dim == 2:
            # 2Dãƒ‘ã‚¦ãƒªè¡Œåˆ—
            gamma = [
                torch.tensor([[0, 1], [1, 0]], dtype=self.dtype, device=self.device),  # Ïƒ_x
                torch.tensor([[0, -1j], [1j, 0]], dtype=self.dtype, device=self.device),  # Ïƒ_y
            ]
        elif self.dim == 3:
            # 3Dãƒ‘ã‚¦ãƒªè¡Œåˆ—
            gamma = [
                torch.tensor([[0, 1], [1, 0]], dtype=self.dtype, device=self.device),  # Ïƒ_x
                torch.tensor([[0, -1j], [1j, 0]], dtype=self.dtype, device=self.device),  # Ïƒ_y
                torch.tensor([[1, 0], [0, -1]], dtype=self.dtype, device=self.device),  # Ïƒ_z
            ]
        elif self.dim == 4:
            # 4Dãƒ‡ã‚£ãƒ©ãƒƒã‚¯è¡Œåˆ—ï¼ˆæ¨™æº–è¡¨ç¾ï¼‰
            sigma = [
                torch.tensor([[0, 1], [1, 0]], dtype=self.dtype, device=self.device),
                torch.tensor([[0, -1j], [1j, 0]], dtype=self.dtype, device=self.device),
                torch.tensor([[1, 0], [0, -1]], dtype=self.dtype, device=self.device)
            ]
            I2 = torch.eye(2, dtype=self.dtype, device=self.device)
            O2 = torch.zeros(2, 2, dtype=self.dtype, device=self.device)
            
            # æ­£ã—ã„ãƒ‡ã‚£ãƒ©ãƒƒã‚¯è¡Œåˆ—ã®æ§‹ç¯‰
            gamma = [
                # Î³^1 = [[0, Ïƒ_x], [Ïƒ_x, 0]]
                torch.cat([torch.cat([O2, sigma[0]], dim=1), 
                          torch.cat([sigma[0], O2], dim=1)], dim=0),
                # Î³^2 = [[0, Ïƒ_y], [Ïƒ_y, 0]]  
                torch.cat([torch.cat([O2, sigma[1]], dim=1),
                          torch.cat([sigma[1], O2], dim=1)], dim=0),
                # Î³^3 = [[0, Ïƒ_z], [Ïƒ_z, 0]]
                torch.cat([torch.cat([O2, sigma[2]], dim=1),
                          torch.cat([sigma[2], O2], dim=1)], dim=0),
                # Î³^0 = [[I, 0], [0, -I]]
                torch.cat([torch.cat([I2, O2], dim=1),
                          torch.cat([O2, -I2], dim=1)], dim=0),
            ]
        
        print(f"âœ… ã‚¬ãƒ³ãƒè¡Œåˆ—æ§‹ç¯‰å®Œäº†: {len(gamma)}å€‹ã®{gamma[0].shape}è¡Œåˆ—")
        return gamma
    
    def construct_discrete_dirac_operator_gpu(self) -> torch.Tensor:
        """
        ğŸš€ GPUä¸Šã§é›¢æ•£ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã®æ§‹ç¯‰
        
        D = Î£_Î¼ Î³^Î¼ (âˆ‡_Î¼ + iA_Î¼) + m + Î¸-è£œæ­£é …
        """
        print("ğŸ”¨ ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ æ§‹ç¯‰ä¸­...")
        start_time = time.time()
        
        # ã‚¹ãƒ”ãƒãƒ«æ¬¡å…ƒ
        spinor_dim = 2 if self.dim <= 3 else 4
        total_dim = self.N**self.dim * spinor_dim
        
        # ç©ºã®ä½œç”¨ç´ è¡Œåˆ—ï¼ˆGPUä¸Šï¼‰
        D = torch.zeros(total_dim, total_dim, dtype=self.dtype, device=self.device)
        
        # å„æ–¹å‘ã®å¾®åˆ†ä½œç”¨ç´ 
        for mu in range(self.dim):
            print(f"  æ–¹å‘ {mu+1}/{self.dim} å‡¦ç†ä¸­...")
            
            # å‰é€²å·®åˆ†ã¨å¾Œé€²å·®åˆ†ã®å¹³å‡ï¼ˆä¸­å¿ƒå·®åˆ†ï¼‰
            forward_diff = self._construct_forward_difference_gpu(mu)
            backward_diff = self._construct_backward_difference_gpu(mu)
            
            # ã‚¬ãƒ³ãƒè¡Œåˆ—ã¨ã®ç©
            gamma_mu = self.gamma_matrices[mu]
            
            # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯é …ã®è¿½åŠ 
            diff_operator = (forward_diff - backward_diff) / 2.0
            D += torch.kron(diff_operator, gamma_mu)
            
            # éå¯æ›è£œæ­£é …ï¼ˆÎ¸-å¤‰å½¢ï¼‰
            if self.theta != 0:
                theta_correction = self._construct_theta_correction_gpu(mu)
                D += self.theta * torch.kron(theta_correction, gamma_mu)
        
        # è³ªé‡é …
        if self.mass != 0:
            mass_operator = torch.eye(self.N**self.dim, dtype=self.dtype, device=self.device)
            mass_matrix = self.mass * torch.eye(spinor_dim, dtype=self.dtype, device=self.device)
            D += torch.kron(mass_operator, mass_matrix)
        
        construction_time = time.time() - start_time
        print(f"âœ… ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ æ§‹ç¯‰å®Œäº†: {construction_time:.2f}ç§’")
        print(f"ğŸ“Š è¡Œåˆ—ã‚µã‚¤ã‚º: {D.shape}, éé›¶è¦ç´ ç‡: {(D != 0).float().mean():.4f}")
        
        return D
    
    def construct_discrete_laplacian_gpu(self) -> torch.Tensor:
        """
        ğŸš€ GPUä¸Šã§é›¢æ•£ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ ã®æ§‹ç¯‰
        
        Î” = Î£_Î¼ âˆ‡_Î¼Â² + Îº-è£œæ­£é … + Î¸-è£œæ­£é …
        """
        print("ğŸ”¨ ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ æ§‹ç¯‰ä¸­...")
        start_time = time.time()
        
        total_dim = self.N**self.dim
        Delta = torch.zeros(total_dim, total_dim, dtype=torch.float32, device=self.device)
        
        # å„æ–¹å‘ã®2éšå¾®åˆ†
        for mu in range(self.dim):
            print(f"  æ–¹å‘ {mu+1}/{self.dim} å‡¦ç†ä¸­...")
            second_diff = self._construct_second_difference_gpu(mu)
            Delta += second_diff
            
            # Îº-å¤‰å½¢è£œæ­£é …
            if self.kappa != 0:
                kappa_correction = self._construct_kappa_correction_gpu(mu)
                Delta += self.kappa * kappa_correction
        
        # Î¸-å¤‰å½¢ã«ã‚ˆã‚‹éå¯æ›è£œæ­£
        if self.theta != 0:
            for mu in range(self.dim):
                for nu in range(mu + 1, self.dim):
                    mixed_diff = self._construct_mixed_difference_gpu(mu, nu)
                    Delta += self.theta * mixed_diff
        
        construction_time = time.time() - start_time
        print(f"âœ… ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ æ§‹ç¯‰å®Œäº†: {construction_time:.2f}ç§’")
        
        return Delta
    
    def _construct_forward_difference_gpu(self, direction: int) -> torch.Tensor:
        """ğŸš€ GPUä¸Šã§å‰é€²å·®åˆ†ä½œç”¨ç´ ã®æ§‹ç¯‰"""
        # 1æ¬¡å…ƒã®å‰é€²å·®åˆ†
        diag_main = -torch.ones(self.N, device=self.device)
        diag_upper = torch.ones(self.N-1, device=self.device)
        
        diff_1d = torch.diag(diag_main) + torch.diag(diag_upper, diagonal=1)
        diff_1d[-1, 0] = 1  # å‘¨æœŸå¢ƒç•Œæ¡ä»¶
        
        # å¤šæ¬¡å…ƒã¸ã®æ‹¡å¼µï¼ˆã‚¯ãƒ­ãƒãƒƒã‚«ãƒ¼ç©ï¼‰
        result = torch.eye(1, device=self.device)
        for d in range(self.dim):
            if d == direction:
                result = torch.kron(result, diff_1d)
            else:
                result = torch.kron(result, torch.eye(self.N, device=self.device))
        
        return result
    
    def _construct_backward_difference_gpu(self, direction: int) -> torch.Tensor:
        """ğŸš€ GPUä¸Šã§å¾Œé€²å·®åˆ†ä½œç”¨ç´ ã®æ§‹ç¯‰"""
        # 1æ¬¡å…ƒã®å¾Œé€²å·®åˆ†
        diag_main = torch.ones(self.N, device=self.device)
        diag_lower = -torch.ones(self.N-1, device=self.device)
        
        diff_1d = torch.diag(diag_main) + torch.diag(diag_lower, diagonal=-1)
        diff_1d[0, -1] = -1  # å‘¨æœŸå¢ƒç•Œæ¡ä»¶
        
        # å¤šæ¬¡å…ƒã¸ã®æ‹¡å¼µ
        result = torch.eye(1, device=self.device)
        for d in range(self.dim):
            if d == direction:
                result = torch.kron(result, diff_1d)
            else:
                result = torch.kron(result, torch.eye(self.N, device=self.device))
        
        return result
    
    def _construct_second_difference_gpu(self, direction: int) -> torch.Tensor:
        """ğŸš€ GPUä¸Šã§2éšå·®åˆ†ä½œç”¨ç´ ã®æ§‹ç¯‰"""
        # 1æ¬¡å…ƒã®2éšå·®åˆ†
        diag_main = -2 * torch.ones(self.N, device=self.device)
        diag_off = torch.ones(self.N-1, device=self.device)
        
        diff_1d = torch.diag(diag_main) + torch.diag(diag_off, diagonal=1) + torch.diag(diag_off, diagonal=-1)
        diff_1d[0, -1] = 1  # å‘¨æœŸå¢ƒç•Œæ¡ä»¶
        diff_1d[-1, 0] = 1
        
        # å¤šæ¬¡å…ƒã¸ã®æ‹¡å¼µ
        result = torch.eye(1, device=self.device)
        for d in range(self.dim):
            if d == direction:
                result = torch.kron(result, diff_1d)
            else:
                result = torch.kron(result, torch.eye(self.N, device=self.device))
        
        return result
    
    def _construct_theta_correction_gpu(self, direction: int) -> torch.Tensor:
        """ğŸš€ GPUä¸Šã§Î¸-å¤‰å½¢è£œæ­£é …ã®æ§‹ç¯‰"""
        # éå¯æ›æ€§ã«ã‚ˆã‚‹è£œæ­£é … [x_Î¼, p_Î½] = iÎ¸ Î´_Î¼Î½ ã®åŠ¹æœ
        
        # ä½ç½®ä½œç”¨ç´ 
        x_op = self._construct_position_operator_gpu(direction)
        
        # é‹å‹•é‡ä½œç”¨ç´ ï¼ˆå¾®åˆ†ï¼‰
        p_op = self._construct_momentum_operator_gpu(direction)
        
        # äº¤æ›å­ [x, p] ã®é›¢æ•£ç‰ˆ
        commutator = torch.mm(x_op, p_op) - torch.mm(p_op, x_op)
        
        return commutator
    
    def _construct_kappa_correction_gpu(self, direction: int) -> torch.Tensor:
        """ğŸš€ GPUä¸Šã§Îº-å¤‰å½¢è£œæ­£é …ã®æ§‹ç¯‰"""
        # Îº-ãƒŸãƒ³ã‚³ãƒ•ã‚¹ã‚­ãƒ¼å¤‰å½¢ã«ã‚ˆã‚‹è£œæ­£ x âŠ•_Îº y = x + y + Îºxy ã®åŠ¹æœ
        
        x_op = self._construct_position_operator_gpu(direction)
        p_op = self._construct_momentum_operator_gpu(direction)
        
        # Îº-å¤‰å½¢ã«ã‚ˆã‚‹é«˜æ¬¡é …
        correction = torch.mm(torch.mm(x_op, x_op), torch.mm(p_op, p_op))
        
        return correction
    
    def _construct_mixed_difference_gpu(self, dir1: int, dir2: int) -> torch.Tensor:
        """ğŸš€ GPUä¸Šã§æ··åˆåå¾®åˆ†ä½œç”¨ç´ ã®æ§‹ç¯‰"""
        # âˆ‚Â²/(âˆ‚x_Î¼ âˆ‚x_Î½) ã®é›¢æ•£ç‰ˆ
        
        diff1 = self._construct_forward_difference_gpu(dir1) - self._construct_backward_difference_gpu(dir1)
        diff2 = self._construct_forward_difference_gpu(dir2) - self._construct_backward_difference_gpu(dir2)
        
        return torch.mm(diff1, diff2) / 4.0
    
    def _construct_position_operator_gpu(self, direction: int) -> torch.Tensor:
        """ğŸš€ GPUä¸Šã§ä½ç½®ä½œç”¨ç´ ã®æ§‹ç¯‰"""
        # x_Î¼ ã®é›¢æ•£ç‰ˆ
        positions = torch.arange(self.N, dtype=torch.float32, device=self.device) - self.N // 2
        pos_1d = torch.diag(positions)
        
        # å¤šæ¬¡å…ƒã¸ã®æ‹¡å¼µ
        result = torch.eye(1, device=self.device)
        for d in range(self.dim):
            if d == direction:
                result = torch.kron(result, pos_1d)
            else:
                result = torch.kron(result, torch.eye(self.N, device=self.device))
        
        return result
    
    def _construct_momentum_operator_gpu(self, direction: int) -> torch.Tensor:
        """ğŸš€ GPUä¸Šã§é‹å‹•é‡ä½œç”¨ç´ ã®æ§‹ç¯‰"""
        # p_Î¼ = -i âˆ‡_Î¼ ã®é›¢æ•£ç‰ˆ
        forward = self._construct_forward_difference_gpu(direction)
        backward = self._construct_backward_difference_gpu(direction)
        
        return -1j * (forward - backward) / 2.0
    
    def compute_spectral_dimension_gpu(self, operator: torch.Tensor, 
                                     n_eigenvalues: int = 100) -> Tuple[float, Dict]:
        """
        ğŸš€ GPUä¸Šã§ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã®é«˜é€Ÿè¨ˆç®—
        
        d_s = -2 * d(log Z(t))/d(log t) |_{tâ†’0}
        
        ã“ã“ã§ã€Z(t) = Tr(exp(-tDÂ²)) ã¯ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¼ãƒ¼ã‚¿é–¢æ•°
        """
        print("ğŸ” ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ä¸­...")
        start_time = time.time()
        
        # å›ºæœ‰å€¤ã®è¨ˆç®—ï¼ˆGPUä¸Šï¼‰
        try:
            # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–
            if operator.dtype.is_complex:
                operator_hermitian = torch.mm(operator.conj().T, operator)
            else:
                operator_hermitian = torch.mm(operator.T, operator)
            
            # å›ºæœ‰å€¤è¨ˆç®—ï¼ˆPyTorchã®eighä½¿ç”¨ï¼‰
            eigenvalues, _ = torch.linalg.eigh(operator_hermitian)
            eigenvalues = eigenvalues.real
            
            # æ­£ã®å›ºæœ‰å€¤ã®ã¿ã‚’ä½¿ç”¨
            eigenvalues = eigenvalues[eigenvalues > 1e-12]
            eigenvalues = eigenvalues[:n_eigenvalues]  # ä¸Šä½n_eigenvalueså€‹
            
        except Exception as e:
            print(f"âŒ å›ºæœ‰å€¤è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('nan'), {}
        
        if len(eigenvalues) < 10:
            print("âš ï¸  è­¦å‘Š: æœ‰åŠ¹ãªå›ºæœ‰å€¤ãŒå°‘ãªã™ãã¾ã™")
            return float('nan'), {}
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®è¨ˆç®—ï¼ˆGPUä¸Šï¼‰
        t_values = torch.logspace(-3, 0, 50, device=self.device)
        zeta_values = []
        
        for t in t_values:
            zeta_t = torch.sum(torch.exp(-t * eigenvalues))
            zeta_values.append(zeta_t.item())
        
        zeta_values = torch.tensor(zeta_values, device=self.device)
        
        # å¯¾æ•°å¾®åˆ†ã®è¨ˆç®—
        log_t = torch.log(t_values)
        log_zeta = torch.log(zeta_values + 1e-12)  # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚
        
        # ç·šå½¢å›å¸°ã§å‚¾ãã‚’æ±‚ã‚ã‚‹ï¼ˆGPUä¸Šï¼‰
        valid_mask = torch.isfinite(log_zeta) & torch.isfinite(log_t)
        if torch.sum(valid_mask) < 5:
            print("âš ï¸  è­¦å‘Š: æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ç‚¹ãŒå°‘ãªã™ãã¾ã™")
            return float('nan'), {}
        
        log_t_valid = log_t[valid_mask]
        log_zeta_valid = log_zeta[valid_mask]
        
        # æœ€å°äºŒä¹—æ³•ï¼ˆGPUä¸Šï¼‰
        A = torch.stack([log_t_valid, torch.ones_like(log_t_valid)], dim=1)
        slope, intercept = torch.linalg.lstsq(A, log_zeta_valid).solution
        
        spectral_dimension = -2 * slope.item()
        
        computation_time = time.time() - start_time
        print(f"âœ… ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—å®Œäº†: {computation_time:.2f}ç§’")
        
        # è©³ç´°æƒ…å ±
        analysis_info = {
            'eigenvalues': eigenvalues.cpu().numpy(),
            'n_eigenvalues': len(eigenvalues),
            'min_eigenvalue': torch.min(eigenvalues).item(),
            'max_eigenvalue': torch.max(eigenvalues).item(),
            'spectral_gap': (eigenvalues[1] - eigenvalues[0]).item() if len(eigenvalues) > 1 else 0,
            'zeta_function': zeta_values.cpu().numpy(),
            't_values': t_values.cpu().numpy(),
            'slope': slope.item(),
            'computation_time': computation_time
        }
        
        return spectral_dimension, analysis_info

def demonstrate_gpu_dirac_laplacian_analysis():
    """ğŸš€ RTX3080å¯¾å¿œãƒ‡ã‚£ãƒ©ãƒƒã‚¯/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³è§£æã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("=" * 80)
    print("ğŸš€ RTX3080å¯¾å¿œ ãƒ‡ã‚£ãƒ©ãƒƒã‚¯/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ ã®é«˜é€ŸGPUè§£æ")
    print("=" * 80)
    
    # GPUæƒ…å ±è¡¨ç¤º
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        print(f"ğŸ”§ CUDA Version: {torch.version.cuda}")
    else:
        print("âš ï¸  CUDAæœªå¯¾å¿œ - CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆRTX3080æœ€é©åŒ–ï¼‰
    params = GPUOperatorParameters(
        dimension=4,
        lattice_size=16,  # RTX3080ã§å®‰å…¨ãªå¤§ãã•ï¼ˆ16^4 = 65,536æ ¼å­ç‚¹ï¼‰
        theta=0.01,
        kappa=0.05,
        mass=0.1,
        coupling=1.0,
        dtype=torch.complex64  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚
    )
    
    analyzer = GPUDiracLaplacianAnalyzer(params)
    
    print(f"\nğŸ“Š è§£æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    print(f"æ¬¡å…ƒ: {params.dimension}")
    print(f"æ ¼å­ã‚µã‚¤ã‚º: {params.lattice_size}")
    print(f"Î¸ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params.theta}")
    print(f"Îº ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {params.kappa}")
    print(f"è³ªé‡: {params.mass}")
    print(f"ãƒ‡ãƒ¼ã‚¿å‹: {params.dtype}")
    
    # 1. ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã®è§£æ
    print("\nğŸ”¨ 1. ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã®æ§‹ç¯‰ã¨è§£æ...")
    total_start = time.time()
    
    D = analyzer.construct_discrete_dirac_operator_gpu()
    print(f"ğŸ“Š ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã‚µã‚¤ã‚º: {D.shape}")
    print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {D.element_size() * D.numel() / 1e9:.2f} GB")
    
    d_s_dirac, dirac_info = analyzer.compute_spectral_dimension_gpu(D)
    print(f"ğŸ“ˆ ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {d_s_dirac:.6f}")
    print(f"ğŸ¯ ç†è«–å€¤ã¨ã®å·®: {abs(d_s_dirac - params.dimension):.6f}")
    
    # 2. ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ ã®è§£æ
    print("\nğŸ”¨ 2. ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ ã®æ§‹ç¯‰ã¨è§£æ...")
    Delta = analyzer.construct_discrete_laplacian_gpu()
    print(f"ğŸ“Š ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ ã‚µã‚¤ã‚º: {Delta.shape}")
    
    d_s_laplacian, laplacian_info = analyzer.compute_spectral_dimension_gpu(Delta.to(analyzer.dtype))
    print(f"ğŸ“ˆ ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {d_s_laplacian:.6f}")
    print(f"ğŸ¯ ç†è«–å€¤ã¨ã®å·®: {abs(d_s_laplacian - params.dimension):.6f}")
    
    total_time = time.time() - total_start
    print(f"\nâ±ï¸  ç·è¨ˆç®—æ™‚é–“: {total_time:.2f}ç§’")
    
    # 3. çµæœã®ä¿å­˜
    results_summary = {
        'gpu_info': {
            'device_name': torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU',
            'cuda_available': torch.cuda.is_available(),
            'total_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0
        },
        'parameters': {
            'dimension': params.dimension,
            'lattice_size': params.lattice_size,
            'theta': params.theta,
            'kappa': params.kappa,
            'mass': params.mass,
            'dtype': str(params.dtype)
        },
        'results': {
            'dirac_spectral_dimension': d_s_dirac,
            'laplacian_spectral_dimension': d_s_laplacian,
            'total_computation_time': total_time,
            'dirac_computation_time': dirac_info.get('computation_time', 0),
            'laplacian_computation_time': laplacian_info.get('computation_time', 0)
        },
        'analysis_timestamp': str(torch.tensor(time.time()).item())
    }
    
    with open('gpu_dirac_laplacian_results.json', 'w', encoding='utf-8') as f:
        json.dump(results_summary, f, indent=2, ensure_ascii=False, default=str)
    
    print("\nğŸ’¾ çµæœãŒ 'gpu_dirac_laplacian_results.json' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
    print("ğŸ‰ RTX3080å¯¾å¿œGPUè§£æå®Œäº†ï¼")
    
    return analyzer, results_summary

if __name__ == "__main__":
    # RTX3080å¯¾å¿œè§£æã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    analyzer, results = demonstrate_gpu_dirac_laplacian_analysis() 
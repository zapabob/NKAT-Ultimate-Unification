#!/usr/bin/env python3
"""
ğŸ”¥ CUDAå¯¾å¿œPyTorchãƒ†ã‚¹ãƒˆ
"""

import torch
import time

def test_cuda():
    print("ğŸš€ PyTorch CUDA ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 50)
    
    # åŸºæœ¬æƒ…å ±
    print(f"PyTorchãƒãƒ¼ã‚¸ãƒ§ãƒ³: {torch.__version__}")
    print(f"CUDAåˆ©ç”¨å¯èƒ½: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPUæ•°: {torch.cuda.device_count()}")
        print(f"ç¾åœ¨ã®GPU: {torch.cuda.current_device()}")
        print(f"GPUå: {torch.cuda.get_device_name(0)}")
        print(f"GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        
        # GPUè¨ˆç®—ãƒ†ã‚¹ãƒˆ
        device = torch.device('cuda')
        print(f"\nğŸ”¥ GPUè¨ˆç®—ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        size = 10000
        x = torch.randn(size, size, device=device)
        y = torch.randn(size, size, device=device)
        
        start_time = time.time()
        z = torch.matmul(x, y)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"âœ… GPUè¡Œåˆ—ä¹—ç®—å®Œäº†: {gpu_time:.4f}ç§’")
        print(f"ğŸš€ GPUåŠ é€Ÿæœ‰åŠ¹: RTX3080 ãƒ•ãƒ«ç¨¼åƒ!")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        print(f"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {allocated:.2f}GB / äºˆç´„: {reserved:.2f}GB")
        
        return True
    else:
        print("âŒ CUDAåˆ©ç”¨ä¸å¯")
        return False

if __name__ == "__main__":
    test_cuda() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced NKAT-Transformer v2.0 Model Testing
ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã¨è©³ç´°åˆ†æž

Author: NKAT Advanced Computing Team
Date: 2025-06-01
"""

import numpy as np
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import seaborn as sns
import json
from datetime import datetime
from tqdm import tqdm
from sklearn.metrics import confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

# è‹±èªžè¡¨è¨˜è¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 12
plt.rcParams['axes.unicode_minus'] = False

# Enhanced NKAT-Transformer v2.0ã®ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from nkat_enhanced_transformer_v2 import (
    EnhancedNKATConfig, 
    EnhancedNKATVisionTransformer,
    AdvancedDataAugmentation
)

def load_model(checkpoint_path):
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰"""
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # è¨­å®šã®å¾©å…ƒ
    config = EnhancedNKATConfig()
    if 'config' in checkpoint:
        for key, value in checkpoint['config'].items():
            setattr(config, key, value)
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰
    model = EnhancedNKATVisionTransformer(config)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    print(f"Model loaded from epoch {checkpoint.get('epoch', 'unknown')}")
    print(f"Checkpoint accuracy: {checkpoint.get('test_accuracy', 'unknown'):.2f}%")
    
    return model, config, checkpoint

def evaluate_model(model, config, device):
    """ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è©•ä¾¡"""
    model.to(device)
    model.eval()
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    augmentation = AdvancedDataAugmentation(config)
    test_transform = augmentation.create_test_transforms()
    
    test_dataset = torchvision.datasets.MNIST(
        root="data", train=False, download=True, transform=test_transform
    )
    
    test_loader = torch.utils.data.DataLoader(
        test_dataset,
        batch_size=128,
        shuffle=False,
        num_workers=4
    )
    
    # è©•ä¾¡å®Ÿè¡Œ
    correct = 0
    total = 0
    all_preds = []
    all_targets = []
    all_confidences = []
    
    with torch.no_grad():
        for data, target in tqdm(test_loader, desc='Evaluating'):
            data, target = data.to(device), target.to(device)
            
            outputs = model(data)
            
            # äºˆæ¸¬ã¨ä¿¡é ¼åº¦
            probabilities = torch.softmax(outputs, dim=1)
            confidences, predicted = torch.max(probabilities, 1)
            
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_confidences.extend(confidences.cpu().numpy())
    
    accuracy = 100. * correct / total
    print(f"\nðŸŽ¯ Test Results:")
    print(f"â€¢ Accuracy: {accuracy:.4f}%")
    print(f"â€¢ Errors: {total - correct} / {total}")
    print(f"â€¢ Error Rate: {100 * (total - correct) / total:.4f}%")
    
    return accuracy, all_preds, all_targets, all_confidences

def detailed_analysis(preds, targets, confidences, config):
    """è©³ç´°åˆ†æžã¨å¯è¦–åŒ–"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(targets, preds)
    
    # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦ã¨çµ±è¨ˆ
    class_stats = {}
    for i in range(10):
        class_mask = np.array(targets) == i
        if np.sum(class_mask) > 0:
            class_accuracy = np.sum(np.array(preds)[class_mask] == i) / np.sum(class_mask) * 100
            class_confidence = np.mean(np.array(confidences)[class_mask])
            class_stats[i] = {
                'accuracy': class_accuracy,
                'confidence': class_confidence,
                'samples': np.sum(class_mask)
            }
    
    # ã‚¨ãƒ©ãƒ¼åˆ†æž
    errors = []
    for i in range(len(targets)):
        if targets[i] != preds[i]:
            errors.append({
                'true': targets[i],
                'pred': preds[i],
                'confidence': confidences[i]
            })
    
    # å¯è¦–åŒ–
    plt.figure(figsize=(20, 15))
    
    # 1. æ··åŒè¡Œåˆ—
    plt.subplot(2, 3, 1)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
               xticklabels=range(10), yticklabels=range(10))
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    
    # 2. ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦
    plt.subplot(2, 3, 2)
    classes = list(class_stats.keys())
    accuracies = [class_stats[c]['accuracy'] for c in classes]
    colors = ['red' if c in config.difficult_classes else 'blue' for c in classes]
    
    bars = plt.bar(classes, accuracies, color=colors, alpha=0.7)
    plt.title('Class-wise Accuracy')
    plt.xlabel('Class')
    plt.ylabel('Accuracy (%)')
    plt.ylim(90, 100)
    
    for bar, acc in zip(bars, accuracies):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                f'{acc:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # 3. ã‚¯ãƒ©ã‚¹åˆ¥ä¿¡é ¼åº¦
    plt.subplot(2, 3, 3)
    confidences_by_class = [class_stats[c]['confidence'] for c in classes]
    plt.bar(classes, confidences_by_class, color='green', alpha=0.7)
    plt.title('Class-wise Confidence')
    plt.xlabel('Class')
    plt.ylabel('Average Confidence')
    
    # 4. ã‚¨ãƒ©ãƒ¼åˆ†å¸ƒ
    plt.subplot(2, 3, 4)
    error_pairs = {}
    for error in errors:
        pair = f"{error['true']}â†’{error['pred']}"
        error_pairs[pair] = error_pairs.get(pair, 0) + 1
    
    # ãƒˆãƒƒãƒ—10ã‚¨ãƒ©ãƒ¼ãƒšã‚¢
    top_errors = sorted(error_pairs.items(), key=lambda x: x[1], reverse=True)[:10]
    if top_errors:
        pairs, counts = zip(*top_errors)
        plt.barh(range(len(pairs)), counts)
        plt.yticks(range(len(pairs)), pairs)
        plt.xlabel('Error Count')
        plt.title('Top 10 Error Patterns')
    
    # 5. ä¿¡é ¼åº¦åˆ†å¸ƒ
    plt.subplot(2, 3, 5)
    plt.hist(confidences, bins=50, alpha=0.7, color='blue', edgecolor='black')
    plt.axvline(np.mean(confidences), color='red', linestyle='--', 
                label=f'Mean: {np.mean(confidences):.3f}')
    plt.xlabel('Confidence Score')
    plt.ylabel('Frequency')
    plt.title('Confidence Distribution')
    plt.legend()
    
    # 6. å›°é›£ã‚¯ãƒ©ã‚¹åˆ†æž
    plt.subplot(2, 3, 6)
    difficult_acc = [class_stats[c]['accuracy'] for c in config.difficult_classes]
    difficult_labels = [f'Class {c}' for c in config.difficult_classes]
    
    bars = plt.bar(difficult_labels, difficult_acc, color='red', alpha=0.7)
    plt.title('Difficult Classes Performance')
    plt.ylabel('Accuracy (%)')
    plt.ylim(95, 100)
    
    for bar, acc in zip(bars, difficult_acc):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                f'{acc:.1f}%', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(f'figures/nkat_enhanced_v2_test_analysis_{timestamp}.png', 
                dpi=300, bbox_inches='tight')
    plt.show()
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    overall_accuracy = np.mean(np.array(preds) == np.array(targets)) * 100
    
    report = {
        'timestamp': timestamp,
        'model_type': 'Enhanced NKAT-Transformer v2.0',
        'overall_accuracy': float(overall_accuracy),
        'total_samples': len(targets),
        'total_errors': len(errors),
        'error_rate': float(len(errors) / len(targets) * 100),
        'average_confidence': float(np.mean(confidences)),
        'class_statistics': {
            str(k): {
                'accuracy': float(v['accuracy']),
                'confidence': float(v['confidence']),
                'samples': int(v['samples'])
            } for k, v in class_stats.items()
        },
        'difficult_classes_performance': {
            str(c): float(class_stats[c]['accuracy']) 
            for c in config.difficult_classes
        },
        'top_error_patterns': dict(top_errors[:10]) if top_errors else {},
        'confusion_matrix': cm.tolist(),
        'target_achievement': {
            '99_percent': bool(overall_accuracy >= 99.0),
            'vs_previous': f"{overall_accuracy:.2f}% vs 97.79% (previous best)"
        }
    }
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    with open(f'analysis/nkat_enhanced_v2_test_report_{timestamp}.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    return report

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ðŸ” Enhanced NKAT-Transformer v2.0 Model Testing")
    print("=" * 60)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    checkpoint_path = 'checkpoints/nkat_enhanced_v2_best.pth'
    try:
        model, config, checkpoint = load_model(checkpoint_path)
        print(f"\nâœ… Model loaded successfully")
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"Model parameters: {total_params:,}")
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return
    
    # è©•ä¾¡å®Ÿè¡Œ
    print(f"\nðŸ§ª Starting evaluation...")
    accuracy, preds, targets, confidences = evaluate_model(model, config, device)
    
    # è©³ç´°åˆ†æž
    print(f"\nðŸ“Š Generating detailed analysis...")
    report = detailed_analysis(preds, targets, confidences, config)
    
    # çµæžœã‚µãƒžãƒªãƒ¼
    print(f"\n" + "=" * 60)
    print(f"ðŸŽ¯ ENHANCED NKAT-TRANSFORMER V2.0 TEST RESULTS")
    print(f"=" * 60)
    print(f"â€¢ Overall Accuracy: {accuracy:.4f}%")
    print(f"â€¢ Previous Best: 97.79%")
    print(f"â€¢ Improvement: {accuracy - 97.79:+.2f}%")
    print(f"â€¢ Target (99%+): {'âœ… ACHIEVED' if accuracy >= 99.0 else 'ðŸ”„ IN PROGRESS'}")
    
    # å›°é›£ã‚¯ãƒ©ã‚¹æ€§èƒ½
    print(f"\nðŸ“ˆ Difficult Classes Performance:")
    for cls in config.difficult_classes:
        cls_acc = report['class_statistics'][str(cls)]['accuracy']
        print(f"â€¢ Class {cls}: {cls_acc:.2f}%")
    
    # ã‚¨ãƒ©ãƒ¼åˆ†æž
    print(f"\nâŒ Error Analysis:")
    print(f"â€¢ Total Errors: {len(targets) - sum(np.array(preds) == np.array(targets))}")
    print(f"â€¢ Error Rate: {100 * (1 - accuracy/100):.4f}%")
    
    if report['top_error_patterns']:
        print(f"â€¢ Top Error Pattern: {list(report['top_error_patterns'].keys())[0]} " +
              f"({list(report['top_error_patterns'].values())[0]} times)")
    
    print(f"\nðŸ’¾ Analysis saved: analysis/nkat_enhanced_v2_test_report_{report['timestamp']}.json")
    print(f"ðŸ“Š Visualization: figures/nkat_enhanced_v2_test_analysis_{report['timestamp']}.png")
    
    if accuracy >= 99.0:
        print(f"\nðŸŽ‰ CONGRATULATIONS!")
        print(f"Enhanced NKAT-Transformer v2.0 has achieved 99%+ accuracy!")
        print(f"Ready for production deployment and publication.")
    else:
        improvement_needed = 99.0 - accuracy
        print(f"\nðŸ“ˆ Progress towards 99%:")
        print(f"Current: {accuracy:.2f}% | Target: 99.00% | Gap: {improvement_needed:.2f}%")
    
    print(f"\n" + "=" * 60)
    print(f"Enhanced NKAT-Transformer v2.0 Testing Complete!")
    print(f"=" * 60)

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
"""
ğŸ¯ NKAT Optuna ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
LLMã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å°å…¥ + TPEæŒ‡æ¨™æœ€é©åŒ–

æ–°è¦å°å…¥ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
- Temperature: Attention/Logitsæ¸©åº¦åˆ¶å¾¡
- TopK: Attentionä¸Šä½Kå€‹é¸æŠ
- TopP: Nucleus Attention ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
- Multi-scale Dropout: å±¤åˆ¥Dropoutç‡
- Dynamic NKAT strength: é©å¿œçš„ç†è«–å¼·åº¦
- Regularization coefficients: L1/L2æ­£å‰‡åŒ–

ç›®çš„é–¢æ•°: TPE (Theory-Practical Equilibrium) ã‚¹ã‚³ã‚¢æœ€å¤§åŒ–
RTX3080æœ€é©åŒ–ã€tqdmé€²æ—ã€è‹±èªã‚°ãƒ©ãƒ•å¯¾å¿œ
"""

import os
import sys
import json
import numpy as np
import pandas as pd
from datetime import datetime
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

import optuna
from optuna.integration import TensorBoardCallback
from optuna.samplers import TPESampler
from optuna.pruners import MedianPruner

# CUDAæœ€é©åŒ–
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    print(f"ğŸš€ RTX3080 CUDA Optimization: {torch.cuda.get_device_name(0)}")

# è‹±èªã‚°ãƒ©ãƒ•è¨­å®šï¼ˆæ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

class LLMStyleAttention(nn.Module):
    """LLMã‚¹ã‚¿ã‚¤ãƒ«ã®Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼ˆTemperatureã€TopK/TopPå¯¾å¿œï¼‰"""
    
    def __init__(self, embed_dim, num_heads, temperature=1.0, top_k=None, top_p=None):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.temperature = temperature
        self.top_k = top_k
        self.top_p = top_p
        
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)
        
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        B, S, D = x.shape
        
        # Q, K, V projection
        q = self.q_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(B, S, self.num_heads, self.head_dim).transpose(1, 2)
        
        # Scaled dot-product attention with temperature
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        scores = scores / self.temperature  # Temperature scaling
        
        # TopK filtering
        if self.top_k is not None and self.top_k > 0:
            top_k = min(self.top_k, scores.size(-1))
            top_k_scores, _ = torch.topk(scores, top_k, dim=-1)
            mask = scores < top_k_scores[..., -1:, :]
            scores = scores.masked_fill(mask, float('-inf'))
        
        # TopP (Nucleus) filtering
        if self.top_p is not None and self.top_p < 1.0:
            sorted_scores, sorted_indices = torch.sort(scores, descending=True, dim=-1)
            sorted_probs = F.softmax(sorted_scores, dim=-1)
            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
            
            # Find cutoff index for nucleus
            nucleus_mask = cumulative_probs <= self.top_p
            nucleus_mask[..., 1:] = nucleus_mask[..., :-1].clone()
            nucleus_mask[..., 0] = True
            
            # Apply nucleus mask
            cutoff_indices = torch.gather(sorted_indices, -1, nucleus_mask.long().sum(-1, keepdim=True) - 1)
            nucleus_scores = torch.gather(sorted_scores, -1, cutoff_indices)
            mask = scores < nucleus_scores
            scores = scores.masked_fill(mask, float('-inf'))
        
        # Standard attention
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        out = torch.matmul(attn_weights, v)
        out = out.transpose(1, 2).contiguous().view(B, S, D)
        out = self.out_proj(out)
        
        return out, attn_weights

class NKATOptimizedTransformer(nn.Module):
    """Optunaæœ€é©åŒ–å¯¾å¿œNKAT-Transformer"""
    
    def __init__(self, 
                 img_size=28, 
                 patch_size=4, 
                 num_classes=10,
                 embed_dim=512, 
                 depth=6, 
                 num_heads=8, 
                 mlp_ratio=4.0,
                 # LLMã‚¹ã‚¿ã‚¤ãƒ«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                 temperature=1.0,
                 top_k=None,
                 top_p=None,
                 nkat_strength=0.0,
                 nkat_decay=1.0,  # NKATå¼·åº¦ã®æ¸›è¡°
                 # Dropoutç³»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                 dropout_attention=0.1,
                 dropout_mlp=0.1,
                 dropout_embedding=0.1,
                 dropout_classifier=0.1,
                 # æ­£å‰‡åŒ–ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                 l1_reg=0.0,
                 l2_reg=0.0):
        super().__init__()
        
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.embed_dim = embed_dim
        self.nkat_strength = nkat_strength
        self.nkat_decay = nkat_decay
        self.l1_reg = l1_reg
        self.l2_reg = l2_reg
        
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ï¼ˆå®Ÿç¸¾ã®ã‚ã‚‹æ®µéšçš„ConvStemï¼‰
        self.patch_embedding = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.GELU(),
            nn.Dropout2d(dropout_embedding),
            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(128),
            nn.GELU(),
            nn.Dropout2d(dropout_embedding),
            nn.Conv2d(128, embed_dim, kernel_size=patch_size, stride=patch_size)
        )
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        self.pos_embedding = nn.Parameter(
            torch.randn(1, self.num_patches + 1, embed_dim) * 0.02
        )
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)
        
        # å…¥åŠ›æ­£è¦åŒ–
        self.input_norm = nn.LayerNorm(embed_dim)
        self.embedding_dropout = nn.Dropout(dropout_embedding)
        
        # LLMã‚¹ã‚¿ã‚¤ãƒ«Transformer layers
        self.layers = nn.ModuleList([
            self._create_llm_style_layer(
                embed_dim, num_heads, mlp_ratio, 
                temperature, top_k, top_p,
                dropout_attention, dropout_mlp
            ) for _ in range(depth)
        ])
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰ï¼ˆå±¤åˆ¥Dropoutå¯¾å¿œï¼‰
        self.classifier = nn.Sequential(
            nn.LayerNorm(embed_dim),
            nn.Dropout(dropout_classifier),
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout_classifier * 0.5),
            nn.Linear(embed_dim // 2, embed_dim // 4),
            nn.GELU(),
            nn.Dropout(dropout_classifier * 0.5),
            nn.Linear(embed_dim // 4, num_classes)
        )
        
        # Logits temperatureåˆ¶å¾¡
        self.logits_temperature = nn.Parameter(torch.tensor(temperature))
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
    
    def _create_llm_style_layer(self, embed_dim, num_heads, mlp_ratio, 
                               temperature, top_k, top_p, dropout_attn, dropout_mlp):
        """LLMã‚¹ã‚¿ã‚¤ãƒ«Transformerãƒ¬ã‚¤ãƒ¤ãƒ¼ä½œæˆ"""
        return nn.ModuleDict({
            'norm1': nn.LayerNorm(embed_dim),
            'attention': LLMStyleAttention(embed_dim, num_heads, temperature, top_k, top_p),
            'norm2': nn.LayerNorm(embed_dim),
            'mlp': nn.Sequential(
                nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),
                nn.GELU(),
                nn.Dropout(dropout_mlp),
                nn.Linear(int(embed_dim * mlp_ratio), embed_dim),
                nn.Dropout(dropout_mlp)
            ),
            'dropout_attn': nn.Dropout(dropout_attn),
            'dropout_mlp': nn.Dropout(dropout_mlp)
        })
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x, layer_idx=None):
        B = x.shape[0]
        
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        x = self.patch_embedding(x)  # (B, embed_dim, H//patch_size, W//patch_size)
        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches + 1, embed_dim)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        x = x + self.pos_embedding
        x = self.input_norm(x)
        x = self.embedding_dropout(x)
        
        # å‹•çš„NKATé©å¿œï¼ˆå±¤ã«ã‚ˆã‚‹æ¸›è¡°ï¼‰
        if self.nkat_strength > 0:
            current_strength = self.nkat_strength
            if layer_idx is not None:
                current_strength *= (self.nkat_decay ** layer_idx)
            
            # ãƒãƒ£ãƒãƒ«æ–¹å‘å¹³å‡ã§ã®é©å¿œçš„èª¿æ•´
            mean_activation = x.mean(dim=-1, keepdim=True)
            nkat_factor = 1.0 + current_strength * 0.01 * torch.tanh(mean_activation)
            x = x * nkat_factor
        
        # LLMã‚¹ã‚¿ã‚¤ãƒ«Transformerå‡¦ç†
        for i, layer in enumerate(self.layers):
            residual = x
            
            # Self-attention
            x = layer['norm1'](x)
            attn_out, _ = layer['attention'](x)
            x = residual + layer['dropout_attn'](attn_out)
            
            # MLP
            residual = x
            x = layer['norm2'](x)
            mlp_out = layer['mlp'](x)
            x = residual + layer['dropout_mlp'](mlp_out)
            
            # å±¤ã”ã¨ã®å‹•çš„NKATé©å¿œ
            if self.nkat_strength > 0:
                current_strength = self.nkat_strength * (self.nkat_decay ** i)
                if current_strength > 1e-4:  # é–¾å€¤ä»¥ä¸‹ã¯ç„¡è¦–
                    mean_activation = x.mean(dim=-1, keepdim=True)
                    nkat_factor = 1.0 + current_strength * 0.01 * torch.tanh(mean_activation)
                    x = x * nkat_factor
        
        # åˆ†é¡ï¼ˆTemperatureåˆ¶å¾¡ä»˜ãï¼‰
        cls_output = x[:, 0]  # (B, embed_dim)
        logits = self.classifier(cls_output)
        logits = logits / self.logits_temperature  # Temperature scaling
        
        return logits
    
    def get_regularization_loss(self):
        """L1/L2æ­£å‰‡åŒ–æå¤±è¨ˆç®—"""
        l1_loss = 0
        l2_loss = 0
        
        for param in self.parameters():
            if param.requires_grad:
                l1_loss += torch.sum(torch.abs(param))
                l2_loss += torch.sum(param ** 2)
        
        return self.l1_reg * l1_loss + self.l2_reg * l2_loss

def calculate_tpe_score(val_accuracy, theory_params, total_params, complexity_penalty=0.1):
    """TPE (Theory-Practical Equilibrium) ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    lambda_theory = max(theory_params, 1)
    complexity_factor = np.log10(1 + lambda_theory) + complexity_penalty
    tpe = val_accuracy / complexity_factor
    return tpe

def count_theory_parameters(model):
    """ç†è«–çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
    theory_params = 0
    
    # NKATé–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    if hasattr(model, 'nkat_strength') and model.nkat_strength > 0:
        theory_params += 100  # NKATç†è«–çš„å¯„ä¸
    
    # LLMã‚¹ã‚¿ã‚¤ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    if hasattr(model, 'logits_temperature'):
        theory_params += model.logits_temperature.numel()
    
    # Attentionç‰¹æ®Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    for layer in model.layers:
        if hasattr(layer['attention'], 'temperature'):
            theory_params += 10  # Temperatureå¯„ä¸
    
    # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ã®ç†è«–çš„å¯„ä¸ï¼ˆ30%ï¼‰
    for param in model.patch_embedding.parameters():
        theory_params += int(param.numel() * 0.3)
    
    return theory_params

def train_and_evaluate_trial(trial_params, num_epochs=15, device='cuda'):
    """å˜ä¸€è©¦è¡Œã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»è©•ä¾¡"""
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    train_transform = transforms.Compose([
        transforms.RandomRotation(degrees=10),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = torchvision.datasets.MNIST(
        root='./data', train=True, download=True, transform=train_transform
    )
    test_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=test_transform
    )
    
    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=4)
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = NKATOptimizedTransformer(**trial_params).to(device)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚«ã‚¦ãƒ³ãƒˆ
    total_params = sum(p.numel() for p in model.parameters())
    theory_params = count_theory_parameters(model)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    criterion = nn.CrossEntropyLoss(label_smoothing=trial_params.get('label_smoothing', 0.08))
    optimizer = optim.AdamW(
        model.parameters(), 
        lr=trial_params.get('learning_rate', 1e-4),
        weight_decay=trial_params.get('weight_decay', 2e-4)
    )
    
    if trial_params.get('use_cosine_lr', True):
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=num_epochs, eta_min=trial_params.get('min_lr', 1e-6)
        )
    else:
        scheduler = None
    
    scaler = torch.amp.GradScaler('cuda')
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    model.train()
    best_accuracy = 0
    
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        correct = 0
        total = 0
        
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            
            try:
                with torch.amp.autocast('cuda'):
                    output = model(data, layer_idx=epoch // 3)  # å‹•çš„å±¤åˆ¶å¾¡
                    loss = criterion(output, target)
                    
                    # æ­£å‰‡åŒ–æå¤±è¿½åŠ 
                    reg_loss = model.get_regularization_loss()
                    total_loss = loss + reg_loss
                
                # æ•°å€¤å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
                if torch.isnan(total_loss) or torch.isinf(total_loss):
                    print(f"âŒ Numerical instability detected")
                    continue
                
                scaler.scale(total_loss).backward()
                
                # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                if trial_params.get('use_grad_clip', True):
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(
                        model.parameters(), 
                        max_norm=trial_params.get('grad_clip_norm', 1.0)
                    )
                
                scaler.step(optimizer)
                scaler.update()
                
            except RuntimeError as e:
                print(f"âŒ Runtime error: {e}")
                torch.cuda.empty_cache()
                continue
            
            epoch_loss += total_loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
        
        if scheduler is not None:
            scheduler.step()
        
        # ä¸­é–“è©•ä¾¡ï¼ˆPruningç”¨ï¼‰
        if epoch % 5 == 4:
            model.eval()
            test_correct = 0
            test_total = 0
            
            with torch.no_grad():
                for data, target in test_loader:
                    data, target = data.to(device), target.to(device)
                    output = model(data)
                    pred = output.argmax(dim=1, keepdim=True)
                    test_correct += pred.eq(target.view_as(pred)).sum().item()
                    test_total += target.size(0)
            
            intermediate_accuracy = 100. * test_correct / test_total
            best_accuracy = max(best_accuracy, intermediate_accuracy)
            model.train()
    
    # æœ€çµ‚ãƒ†ã‚¹ãƒˆè©•ä¾¡
    model.eval()
    test_correct = 0
    test_total = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1, keepdim=True)
            test_correct += pred.eq(target.view_as(pred)).sum().item()
            test_total += target.size(0)
    
    final_accuracy = 100. * test_correct / test_total
    
    # TPEã‚¹ã‚³ã‚¢è¨ˆç®—
    tpe_score = calculate_tpe_score(final_accuracy / 100.0, theory_params, total_params)
    
    return {
        'accuracy': final_accuracy,
        'tpe_score': tpe_score,
        'total_params': total_params,
        'theory_params': theory_params
    }

def objective(trial):
    """Optunaç›®çš„é–¢æ•°ï¼šTPEã‚¹ã‚³ã‚¢æœ€å¤§åŒ–"""
    
    # LLMã‚¹ã‚¿ã‚¤ãƒ«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢
    temperature = trial.suggest_float('temperature', 0.5, 2.0)
    top_k = trial.suggest_int('top_k', 0, 20) if trial.suggest_categorical('use_top_k', [True, False]) else None
    top_p = trial.suggest_float('top_p', 0.1, 1.0) if trial.suggest_categorical('use_top_p', [True, False]) else None
    
    # NKATé–¢é€£
    nkat_strength = trial.suggest_float('nkat_strength', 0.0, 0.05)
    nkat_decay = trial.suggest_float('nkat_decay', 0.8, 1.0)
    
    # Dropoutç³»
    dropout_attention = trial.suggest_float('dropout_attention', 0.0, 0.3)
    dropout_mlp = trial.suggest_float('dropout_mlp', 0.0, 0.3)
    dropout_embedding = trial.suggest_float('dropout_embedding', 0.0, 0.2)
    dropout_classifier = trial.suggest_float('dropout_classifier', 0.0, 0.4)
    
    # æ­£å‰‡åŒ–
    l1_reg = trial.suggest_float('l1_reg', 0.0, 1e-4, log=True) if trial.suggest_categorical('use_l1', [True, False]) else 0.0
    l2_reg = trial.suggest_float('l2_reg', 0.0, 1e-3, log=True) if trial.suggest_categorical('use_l2', [True, False]) else 0.0
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®š
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)
    label_smoothing = trial.suggest_float('label_smoothing', 0.0, 0.15)
    grad_clip_norm = trial.suggest_float('grad_clip_norm', 0.5, 2.0)
    min_lr = trial.suggest_float('min_lr', 1e-7, 1e-5, log=True)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¾æ›¸ä½œæˆ
    trial_params = {
        'temperature': temperature,
        'top_k': top_k,
        'top_p': top_p,
        'nkat_strength': nkat_strength,
        'nkat_decay': nkat_decay,
        'dropout_attention': dropout_attention,
        'dropout_mlp': dropout_mlp,
        'dropout_embedding': dropout_embedding,
        'dropout_classifier': dropout_classifier,
        'l1_reg': l1_reg,
        'l2_reg': l2_reg,
        'learning_rate': learning_rate,
        'weight_decay': weight_decay,
        'label_smoothing': label_smoothing,
        'grad_clip_norm': grad_clip_norm,
        'min_lr': min_lr,
        'use_cosine_lr': True,
        'use_grad_clip': True
    }
    
    # GPUä½¿ç”¨å¯èƒ½æ€§ç¢ºèª
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    try:
        result = train_and_evaluate_trial(trial_params, num_epochs=12, device=device)
        return result['tpe_score']  # TPEã‚¹ã‚³ã‚¢ã‚’æœ€å¤§åŒ–
        
    except Exception as e:
        print(f"Trial failed: {e}")
        return 0.0  # å¤±æ•—æ™‚ã¯æœ€ä½ã‚¹ã‚³ã‚¢

def run_optuna_optimization(n_trials=50, timeout=3600):
    """Optunaæœ€é©åŒ–å®Ÿè¡Œ"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print("ğŸ¯ NKAT Optuna ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–é–‹å§‹")
    print(f"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {timestamp}")
    print(f"ğŸ”¢ è©¦è¡Œå›æ•°: {n_trials}")
    print(f"â±ï¸  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {timeout}ç§’")
    
    # Optunaè¨­å®š
    sampler = TPESampler(seed=42)
    pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=5)
    
    study = optuna.create_study(
        direction='maximize',  # TPEã‚¹ã‚³ã‚¢æœ€å¤§åŒ–
        sampler=sampler,
        pruner=pruner,
        study_name=f'nkat_optimization_{timestamp}'
    )
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    study.optimize(
        objective, 
        n_trials=n_trials, 
        timeout=timeout,
        show_progress_bar=True
    )
    
    # çµæœåˆ†æ
    best_trial = study.best_trial
    
    print(f"\nğŸ† æœ€é©åŒ–å®Œäº†!")
    print(f"ğŸ“Š æœ€é«˜TPEã‚¹ã‚³ã‚¢: {best_trial.value:.6f}")
    print(f"ğŸ¯ æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    
    for key, value in best_trial.params.items():
        print(f"  {key}: {value}")
    
    # çµæœä¿å­˜
    results = {
        'best_tpe_score': best_trial.value,
        'best_params': best_trial.params,
        'n_trials': len(study.trials),
        'timestamp': timestamp
    }
    
    # å¯è¦–åŒ–ä½œæˆ
    create_optimization_visualization(study, timestamp)
    
    # çµæœJSONä¿å­˜
    results_file = f'nkat_optuna_results_{timestamp}.json'
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ çµæœä¿å­˜: {results_file}")
    
    return study, best_trial

def create_optimization_visualization(study, timestamp):
    """æœ€é©åŒ–çµæœå¯è¦–åŒ–"""
    
    fig = plt.figure(figsize=(20, 12))
    
    # 1. æœ€é©åŒ–å±¥æ­´
    plt.subplot(2, 3, 1)
    trials = study.trials
    values = [t.value for t in trials if t.value is not None]
    plt.plot(values, alpha=0.7)
    plt.xlabel('Trial')
    plt.ylabel('TPE Score')
    plt.title('ğŸ¯ Optimization History')
    plt.grid(True, alpha=0.3)
    
    # 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é‡è¦åº¦
    plt.subplot(2, 3, 2)
    try:
        importances = optuna.importance.get_param_importances(study)
        params = list(importances.keys())[:10]  # ä¸Šä½10å€‹
        values = [importances[p] for p in params]
        
        plt.barh(params, values)
        plt.xlabel('Importance')
        plt.title('ğŸ“Š Parameter Importance')
    except:
        plt.text(0.5, 0.5, 'Importance analysis\nnot available', 
                ha='center', va='center', transform=plt.gca().transAxes)
    
    # 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç›¸é–¢ï¼ˆä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
    plt.subplot(2, 3, 3)
    df = study.trials_dataframe()
    if len(df) > 10:
        # ä¸»è¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç›¸é–¢
        key_params = ['params_temperature', 'params_nkat_strength', 'params_learning_rate']
        existing_params = [p for p in key_params if p in df.columns]
        
        if len(existing_params) >= 2:
            corr_matrix = df[existing_params + ['value']].corr()
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)
            plt.title('ğŸ”— Parameter Correlations')
        else:
            plt.text(0.5, 0.5, 'Insufficient data\nfor correlation', 
                    ha='center', va='center', transform=plt.gca().transAxes)
    else:
        plt.text(0.5, 0.5, 'Insufficient trials\nfor correlation', 
                ha='center', va='center', transform=plt.gca().transAxes)
    
    # 4. Temperature vs Performance
    plt.subplot(2, 3, 4)
    if 'params_temperature' in df.columns:
        plt.scatter(df['params_temperature'], df['value'], alpha=0.6)
        plt.xlabel('Temperature')
        plt.ylabel('TPE Score')
        plt.title('ğŸŒ¡ï¸ Temperature vs Performance')
        plt.grid(True, alpha=0.3)
    else:
        plt.text(0.5, 0.5, 'Temperature data\nnot available', 
                ha='center', va='center', transform=plt.gca().transAxes)
    
    # 5. NKAT Strength vs Performance  
    plt.subplot(2, 3, 5)
    if 'params_nkat_strength' in df.columns:
        plt.scatter(df['params_nkat_strength'], df['value'], alpha=0.6, color='orange')
        plt.xlabel('NKAT Strength')
        plt.ylabel('TPE Score')
        plt.title('âš¡ NKAT Strength vs Performance')
        plt.grid(True, alpha=0.3)
    else:
        plt.text(0.5, 0.5, 'NKAT Strength data\nnot available', 
                ha='center', va='center', transform=plt.gca().transAxes)
    
    # 6. æœ€é©è§£çµ±è¨ˆ
    plt.subplot(2, 3, 6)
    plt.axis('off')
    
    best_trial = study.best_trial
    stats_text = f"""
ğŸ† Optimization Summary

ğŸ“Š Best TPE Score: {best_trial.value:.6f}
ğŸ”¢ Total Trials: {len(study.trials)}
â±ï¸ Best Trial: #{best_trial.number}

ğŸ¯ Key Parameters:
Temperature: {best_trial.params.get('temperature', 'N/A'):.3f}
NKAT Strength: {best_trial.params.get('nkat_strength', 'N/A'):.4f}
Learning Rate: {best_trial.params.get('learning_rate', 'N/A'):.2e}
"""
    
    plt.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen", alpha=0.7))
    
    plt.tight_layout()
    
    # ä¿å­˜
    viz_file = f'nkat_optuna_optimization_analysis_{timestamp}.png'
    plt.savefig(viz_file, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š æœ€é©åŒ–å¯è¦–åŒ–ä¿å­˜: {viz_file}")
    
    return viz_file

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='NKAT Optuna ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–')
    parser.add_argument('--n_trials', type=int, default=30, help='æœ€é©åŒ–è©¦è¡Œå›æ•°')
    parser.add_argument('--timeout', type=int, default=3600, help='ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰')
    parser.add_argument('--device', default='cuda', help='ãƒ‡ãƒã‚¤ã‚¹')
    
    args = parser.parse_args()
    
    print("ğŸ¯ NKAT Optuna ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ")
    print("ğŸ”¥ LLMã‚¹ã‚¿ã‚¤ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ + TPEæŒ‡æ¨™æœ€é©åŒ–")
    
    # GPUç¢ºèª
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    if device.type == 'cuda':
        print(f"ğŸš€ CUDA ãƒ‡ãƒã‚¤ã‚¹: {torch.cuda.get_device_name(0)}")
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    study, best_trial = run_optuna_optimization(args.n_trials, args.timeout)
    
    print(f"\nğŸ‰ æœ€é©åŒ–å®Œäº†ï¼")
    print(f"ğŸ† æœ€é«˜TPEã‚¹ã‚³ã‚¢: {best_trial.value:.6f}")
    print(f"ğŸ’¡ ã“ã‚Œã§NKAT-Transformerã®ç†è«–â‡”å·¥å­¦ãƒãƒ©ãƒ³ã‚¹ãŒå®Œç’§ã«æœ€é©åŒ–ã•ã‚Œã¾ã—ãŸï¼")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKAT-Transformer Performance Optimization Strategy
97.79% â†’ 99%+ Accuracy Enhancement Plan

ç¾åœ¨ã®åˆ†æçµæœã«åŸºã¥ãåŒ…æ‹¬çš„æ”¹å–„æˆ¦ç•¥:
1. æ··åˆç²¾åº¦å•é¡Œã®è§£æ±º
2. å›°é›£ã‚¯ãƒ©ã‚¹(5,9,7)ã®ç‰¹åˆ¥å¯¾ç­–
3. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®å¼·åŒ–
4. ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã®æœ€é©åŒ–
5. è¨“ç·´æˆ¦ç•¥ã®æ”¹å–„

Author: NKAT Advanced Computing Team
Date: 2025-06-01
Target: 99%+ Test Accuracy on MNIST
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader, WeightedRandomSampler
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import json
import os
from datetime import datetime
from tqdm import tqdm
import logging

# è‹±èªè¡¨è¨˜è¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.size'] = 12
plt.rcParams['axes.unicode_minus'] = False

class NKATOptimizationConfig:
    """æœ€é©åŒ–è¨­å®š"""
    
    def __init__(self):
        # åŸºæœ¬è¨­å®šï¼ˆæ—¢å­˜ï¼‰
        self.image_size = 28
        self.patch_size = 7
        self.num_patches = (self.image_size // self.patch_size) ** 2
        self.channels = 1
        self.num_classes = 10
        
        # æ”¹å–„ç‰ˆãƒ¢ãƒ‡ãƒ«è¨­å®š
        self.d_model = 512  # 384 â†’ 512ã«å¢—åŠ 
        self.nhead = 8      # 6 â†’ 8ã«å¢—åŠ 
        self.num_layers = 12  # 8 â†’ 12ã«å¢—åŠ ï¼ˆæ·±ã•å‘ä¸Šï¼‰
        self.dim_feedforward = 2048  # 1536 â†’ 2048ã«å¢—åŠ 
        self.dropout = 0.08  # 0.1 â†’ 0.08ã«èª¿æ•´
        
        # æ··åˆç²¾åº¦å¯¾ç­–
        self.use_mixed_precision = False  # å®‰å®šæ€§ã®ãŸã‚ç„¡åŠ¹åŒ–
        self.use_gradient_clipping = True
        self.max_grad_norm = 1.0
        
        # å›°é›£ã‚¯ãƒ©ã‚¹å¯¾ç­–
        self.use_class_weights = True
        self.difficult_classes = [5, 7, 9]  # åˆ†æçµæœã‚ˆã‚Š
        self.class_weight_boost = 1.5
        
        # å¼·åŒ–ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        self.use_advanced_augmentation = True
        self.rotation_range = 15  # 10 â†’ 15åº¦ã«æ‹¡å¤§
        self.zoom_range = 0.15
        self.use_elastic_deformation = True
        self.use_random_erasing = True
        
        # è¨“ç·´æˆ¦ç•¥æ”¹å–„
        self.num_epochs = 100  # 50 â†’ 100ã«å»¶é•·
        self.batch_size = 64   # 128 â†’ 64ã«èª¿æ•´ï¼ˆå®‰å®šæ€§å‘ä¸Šï¼‰
        self.learning_rate = 1e-4  # 3e-4 â†’ 1e-4ã«èª¿æ•´
        self.use_cosine_restart = True
        self.restart_period = 20
        
        # æ­£å‰‡åŒ–å¼·åŒ–
        self.use_label_smoothing = True
        self.label_smoothing = 0.08  # 0.05 â†’ 0.08ã«å¼·åŒ–
        self.use_mixup = True
        self.mixup_alpha = 0.4
        self.weight_decay = 2e-4  # 1e-4 â†’ 2e-4ã«å¼·åŒ–
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æº–å‚™
        self.save_multiple_models = True
        self.model_variants = 3

class AdvancedDataAugmentation:
    """å¼·åŒ–ç‰ˆãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
    
    def __init__(self, config):
        self.config = config
        
    def create_train_transforms(self):
        """è¨“ç·´ç”¨å¤‰æ›"""
        transforms_list = [
            transforms.ToPILImage(),
            
            # å›è»¢æ‹¡å¼µå¼·åŒ–
            transforms.RandomRotation(
                degrees=self.config.rotation_range,
                fill=0,
                interpolation=transforms.InterpolationMode.BILINEAR
            ),
            
            # ã‚¢ãƒ•ã‚£ãƒ³å¤‰æ›
            transforms.RandomAffine(
                degrees=0,
                translate=(0.1, 0.1),
                scale=(0.9, 1.1),
                shear=5,
                fill=0
            ),
            
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ]
        
        # ãƒ©ãƒ³ãƒ€ãƒ æ¶ˆå»
        if self.config.use_random_erasing:
            transforms_list.append(
                transforms.RandomErasing(
                    p=0.1,
                    scale=(0.02, 0.1),
                    ratio=(0.3, 3.3),
                    value=0
                )
            )
        
        return transforms.Compose(transforms_list)
    
    def elastic_deformation(self, image, alpha=100, sigma=10):
        """å¼¾æ€§å¤‰å½¢ï¼ˆæ‰‹æ›¸ãæ–‡å­—ã®è‡ªç„¶ãªå¤‰å‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰"""
        random_state = np.random.RandomState(None)
        
        shape = image.shape
        dx = random_state.uniform(-1, 1, shape) * alpha
        dy = random_state.uniform(-1, 1, shape) * alpha
        
        x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))
        indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1))
        
        from scipy.ndimage import map_coordinates, gaussian_filter
        dx = gaussian_filter(dx, sigma, mode='constant', cval=0) 
        dy = gaussian_filter(dy, sigma, mode='constant', cval=0)
        
        distorted_image = map_coordinates(image, indices, order=1, mode='reflect')
        return distorted_image.reshape(shape)

class ImprovedNKATVisionTransformer(nn.Module):
    """æ”¹å–„ç‰ˆNKAT Vision Transformer"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # æ”¹å–„ç‰ˆãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        self.patch_embedding = nn.Sequential(
            nn.Conv2d(config.channels, config.d_model // 4, 3, padding=1),
            nn.BatchNorm2d(config.d_model // 4),
            nn.GELU(),
            nn.Conv2d(config.d_model // 4, config.d_model, 
                     config.patch_size, stride=config.patch_size)
        )
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        self.pos_embedding = nn.Parameter(
            torch.randn(1, config.num_patches + 1, config.d_model) * 0.02
        )
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
        self.cls_token = nn.Parameter(
            torch.randn(1, 1, config.d_model) * 0.02
        )
        
        # Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆæ”¹å–„ç‰ˆï¼‰
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=config.d_model,
            nhead=config.nhead,
            dim_feedforward=config.dim_feedforward,
            dropout=config.dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, config.num_layers)
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰ï¼ˆæ”¹å–„ç‰ˆï¼‰
        self.classifier = nn.Sequential(
            nn.LayerNorm(config.d_model),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_model, config.d_model // 2),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.d_model // 2, config.num_classes)
        )
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        """é‡ã¿åˆæœŸåŒ–"""
        if isinstance(m, nn.Linear):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
    
    def forward(self, x):
        batch_size = x.shape[0]
        
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        x = self.patch_embedding(x)  # [B, d_model, H', W']
        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, d_model]
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        x = x + self.pos_embedding
        
        # Transformerå‡¦ç†
        x = self.transformer(x)
        
        # åˆ†é¡
        cls_output = x[:, 0]
        logits = self.classifier(cls_output)
        
        return {
            'logits': logits,
            'cls_features': cls_output,
            'attention_weights': None,  # å¿…è¦ã«å¿œã˜ã¦å®Ÿè£…
            'quantum_contribution': torch.tensor(0.0)  # NKATäº’æ›æ€§ã®ãŸã‚
        }

def create_class_weighted_sampler(dataset, config):
    """ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒ©ãƒ¼"""
    labels = [dataset[i][1] for i in range(len(dataset))]
    class_counts = np.bincount(labels)
    
    # å›°é›£ã‚¯ãƒ©ã‚¹ã®é‡ã¿ã‚’ä¸Šã’ã‚‹
    class_weights = 1.0 / class_counts
    for difficult_class in config.difficult_classes:
        class_weights[difficult_class] *= config.class_weight_boost
    
    sample_weights = [class_weights[label] for label in labels]
    sampler = WeightedRandomSampler(
        weights=sample_weights,
        num_samples=len(sample_weights),
        replacement=True
    )
    
    return sampler

def mixup_data(x, y, alpha=1.0):
    """Mixup ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Mixupæå¤±"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

class OptimizationStrategy:
    """æœ€é©åŒ–æˆ¦ç•¥å®Ÿè¡Œã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def create_optimized_model(self):
        """æœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"""
        model = ImprovedNKATVisionTransformer(self.config).to(self.device)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ç¢ºèª
        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"Optimized model parameters: {total_params:,}")
        
        return model
    
    def create_optimized_dataloader(self):
        """æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼"""
        augmentation = AdvancedDataAugmentation(self.config)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        train_transform = augmentation.create_train_transforms()
        train_dataset = torchvision.datasets.MNIST(
            root="data", train=True, download=True, transform=train_transform
        )
        
        # ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒ©ãƒ¼
        if self.config.use_class_weights:
            sampler = create_class_weighted_sampler(train_dataset, self.config)
            shuffle = False
        else:
            sampler = None
            shuffle = True
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            sampler=sampler,
            shuffle=shuffle,
            num_workers=4,
            pin_memory=True,
            drop_last=True
        )
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        test_dataset = torchvision.datasets.MNIST(
            root="data", train=False, download=True, transform=test_transform
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size * 2,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
        
        return train_loader, test_loader
    
    def create_optimized_optimizer(self, model):
        """æœ€é©åŒ–ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼"""
        optimizer = optim.AdamW(
            model.parameters(),
            lr=self.config.learning_rate,
            weight_decay=self.config.weight_decay,
            betas=(0.9, 0.999),
            eps=1e-8
        )
        
        if self.config.use_cosine_restart:
            scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
                optimizer, 
                T_0=self.config.restart_period,
                T_mult=2,
                eta_min=self.config.learning_rate * 0.01
            )
        else:
            scheduler = optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=self.config.num_epochs,
                eta_min=self.config.learning_rate * 0.1
            )
        
        return optimizer, scheduler
    
    def create_optimized_criterion(self):
        """æœ€é©åŒ–æå¤±é–¢æ•°"""
        if self.config.use_label_smoothing:
            criterion = nn.CrossEntropyLoss(
                label_smoothing=self.config.label_smoothing
            )
        else:
            criterion = nn.CrossEntropyLoss()
        
        return criterion
    
    def generate_optimization_report(self):
        """æœ€é©åŒ–æˆ¦ç•¥ãƒ¬ãƒãƒ¼ãƒˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        report = {
            "optimization_strategy": "NKAT-Transformer 97.79% â†’ 99%+ Enhancement",
            "timestamp": timestamp,
            "current_performance": {
                "test_accuracy": 97.79,
                "problematic_classes": [5, 7, 9],
                "main_confusions": ["3â†’5", "9â†’4", "4â†’9", "7â†’2"]
            },
            "improvements": {
                "model_architecture": {
                    "d_model": f"{384} â†’ {self.config.d_model}",
                    "num_layers": f"{8} â†’ {self.config.num_layers}",
                    "nhead": f"{6} â†’ {self.config.nhead}",
                    "deeper_classifier": True
                },
                "training_strategy": {
                    "mixed_precision": f"Disabled (stability)",
                    "gradient_clipping": self.config.use_gradient_clipping,
                    "class_weights": self.config.use_class_weights,
                    "longer_training": f"{50} â†’ {self.config.num_epochs} epochs"
                },
                "data_augmentation": {
                    "advanced_rotation": f"Â±{self.config.rotation_range}Â°",
                    "elastic_deformation": self.config.use_elastic_deformation,
                    "mixup": self.config.use_mixup,
                    "random_erasing": self.config.use_random_erasing
                },
                "regularization": {
                    "label_smoothing": self.config.label_smoothing,
                    "weight_decay": self.config.weight_decay,
                    "dropout": self.config.dropout
                }
            },
            "expected_improvements": {
                "target_accuracy": "99.0%+",
                "error_reduction": "50%+ (from 2.21% to <1.0%)",
                "problematic_class_boost": "+2-3% for classes 5,7,9"
            }
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        with open(f"analysis/nkat_optimization_strategy_{timestamp}.json", "w") as f:
            json.dump(report, f, indent=2)
        
        return report

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("NKAT-Transformer Performance Optimization Strategy")
    print("=" * 60)
    print("Current Performance: 97.79% â†’ Target: 99%+")
    print("=" * 60)
    
    # è¨­å®š
    config = NKATOptimizationConfig()
    strategy = OptimizationStrategy(config)
    
    print("\nğŸ“Š Current Analysis Summary:")
    print("â€¢ Test Accuracy: 97.79% (221/10000 errors)")
    print("â€¢ Problematic Classes: 5 (96.75%), 7 (97.08%), 9 (96.13%)")
    print("â€¢ Main Confusions: 3â†’5 (15x), 9â†’4 (14x), 4â†’9 (12x)")
    print("â€¢ Issue Found: Mixed precision causing NaN")
    
    print("\nğŸš€ Optimization Strategy:")
    print("1. Model Architecture Enhancement:")
    print(f"   â€¢ d_model: 384 â†’ {config.d_model}")
    print(f"   â€¢ Layers: 8 â†’ {config.num_layers}")
    print(f"   â€¢ Attention heads: 6 â†’ {config.nhead}")
    print("   â€¢ Deeper classification head")
    
    print("\n2. Training Strategy Improvements:")
    print("   â€¢ Mixed precision: DISABLED (stability)")
    print("   â€¢ Class-weighted sampling for difficult classes")
    print(f"   â€¢ Extended training: 50 â†’ {config.num_epochs} epochs")
    print("   â€¢ Gradient clipping for stability")
    
    print("\n3. Advanced Data Augmentation:")
    print(f"   â€¢ Enhanced rotation: Â±{config.rotation_range}Â°")
    print("   â€¢ Elastic deformation")
    print("   â€¢ Mixup augmentation")
    print("   â€¢ Random erasing")
    
    print("\n4. Regularization Enhancement:")
    print(f"   â€¢ Label smoothing: {config.label_smoothing}")
    print(f"   â€¢ Weight decay: {config.weight_decay}")
    print("   â€¢ Cosine annealing with restarts")
    
    # æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = strategy.generate_optimization_report()
    
    print(f"\nâœ… Optimization strategy saved: analysis/nkat_optimization_strategy_{report['timestamp']}.json")
    
    print("\nğŸ¯ Expected Results:")
    print("â€¢ Target Accuracy: 99.0%+")
    print("â€¢ Error Reduction: 50%+ (2.21% â†’ <1.0%)")
    print("â€¢ Class 5,7,9 Boost: +2-3% each")
    print("â€¢ Stable mixed precision alternative")
    
    print("\nğŸ“‹ Next Steps:")
    print("1. Implement improved model architecture")
    print("2. Set up advanced data augmentation pipeline")
    print("3. Configure class-weighted training")
    print("4. Run extended training with monitoring")
    print("5. Validate on full test set")
    
    print("\n" + "=" * 60)
    print("NKAT-Transformer Optimization Strategy Complete!")
    print("Ready for 99%+ accuracy implementation")
    print("=" * 60)

if __name__ == "__main__":
    main() 
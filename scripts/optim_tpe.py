# optim_tpe.py
import optuna
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import gc
import time
import numpy as np
from typing import Dict, Tuple
from tqdm import tqdm
import torch.nn.functional as F

from nkat_transformer.model import NKATVisionTransformer
from utils.metrics import tpe_metric, count_nkat_parameters, comprehensive_model_analysis

# è‹±èªã‚°ãƒ©ãƒ•è¨­å®šï¼ˆæ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
import matplotlib.pyplot as plt
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']

# CUDAæœ€é©åŒ–
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    print(f"ğŸš€ RTX3080 CUDA Optimization: {torch.cuda.get_device_name(0)}")


def get_dataloaders(batch_size: int = 128, num_workers: int = 0) -> Tuple[DataLoader, DataLoader]:
    """
    ğŸ“¦ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰
    Windowså¯¾å¿œï¼šnum_workers=0ã§ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ãƒƒã‚·ãƒ³ã‚°å›é¿
    
    Args:
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        num_workers: ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°ï¼ˆWindowsç’°å¢ƒã§ã¯0æ¨å¥¨ï¼‰
        
    Returns:
        (train_loader, val_loader)
    """
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    # MNIST ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    train_dataset = torchvision.datasets.MNIST(
        root='./data', train=True, download=True, transform=transform
    )
    val_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=transform
    )
    
    # é«˜é€ŸåŒ–ï¼šã‚¯ã‚¤ãƒƒã‚¯è¨“ç·´ç”¨ã«ã‚µãƒ–ã‚»ãƒƒãƒˆä½œæˆ
    quick_train_size = min(10000, len(train_dataset))  # 1ä¸‡ã‚µãƒ³ãƒ—ãƒ«
    quick_val_size = min(2000, len(val_dataset))       # 2åƒã‚µãƒ³ãƒ—ãƒ«
    
    train_indices = torch.randperm(len(train_dataset))[:quick_train_size]
    val_indices = torch.randperm(len(val_dataset))[:quick_val_size]
    
    train_subset = torch.utils.data.Subset(train_dataset, train_indices)
    val_subset = torch.utils.data.Subset(val_dataset, val_indices)
    
    train_loader = DataLoader(
        train_subset, batch_size=batch_size, shuffle=True, 
        num_workers=num_workers, pin_memory=True
    )
    val_loader = DataLoader(
        val_subset, batch_size=batch_size, shuffle=False,
        num_workers=num_workers, pin_memory=True
    )
    
    return train_loader, val_loader


def quick_train_and_eval(model: nn.Module, 
                        lr: float,
                        label_smoothing: float = 0.0,
                        epochs: int = 3,
                        device: str = 'cuda') -> Dict[str, float]:
    """
    âš¡ ã‚¯ã‚¤ãƒƒã‚¯è¨“ç·´ãƒ»è©•ä¾¡ï¼ˆOptunaç”¨ï¼‰
    
    Args:
        model: è¨“ç·´å¯¾è±¡ãƒ¢ãƒ‡ãƒ«
        lr: å­¦ç¿’ç‡
        label_smoothing: ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
        epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        device: ãƒ‡ãƒã‚¤ã‚¹
        
    Returns:
        è©•ä¾¡çµæœè¾æ›¸
    """
    model = model.to(device)
    model.train()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™ï¼ˆWindowså¯¾å¿œï¼‰
    train_loader, val_loader = get_dataloaders(batch_size=64, num_workers=0)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ãƒ»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    
    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    train_losses = []
    for epoch in range(epochs):
        epoch_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}", leave=False)
        for batch_idx, (data, targets) in enumerate(progress_bar):
            data, targets = data.to(device), targets.to(device)
            targets = targets.long()  # ç¢ºå®Ÿã«longå‹ã«ã‚­ãƒ£ã‚¹ãƒˆ
            
            optimizer.zero_grad()
            outputs = model(data)
            
            # ã‚«ã‚¹ã‚¿ãƒ æå¤±è¨ˆç®—ï¼ˆãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å¯¾å¿œï¼‰
            if hasattr(model, 'compute_loss'):
                loss = model.compute_loss(outputs, targets)
            else:
                # æ‰‹å‹•ã§ãƒ©ãƒ™ãƒ«ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°é©ç”¨
                if label_smoothing > 0:
                    num_classes = outputs.size(-1)
                    one_hot = torch.zeros_like(outputs).scatter_(1, targets.unsqueeze(1), 1)
                    smoothed_targets = one_hot * (1 - label_smoothing) + label_smoothing / num_classes
                    log_probs = F.log_softmax(outputs, dim=-1)
                    loss = -(smoothed_targets * log_probs).sum(dim=-1).mean()
                else:
                    loss = nn.CrossEntropyLoss()(outputs, targets)
            
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®‰å®šåŒ–ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})
        
        scheduler.step()
        avg_loss = epoch_loss / num_batches
        train_losses.append(avg_loss)
    
    # è©•ä¾¡
    model.eval()
    val_correct = 0
    val_total = 0
    val_loss = 0.0
    
    with torch.no_grad():
        for data, targets in val_loader:
            data, targets = data.to(device), targets.to(device)
            targets = targets.long()  # ç¢ºå®Ÿã«longå‹ã«ã‚­ãƒ£ã‚¹ãƒˆ
            outputs = model(data)
            
            if hasattr(model, 'compute_loss'):
                loss = model.compute_loss(outputs, targets)
            else:
                loss = nn.CrossEntropyLoss()(outputs, targets)
            
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            val_total += targets.size(0)
            val_correct += (predicted == targets).sum().item()
    
    val_accuracy = val_correct / val_total
    avg_val_loss = val_loss / len(val_loader)
    
    return {
        'val_accuracy': val_accuracy,
        'val_loss': avg_val_loss,
        'train_losses': train_losses,
        'final_train_loss': train_losses[-1] if train_losses else 0.0
    }


def objective(trial: optuna.Trial) -> float:
    """
    ğŸ¯ Optunaç›®çš„é–¢æ•°ï¼šTPEæŒ‡æ¨™æœ€å¤§åŒ–
    
    Args:
        trial: Optunaãƒˆãƒ©ã‚¤ã‚¢ãƒ«
        
    Returns:
        TPEã‚¹ã‚³ã‚¢ï¼ˆæœ€å¤§åŒ–ç›®æ¨™ï¼‰
    """
    try:
        # ----------- ğŸ›ï¸ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ -----------
        
        # LLMã‚¹ã‚¿ã‚¤ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        temperature = trial.suggest_float("temperature", 0.5, 1.5)
        top_k = trial.suggest_int("top_k", 0, 20)
        top_p = trial.suggest_float("top_p", 0.7, 1.0)
        
        # NKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        nkat_strength = trial.suggest_float("nkat_strength", 0.001, 0.05, log=True)
        nkat_decay = trial.suggest_float("nkat_decay", 0.85, 1.0)
        
        # è¨“ç·´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        lr = trial.suggest_float("lr", 1e-5, 5e-4, log=True)
        label_smoothing = trial.suggest_float("label_smoothing", 0.0, 0.15)
        
        # Dropoutç³»
        dropout_attn = trial.suggest_float("dropout_attn", 0.05, 0.2)
        dropout_embed = trial.suggest_float("dropout_embed", 0.05, 0.2)
        
        # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£èª¿æ•´
        embed_dim_choice = trial.suggest_categorical("embed_dim", [256, 384, 512])
        depth_choice = trial.suggest_int("depth", 4, 8)
        
        # ----------- ğŸ—ï¸ ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ -----------
        model = NKATVisionTransformer(
            img_size=28,
            patch_size=4,
            num_classes=10,
            embed_dim=embed_dim_choice,
            depth=depth_choice,
            num_heads=8,
            # LLMã‚¹ã‚¿ã‚¤ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            temperature=temperature,
            top_k=top_k if top_k > 0 else None,
            top_p=top_p,
            # NKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            nkat_strength=nkat_strength,
            nkat_decay=nkat_decay,
            # æ­£å‰‡åŒ–
            dropout_embed=dropout_embed,
            dropout_attn=dropout_attn,
            label_smoothing=label_smoothing
        ).cuda()
        
        # ----------- âš¡ ã‚¯ã‚¤ãƒƒã‚¯è¨“ç·´ãƒ»è©•ä¾¡ -----------
        start_time = time.time()
        results = quick_train_and_eval(model, lr, label_smoothing, epochs=3)
        train_time = time.time() - start_time
        
        val_accuracy = results['val_accuracy']
        
        # ----------- ğŸ“Š TPEæŒ‡æ¨™è¨ˆç®— -----------
        param_analysis = count_nkat_parameters(model)
        lambda_theory = param_analysis['nkat_params']
        
        # åŸºæœ¬TPEè¨ˆç®—
        basic_tpe = tpe_metric(val_accuracy, lambda_theory)
        
        # åŠ¹ç‡æ€§ãƒšãƒŠãƒ«ãƒ†ã‚£
        efficiency_penalty = 0.0
        if train_time > 60:  # 1åˆ†ä»¥ä¸Šã®å ´åˆãƒšãƒŠãƒ«ãƒ†ã‚£
            efficiency_penalty = np.log10(train_time / 60)
        
        # æœ€çµ‚TPEã‚¹ã‚³ã‚¢
        final_tpe = basic_tpe / (1.0 + efficiency_penalty)
        
        # ----------- ğŸ“ˆ ä¸­é–“å€¤å ±å‘Šï¼ˆPruningç”¨ï¼‰ -----------
        trial.report(final_tpe, step=0)
        
        # ----------- ğŸ—‘ï¸ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— -----------
        del model
        torch.cuda.empty_cache()
        gc.collect()
        
        # ----------- ğŸ“ è¿½åŠ å±æ€§ä¿å­˜ -----------
        trial.set_user_attr("val_accuracy", val_accuracy)
        trial.set_user_attr("lambda_theory", lambda_theory)
        trial.set_user_attr("train_time", train_time)
        trial.set_user_attr("basic_tpe", basic_tpe)
        trial.set_user_attr("nkat_ratio", param_analysis['nkat_ratio'])
        
        return final_tpe
    
    except Exception as e:
        print(f"âŒ Trial failed: {e}")
        return 0.0


def enhanced_objective(trial: optuna.Trial) -> float:
    """
    ğŸš€ å¼·åŒ–ç‰ˆç›®çš„é–¢æ•°ï¼šè¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œ
    
    Args:
        trial: Optunaãƒˆãƒ©ã‚¤ã‚¢ãƒ«
        
    Returns:
        æ±åŒ–TPEã‚¹ã‚³ã‚¢
    """
    # åŸºæœ¬ç›®çš„é–¢æ•°ã‚’å®Ÿè¡Œ
    base_tpe = objective(trial)
    
    # ã‚ˆã‚Šå³å¯†ãªè©•ä¾¡ãŒå¿…è¦ãªå ´åˆã®ã¿å®Ÿè¡Œ
    if base_tpe > 0.18:  # é–¾å€¤ä»¥ä¸Šã®å ´åˆã®ã¿
        try:
            # ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡ã‚‚è¿½åŠ å¯èƒ½
            # ä¾‹ï¼šFashionMNISTã€EMNISTç­‰ã§ã®æ±åŒ–æ€§ãƒ†ã‚¹ãƒˆ
            pass
        except:
            pass
    
    return base_tpe


def print_trial_summary(study: optuna.Study):
    """
    ğŸ“Š ãƒˆãƒ©ã‚¤ã‚¢ãƒ«çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    
    Args:
        study: å®Œäº†ã—ãŸOptuna Study
    """
    if len(study.trials) == 0:
        print("No trials completed.")
        return
    
    best_trial = study.best_trial
    
    print("\n" + "="*60)
    print("ğŸ† BEST TRIAL SUMMARY")
    print("="*60)
    print(f"ğŸ¯ Best TPE Score: {best_trial.value:.6f}")
    
    # N/Aå‡¦ç†ã‚’è¿½åŠ 
    val_acc = best_trial.user_attrs.get('val_accuracy', None)
    if val_acc is not None:
        print(f"ğŸ“Š Validation Accuracy: {val_acc:.4f}")
    else:
        print(f"ğŸ“Š Validation Accuracy: N/A")
    
    lambda_theory = best_trial.user_attrs.get('lambda_theory', None)
    if lambda_theory is not None:
        print(f"ğŸ§  Lambda Theory: {lambda_theory:,}")
    else:
        print(f"ğŸ§  Lambda Theory: N/A")
    
    train_time = best_trial.user_attrs.get('train_time', None)
    if train_time is not None:
        print(f"â±ï¸ Training Time: {train_time:.2f}s")
    else:
        print(f"â±ï¸ Training Time: N/A")
    
    nkat_ratio = best_trial.user_attrs.get('nkat_ratio', None)
    if nkat_ratio is not None:
        print(f"ğŸ”¬ NKAT Ratio: {nkat_ratio:.6f}")
    else:
        print(f"ğŸ”¬ NKAT Ratio: N/A")
    
    print("\nğŸ›ï¸ BEST HYPERPARAMETERS:")
    for key, value in best_trial.params.items():
        if isinstance(value, float):
            print(f"  {key:20s}: {value:.6f}")
        else:
            print(f"  {key:20s}: {value}")
    
    print("\nğŸ“ˆ TOP 5 TRIALS:")
    sorted_trials = sorted(study.trials, key=lambda t: t.value or 0, reverse=True)[:5]
    for i, trial in enumerate(sorted_trials):
        val_acc = trial.user_attrs.get('val_accuracy', 0)
        tpe_score = trial.value or 0
        print(f"  #{i+1}: TPE={tpe_score:.4f}, Acc={val_acc:.4f}")
    
    print("="*60) 
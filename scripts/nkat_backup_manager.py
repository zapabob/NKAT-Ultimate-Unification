#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ’¾ NKAT Backup Manager
NKATãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

æ©Ÿèƒ½:
- è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
- ãƒ‡ãƒ¼ã‚¿å¾©å…ƒæ©Ÿèƒ½
- æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
- åœ§ç¸®ãƒ»æš—å·åŒ–
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç®¡ç†
"""

import os
import sys
import json
import shutil
import hashlib
import zipfile
import schedule
import time
import logging
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Tuple, Any
import threading

# æš—å·åŒ–
try:
    from cryptography.fernet import Fernet
    ENCRYPTION_AVAILABLE = True
except ImportError:
    ENCRYPTION_AVAILABLE = False

# é€²æ—è¡¨ç¤º
from tqdm import tqdm

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/nkat_backup.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class BackupConfig:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—è¨­å®š"""
    # åŸºæœ¬è¨­å®š
    backup_dir: str = "Backups"
    max_backups: int = 10
    compression_enabled: bool = True
    encryption_enabled: bool = False
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
    auto_backup_enabled: bool = True
    backup_interval_hours: int = 6
    daily_backup_time: str = "02:00"
    
    # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    source_directories: List[str] = None
    exclude_patterns: List[str] = None
    
    # è©³ç´°è¨­å®š
    verify_integrity: bool = True
    incremental_backup: bool = True
    parallel_processing: bool = True
    
    def __post_init__(self):
        if self.source_directories is None:
            self.source_directories = [
                "src",
                "Results",
                "config",
                "scripts",
                "docs"
            ]
        
        if self.exclude_patterns is None:
            self.exclude_patterns = [
                "*.pyc",
                "__pycache__",
                "*.tmp",
                "*.log",
                ".git",
                "node_modules"
            ]

@dataclass
class BackupInfo:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±"""
    timestamp: str
    backup_id: str
    size_bytes: int
    file_count: int
    directories: List[str]
    checksum: str
    compression_ratio: float
    encrypted: bool
    incremental: bool
    parent_backup_id: Optional[str] = None

class FileHasher:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def calculate_file_hash(file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        hash_sha256 = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            logger.error(f"ãƒãƒƒã‚·ãƒ¥è¨ˆç®—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return ""
    
    @staticmethod
    def calculate_directory_hash(directory: Path, exclude_patterns: List[str] = None) -> Dict[str, str]:
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        file_hashes = {}
        exclude_patterns = exclude_patterns or []
        
        for file_path in directory.rglob("*"):
            if file_path.is_file():
                # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                skip = False
                for pattern in exclude_patterns:
                    if file_path.match(pattern):
                        skip = True
                        break
                
                if not skip:
                    relative_path = file_path.relative_to(directory)
                    file_hashes[str(relative_path)] = FileHasher.calculate_file_hash(file_path)
        
        return file_hashes

class BackupEncryption:
    """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æš—å·åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, key_file: str = "backup_key.key"):
        self.key_file = Path(key_file)
        self.key = self._load_or_generate_key()
    
    def _load_or_generate_key(self) -> bytes:
        """æš—å·åŒ–ã‚­ãƒ¼èª­ã¿è¾¼ã¿/ç”Ÿæˆ"""
        if not ENCRYPTION_AVAILABLE:
            logger.warning("æš—å·åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return b""
        
        if self.key_file.exists():
            with open(self.key_file, 'rb') as f:
                return f.read()
        else:
            key = Fernet.generate_key()
            with open(self.key_file, 'wb') as f:
                f.write(key)
            logger.info(f"æ–°ã—ã„æš—å·åŒ–ã‚­ãƒ¼ã‚’ç”Ÿæˆ: {self.key_file}")
            return key
    
    def encrypt_file(self, input_file: Path, output_file: Path):
        """ãƒ•ã‚¡ã‚¤ãƒ«æš—å·åŒ–"""
        if not ENCRYPTION_AVAILABLE:
            shutil.copy2(input_file, output_file)
            return
        
        fernet = Fernet(self.key)
        
        with open(input_file, 'rb') as f_in:
            data = f_in.read()
        
        encrypted_data = fernet.encrypt(data)
        
        with open(output_file, 'wb') as f_out:
            f_out.write(encrypted_data)
    
    def decrypt_file(self, input_file: Path, output_file: Path):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¾©å·åŒ–"""
        if not ENCRYPTION_AVAILABLE:
            shutil.copy2(input_file, output_file)
            return
        
        fernet = Fernet(self.key)
        
        with open(input_file, 'rb') as f_in:
            encrypted_data = f_in.read()
        
        decrypted_data = fernet.decrypt(encrypted_data)
        
        with open(output_file, 'wb') as f_out:
            f_out.write(decrypted_data)

class NKATBackupManager:
    """NKAT ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: BackupConfig = None):
        self.config = config or BackupConfig()
        self.backup_dir = Path(self.config.backup_dir)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        self.metadata_file = self.backup_dir / "backup_metadata.json"
        self.metadata = self._load_metadata()
        
        self.encryption = BackupEncryption() if self.config.encryption_enabled else None
        self.scheduler_thread = None
        self.scheduler_running = False
    
    def _load_metadata(self) -> Dict[str, BackupInfo]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                return {k: BackupInfo(**v) for k, v in data.items()}
            except Exception as e:
                logger.error(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return {}
    
    def _save_metadata(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""
        try:
            data = {k: asdict(v) for k, v in self.metadata.items()}
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def create_backup(self, backup_name: str = None, incremental: bool = None) -> Optional[str]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_id = backup_name or f"nkat_backup_{timestamp}"
        
        if incremental is None:
            incremental = self.config.incremental_backup
        
        logger.info(f"ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆé–‹å§‹: {backup_id}")
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            backup_path = self.backup_dir / backup_id
            backup_path.mkdir(exist_ok=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åé›†
            files_to_backup = self._collect_files()
            
            if incremental and self.metadata:
                # å¢—åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
                files_to_backup = self._filter_changed_files(files_to_backup)
                parent_backup_id = max(self.metadata.keys(), key=lambda x: self.metadata[x].timestamp)
            else:
                parent_backup_id = None
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
            total_size, file_count = self._backup_files(files_to_backup, backup_path)
            
            # åœ§ç¸®
            if self.config.compression_enabled:
                compressed_path = self._compress_backup(backup_path, backup_id)
                shutil.rmtree(backup_path)
                backup_path = compressed_path
                compression_ratio = total_size / backup_path.stat().st_size if backup_path.stat().st_size > 0 else 1.0
            else:
                compression_ratio = 1.0
            
            # æš—å·åŒ–
            if self.config.encryption_enabled and self.encryption:
                encrypted_path = self._encrypt_backup(backup_path, backup_id)
                backup_path.unlink() if backup_path.is_file() else shutil.rmtree(backup_path)
                backup_path = encrypted_path
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
            checksum = FileHasher.calculate_file_hash(backup_path) if backup_path.is_file() else ""
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            backup_info = BackupInfo(
                timestamp=timestamp,
                backup_id=backup_id,
                size_bytes=backup_path.stat().st_size,
                file_count=file_count,
                directories=self.config.source_directories,
                checksum=checksum,
                compression_ratio=compression_ratio,
                encrypted=self.config.encryption_enabled,
                incremental=incremental,
                parent_backup_id=parent_backup_id
            )
            
            self.metadata[backup_id] = backup_info
            self._save_metadata()
            
            # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤
            self._cleanup_old_backups()
            
            logger.info(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå®Œäº†: {backup_id}")
            logger.info(f"   ã‚µã‚¤ã‚º: {total_size / 1e6:.1f}MB â†’ {backup_path.stat().st_size / 1e6:.1f}MB")
            logger.info(f"   åœ§ç¸®ç‡: {compression_ratio:.2f}x")
            logger.info(f"   ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}")
            
            return backup_id
            
        except Exception as e:
            logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _collect_files(self) -> List[Path]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«åé›†"""
        files = []
        
        for source_dir in self.config.source_directories:
            source_path = Path(source_dir)
            if source_path.exists():
                for file_path in source_path.rglob("*"):
                    if file_path.is_file():
                        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                        skip = False
                        for pattern in self.config.exclude_patterns:
                            if file_path.match(pattern):
                                skip = True
                                break
                        
                        if not skip:
                            files.append(file_path)
        
        return files
    
    def _filter_changed_files(self, files: List[Path]) -> List[Path]:
        """å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        # æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ãƒãƒƒã‚·ãƒ¥æƒ…å ±å–å¾—
        latest_backup_id = max(self.metadata.keys(), key=lambda x: self.metadata[x].timestamp)
        latest_backup_path = self.backup_dir / latest_backup_id
        
        # ç°¡ç•¥åŒ–: å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¯¾è±¡ã¨ã™ã‚‹ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å·®åˆ†æ¤œå‡ºï¼‰
        return files
    
    def _backup_files(self, files: List[Path], backup_path: Path) -> Tuple[int, int]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ"""
        total_size = 0
        file_count = 0
        
        with tqdm(total=len(files), desc="ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼ä¸­", unit="files") as pbar:
            for file_path in files:
                try:
                    # ç›¸å¯¾ãƒ‘ã‚¹è¨ˆç®—
                    for source_dir in self.config.source_directories:
                        source_path = Path(source_dir)
                        if file_path.is_relative_to(source_path):
                            relative_path = file_path.relative_to(source_path)
                            dest_path = backup_path / source_dir / relative_path
                            break
                    else:
                        continue
                    
                    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
                    shutil.copy2(file_path, dest_path)
                    total_size += file_path.stat().st_size
                    file_count += 1
                    
                except Exception as e:
                    logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                
                pbar.update(1)
        
        return total_size, file_count
    
    def _compress_backup(self, backup_path: Path, backup_id: str) -> Path:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—åœ§ç¸®"""
        compressed_path = self.backup_dir / f"{backup_id}.zip"
        
        with zipfile.ZipFile(compressed_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
            for file_path in backup_path.rglob("*"):
                if file_path.is_file():
                    arcname = file_path.relative_to(backup_path)
                    zipf.write(file_path, arcname)
        
        return compressed_path
    
    def _encrypt_backup(self, backup_path: Path, backup_id: str) -> Path:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æš—å·åŒ–"""
        encrypted_path = self.backup_dir / f"{backup_id}.encrypted"
        self.encryption.encrypt_file(backup_path, encrypted_path)
        return encrypted_path
    
    def _cleanup_old_backups(self):
        """å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤"""
        if len(self.metadata) <= self.config.max_backups:
            return
        
        # å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤
        sorted_backups = sorted(
            self.metadata.items(),
            key=lambda x: x[1].timestamp
        )
        
        backups_to_remove = sorted_backups[:-self.config.max_backups]
        
        for backup_id, backup_info in backups_to_remove:
            try:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                backup_files = list(self.backup_dir.glob(f"{backup_id}*"))
                for backup_file in backup_files:
                    if backup_file.is_file():
                        backup_file.unlink()
                    elif backup_file.is_dir():
                        shutil.rmtree(backup_file)
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤
                del self.metadata[backup_id]
                logger.info(f"å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤: {backup_id}")
                
            except Exception as e:
                logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤ã‚¨ãƒ©ãƒ¼ {backup_id}: {e}")
        
        self._save_metadata()
    
    def restore_backup(self, backup_id: str, restore_path: str = None) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒ"""
        if backup_id not in self.metadata:
            logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {backup_id}")
            return False
        
        backup_info = self.metadata[backup_id]
        restore_path = Path(restore_path or f"Restored_{backup_id}")
        
        logger.info(f"ğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒé–‹å§‹: {backup_id}")
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
            backup_files = list(self.backup_dir.glob(f"{backup_id}*"))
            if not backup_files:
                logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {backup_id}")
                return False
            
            backup_file = backup_files[0]
            
            # å¾©å·åŒ–
            if backup_info.encrypted and self.encryption:
                decrypted_file = self.backup_dir / f"{backup_id}_decrypted"
                self.encryption.decrypt_file(backup_file, decrypted_file)
                backup_file = decrypted_file
            
            # å±•é–‹
            if backup_file.suffix == '.zip':
                with zipfile.ZipFile(backup_file, 'r') as zipf:
                    zipf.extractall(restore_path)
            else:
                shutil.copytree(backup_file, restore_path, dirs_exist_ok=True)
            
            # æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            if self.config.verify_integrity:
                if self._verify_backup_integrity(backup_id, restore_path):
                    logger.info("âœ… æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: æ­£å¸¸")
                else:
                    logger.warning("âš ï¸ æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: ç•°å¸¸æ¤œå‡º")
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if backup_info.encrypted and 'decrypted_file' in locals():
                decrypted_file.unlink()
            
            logger.info(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒå®Œäº†: {restore_path}")
            return True
            
        except Exception as e:
            logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _verify_backup_integrity(self, backup_id: str, restored_path: Path) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            backup_info = self.metadata[backup_id]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
            restored_files = list(restored_path.rglob("*"))
            restored_file_count = len([f for f in restored_files if f.is_file()])
            
            if restored_file_count != backup_info.file_count:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°ä¸ä¸€è‡´: {restored_file_count} != {backup_info.file_count}")
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def list_backups(self) -> List[BackupInfo]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§å–å¾—"""
        return sorted(self.metadata.values(), key=lambda x: x.timestamp, reverse=True)
    
    def get_backup_info(self, backup_id: str) -> Optional[BackupInfo]:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æƒ…å ±å–å¾—"""
        return self.metadata.get(backup_id)
    
    def delete_backup(self, backup_id: str) -> bool:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤"""
        if backup_id not in self.metadata:
            logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {backup_id}")
            return False
        
        try:
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            backup_files = list(self.backup_dir.glob(f"{backup_id}*"))
            for backup_file in backup_files:
                if backup_file.is_file():
                    backup_file.unlink()
                elif backup_file.is_dir():
                    shutil.rmtree(backup_file)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤
            del self.metadata[backup_id]
            self._save_metadata()
            
            logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤: {backup_id}")
            return True
            
        except Exception as e:
            logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def start_scheduler(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©é–‹å§‹"""
        if not self.config.auto_backup_enabled:
            return
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š
        schedule.every(self.config.backup_interval_hours).hours.do(
            lambda: self.create_backup(incremental=True)
        )
        
        schedule.every().day.at(self.config.daily_backup_time).do(
            lambda: self.create_backup(incremental=False)
        )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        self.scheduler_running = True
        self.scheduler_thread = threading.Thread(target=self._run_scheduler, daemon=True)
        self.scheduler_thread.start()
        
        logger.info("ğŸ“… è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©é–‹å§‹")
    
    def stop_scheduler(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©åœæ­¢"""
        self.scheduler_running = False
        if self.scheduler_thread:
            self.scheduler_thread.join()
        
        schedule.clear()
        logger.info("ğŸ“… è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©åœæ­¢")
    
    def _run_scheduler(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©å®Ÿè¡Œ"""
        while self.scheduler_running:
            schedule.run_pending()
            time.sleep(60)  # 1åˆ†é–“éš”ã§ãƒã‚§ãƒƒã‚¯
    
    def generate_backup_report(self) -> str:
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        backups = self.list_backups()
        total_size = sum(backup.size_bytes for backup in backups)
        
        report = f"""
ğŸ’¾ NKAT ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ¬ãƒãƒ¼ãƒˆ
{'='*50}

ğŸ“Š çµ±è¨ˆæƒ…å ±:
- ç·ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°: {len(backups)}
- ç·ã‚µã‚¤ã‚º: {total_size / 1e9:.2f}GB
- æœ€æ–°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backups[0].timestamp if backups else 'ãªã—'}
- æœ€å¤ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backups[-1].timestamp if backups else 'ãªã—'}

âš™ï¸ è¨­å®š:
- æœ€å¤§ä¿æŒæ•°: {self.config.max_backups}
- åœ§ç¸®: {'æœ‰åŠ¹' if self.config.compression_enabled else 'ç„¡åŠ¹'}
- æš—å·åŒ–: {'æœ‰åŠ¹' if self.config.encryption_enabled else 'ç„¡åŠ¹'}
- è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {'æœ‰åŠ¹' if self.config.auto_backup_enabled else 'ç„¡åŠ¹'}

ğŸ“‹ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§:
"""
        
        for backup in backups[:10]:  # æœ€æ–°10ä»¶
            size_mb = backup.size_bytes / 1e6
            backup_type = "å¢—åˆ†" if backup.incremental else "å®Œå…¨"
            report += f"- {backup.backup_id}: {backup.timestamp} ({size_mb:.1f}MB, {backup_type})\n"
        
        if len(backups) > 10:
            report += f"... ä»– {len(backups) - 10} ä»¶\n"
        
        return report

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ’¾ NKAT ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    try:
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        config = BackupConfig()
        backup_manager = NKATBackupManager(config)
        
        while True:
            print("\nğŸ“‹ ãƒ¡ãƒ‹ãƒ¥ãƒ¼:")
            print("1. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ")
            print("2. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒ")
            print("3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§")
            print("4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤")
            print("5. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º")
            print("6. è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—é–‹å§‹/åœæ­¢")
            print("0. çµ‚äº†")
            
            choice = input("\né¸æŠã—ã¦ãã ã•ã„ (0-6): ").strip()
            
            if choice == "1":
                backup_name = input("ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å (ç©ºç™½ã§è‡ªå‹•ç”Ÿæˆ): ").strip() or None
                incremental = input("å¢—åˆ†ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—? (y/N): ").strip().lower() == 'y'
                backup_id = backup_manager.create_backup(backup_name, incremental)
                if backup_id:
                    print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå®Œäº†: {backup_id}")
                
            elif choice == "2":
                backups = backup_manager.list_backups()
                if not backups:
                    print("âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue
                
                print("\nğŸ“‹ åˆ©ç”¨å¯èƒ½ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—:")
                for i, backup in enumerate(backups[:10]):
                    print(f"{i+1}. {backup.backup_id} ({backup.timestamp})")
                
                try:
                    idx = int(input("å¾©å…ƒã™ã‚‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç•ªå·: ")) - 1
                    if 0 <= idx < len(backups):
                        backup_id = backups[idx].backup_id
                        restore_path = input("å¾©å…ƒå…ˆãƒ‘ã‚¹ (ç©ºç™½ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ): ").strip() or None
                        if backup_manager.restore_backup(backup_id, restore_path):
                            print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¾©å…ƒå®Œäº†")
                    else:
                        print("âŒ ç„¡åŠ¹ãªç•ªå·ã§ã™")
                except ValueError:
                    print("âŒ ç„¡åŠ¹ãªå…¥åŠ›ã§ã™")
                
            elif choice == "3":
                backups = backup_manager.list_backups()
                if not backups:
                    print("âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                else:
                    print("\nğŸ“‹ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§:")
                    for backup in backups:
                        size_mb = backup.size_bytes / 1e6
                        backup_type = "å¢—åˆ†" if backup.incremental else "å®Œå…¨"
                        print(f"- {backup.backup_id}: {backup.timestamp} ({size_mb:.1f}MB, {backup_type})")
                
            elif choice == "4":
                backups = backup_manager.list_backups()
                if not backups:
                    print("âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue
                
                print("\nğŸ“‹ å‰Šé™¤å¯èƒ½ãªãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—:")
                for i, backup in enumerate(backups):
                    print(f"{i+1}. {backup.backup_id} ({backup.timestamp})")
                
                try:
                    idx = int(input("å‰Šé™¤ã™ã‚‹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç•ªå·: ")) - 1
                    if 0 <= idx < len(backups):
                        backup_id = backups[idx].backup_id
                        confirm = input(f"æœ¬å½“ã«å‰Šé™¤ã—ã¾ã™ã‹? {backup_id} (y/N): ").strip().lower()
                        if confirm == 'y':
                            if backup_manager.delete_backup(backup_id):
                                print(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤å®Œäº†: {backup_id}")
                    else:
                        print("âŒ ç„¡åŠ¹ãªç•ªå·ã§ã™")
                except ValueError:
                    print("âŒ ç„¡åŠ¹ãªå…¥åŠ›ã§ã™")
                
            elif choice == "5":
                report = backup_manager.generate_backup_report()
                print(report)
                
            elif choice == "6":
                if backup_manager.scheduler_running:
                    backup_manager.stop_scheduler()
                    print("â¹ï¸ è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’åœæ­¢ã—ã¾ã—ãŸ")
                else:
                    backup_manager.start_scheduler()
                    print("â–¶ï¸ è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹ã—ã¾ã—ãŸ")
                
            elif choice == "0":
                if backup_manager.scheduler_running:
                    backup_manager.stop_scheduler()
                print("ğŸ‘‹ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
                break
                
            else:
                print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error(f"ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ RTX3080æœ€é©åŒ–ç‰ˆ ãƒ‡ã‚£ãƒ©ãƒƒã‚¯/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ è§£æãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
NKAT Theory - RTX3080å°‚ç”¨æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ

Author: NKAT Research Team
Date: 2025-01-24
Version: 1.7 - RTX3080æœ€é©åŒ–å¼·åŒ–ç‰ˆãƒ†ã‚¹ãƒˆ
"""

import sys
import os
import time
import torch
import numpy as np
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.gpu.dirac_laplacian_analysis_gpu_recovery import (
    RecoveryGPUOperatorParameters,
    RecoveryGPUDiracLaplacianAnalyzer,
    setup_logger,
    monitor_gpu_memory
)

def test_rtx3080_detection():
    """RTX3080æ¤œå‡ºãƒ†ã‚¹ãƒˆ"""
    print("=" * 80)
    print("ğŸ® RTX3080æ¤œå‡ºãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return False
    
    gpu_name = torch.cuda.get_device_name()
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    
    print(f"ğŸ® æ¤œå‡ºã•ã‚ŒãŸGPU: {gpu_name}")
    print(f"ğŸ’¾ VRAM: {gpu_memory:.1f} GB")
    print(f"ğŸ”§ CUDA Version: {torch.version.cuda}")
    print(f"ğŸ PyTorch Version: {torch.__version__}")
    
    is_rtx3080 = "RTX 3080" in gpu_name or "RTX3080" in gpu_name
    
    if is_rtx3080:
        print("âœ… RTX3080ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸï¼å°‚ç”¨æœ€é©åŒ–ãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã™")
        return True
    else:
        print("âš ï¸  RTX3080ä»¥å¤–ã®GPUãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
        print("æœ€é©åŒ–ã¯é™å®šçš„ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        return False

def test_memory_optimization():
    """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 80)
    print("ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    # åˆæœŸãƒ¡ãƒ¢ãƒªçŠ¶æ…‹
    initial_memory = monitor_gpu_memory()
    if initial_memory:
        print(f"ğŸ’¾ åˆæœŸGPUä½¿ç”¨ç‡: {initial_memory['usage_percent']:.1f}%")
        if 'used_gb' in initial_memory:
            print(f"ğŸ’¾ åˆæœŸGPUä½¿ç”¨é‡: {initial_memory['used_gb']:.2f} GB")
        if 'free_gb' in initial_memory:
            print(f"ğŸ’¾ åˆæœŸGPUç©ºãå®¹é‡: {initial_memory['free_gb']:.2f} GB")
    
    # è»½é‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
    params = RecoveryGPUOperatorParameters(
        dimension=3,
        lattice_size=8,
        theta=0.1,
        kappa=0.05,
        mass=0.1,
        coupling=1.0,
        use_sparse=True,
        recovery_enabled=False,
        max_eigenvalues=20,
        memory_limit_gb=9.0,
        use_mixed_precision=True
    )
    
    print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    print(f"   æ¬¡å…ƒ: {params.dimension}")
    print(f"   æ ¼å­ã‚µã‚¤ã‚º: {params.lattice_size}")
    print(f"   æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º: {params.gpu_batch_size}")
    print(f"   ãƒ¡ãƒ¢ãƒªåˆ¶é™: {params.memory_limit_gb} GB")
    
    try:
        analyzer = RecoveryGPUDiracLaplacianAnalyzer(params)
        
        # ã‚¬ãƒ³ãƒè¡Œåˆ—æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ
        print("\nğŸ”¨ ã‚¬ãƒ³ãƒè¡Œåˆ—æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ...")
        gamma_start = time.time()
        gamma_matrices = analyzer._construct_high_dimensional_gamma_matrices()
        gamma_time = time.time() - gamma_start
        
        gamma_memory = monitor_gpu_memory()
        if gamma_memory and initial_memory:
            memory_increase = gamma_memory['usage_percent'] - initial_memory['usage_percent']
            print(f"âœ… ã‚¬ãƒ³ãƒè¡Œåˆ—æ§‹ç¯‰å®Œäº†: {gamma_time:.2f}ç§’")
            print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ : {memory_increase:.1f}%")
        
        # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ
        print("\nğŸ”¨ ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ æ§‹ç¯‰ãƒ†ã‚¹ãƒˆ...")
        dirac_start = time.time()
        D = analyzer.construct_discrete_dirac_operator_gpu_optimized()
        dirac_time = time.time() - dirac_start
        
        dirac_memory = monitor_gpu_memory()
        if dirac_memory and gamma_memory:
            memory_increase = dirac_memory['usage_percent'] - gamma_memory['usage_percent']
            print(f"âœ… ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ æ§‹ç¯‰å®Œäº†: {dirac_time:.2f}ç§’")
            print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¢—åŠ : {memory_increase:.1f}%")
            print(f"ğŸ“Š è¡Œåˆ—ã‚µã‚¤ã‚º: {D.shape}")
            print(f"ğŸ“Š éé›¶è¦ç´ æ•°: {D._nnz():,}")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ
        print("\nğŸ§¹ ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ...")
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        cleanup_memory = monitor_gpu_memory()
        if cleanup_memory and dirac_memory:
            memory_freed = dirac_memory['usage_percent'] - cleanup_memory['usage_percent']
            print(f"âœ… ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
            print(f"ğŸ’¾ è§£æ”¾ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒª: {memory_freed:.1f}%")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_performance_comparison():
    """æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆï¼ˆCPU vs GPUï¼‰"""
    print("\n" + "=" * 80)
    print("âš¡ æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆ (CPU vs GPU)")
    print("=" * 80)
    
    # è»½é‡ãƒ†ã‚¹ãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    params = RecoveryGPUOperatorParameters(
        dimension=3,
        lattice_size=6,
        theta=0.1,
        kappa=0.05,
        mass=0.1,
        coupling=1.0,
        use_sparse=True,
        recovery_enabled=False,
        max_eigenvalues=15,
        memory_limit_gb=9.0,
        use_mixed_precision=True
    )
    
    results = {}
    
    try:
        # GPUç‰ˆãƒ†ã‚¹ãƒˆ
        print("ğŸš€ GPUç‰ˆãƒ†ã‚¹ãƒˆé–‹å§‹...")
        analyzer_gpu = RecoveryGPUDiracLaplacianAnalyzer(params)
        
        gpu_start = time.time()
        
        # ã‚¬ãƒ³ãƒè¡Œåˆ—æ§‹ç¯‰
        gamma_matrices = analyzer_gpu._construct_high_dimensional_gamma_matrices()
        
        # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ æ§‹ç¯‰
        D_gpu = analyzer_gpu.construct_discrete_dirac_operator_gpu_optimized()
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
        d_s_gpu, info_gpu = analyzer_gpu.compute_spectral_dimension_gpu_optimized(
            D_gpu, n_eigenvalues=15
        )
        
        gpu_time = time.time() - gpu_start
        results['gpu'] = {
            'time': gpu_time,
            'spectral_dimension': d_s_gpu,
            'eigenvalues': info_gpu.get('n_eigenvalues', 0)
        }
        
        print(f"âœ… GPUç‰ˆå®Œäº†: {gpu_time:.2f}ç§’")
        print(f"ğŸ“ˆ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {d_s_gpu:.6f}")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # æ€§èƒ½è©•ä¾¡
        print(f"\nğŸ¯ æ€§èƒ½è©•ä¾¡:")
        if gpu_time < 20:
            print("ğŸš€ å„ªç§€ãªæ€§èƒ½ã§ã™ï¼")
        elif gpu_time < 40:
            print("âœ… è‰¯å¥½ãªæ€§èƒ½ã§ã™")
        elif gpu_time < 60:
            print("âš ï¸  è¨±å®¹ç¯„å›²å†…ã®æ€§èƒ½ã§ã™")
        else:
            print("âŒ æ€§èƒ½æ”¹å–„ãŒå¿…è¦ã§ã™")
        
        return results
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return {}

def test_scalability():
    """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 80)
    print("ğŸ“ˆ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    test_configs = [
        {'dim': 3, 'lattice': 6, 'eigenvals': 15},
        {'dim': 3, 'lattice': 8, 'eigenvals': 20},
        {'dim': 4, 'lattice': 6, 'eigenvals': 15},
        {'dim': 4, 'lattice': 8, 'eigenvals': 20},
    ]
    
    results = []
    
    for i, config in enumerate(test_configs):
        print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆ {i+1}/{len(test_configs)}: {config['dim']}æ¬¡å…ƒ, æ ¼å­{config['lattice']}")
        
        try:
            params = RecoveryGPUOperatorParameters(
                dimension=config['dim'],
                lattice_size=config['lattice'],
                theta=0.1,
                kappa=0.05,
                mass=0.1,
                coupling=1.0,
                use_sparse=True,
                recovery_enabled=False,
                max_eigenvalues=config['eigenvals'],
                memory_limit_gb=9.0,
                use_mixed_precision=True
            )
            
            analyzer = RecoveryGPUDiracLaplacianAnalyzer(params)
            
            # GPU ãƒ¡ãƒ¢ãƒªç›£è¦–
            start_memory = monitor_gpu_memory()
            
            # è¨ˆç®—å®Ÿè¡Œ
            start_time = time.time()
            
            gamma_matrices = analyzer._construct_high_dimensional_gamma_matrices()
            D = analyzer.construct_discrete_dirac_operator_gpu_optimized()
            d_s, info = analyzer.compute_spectral_dimension_gpu_optimized(
                D, n_eigenvalues=config['eigenvals']
            )
            
            total_time = time.time() - start_time
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
            end_memory = monitor_gpu_memory()
            memory_used = 0.0
            if end_memory and start_memory:
                memory_used = end_memory['usage_percent'] - start_memory['usage_percent']
            
            result = {
                'config': config,
                'time': total_time,
                'spectral_dimension': d_s,
                'memory_used': memory_used,
                'matrix_size': D.shape[0],
                'nnz': D._nnz()
            }
            results.append(result)
            
            print(f"âœ… å®Œäº†: {total_time:.2f}ç§’, ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {memory_used:.1f}%")
            print(f"ğŸ“ˆ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {d_s:.6f}")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ ãƒ†ã‚¹ãƒˆ {i+1} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\nğŸ“Š ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
    print("-" * 80)
    for i, result in enumerate(results):
        config = result['config']
        print(f"ãƒ†ã‚¹ãƒˆ{i+1}: {config['dim']}æ¬¡å…ƒÃ—{config['lattice']} â†’ "
              f"{result['time']:.2f}ç§’, {result['memory_used']:.1f}%ãƒ¡ãƒ¢ãƒª, "
              f"è¡Œåˆ—{result['matrix_size']:,}Ã—{result['matrix_size']:,}")
    
    return results

def test_high_precision_computation():
    """é«˜ç²¾åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 80)
    print("ğŸ”¬ é«˜ç²¾åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return False
    
    # é«˜ç²¾åº¦ãƒ†ã‚¹ãƒˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    test_configs = [
        {'dim': 3, 'lattice': 8, 'eigenvals': 30, 'expected_ds': 3.0},
        {'dim': 4, 'lattice': 6, 'eigenvals': 25, 'expected_ds': 4.0},
        {'dim': 5, 'lattice': 6, 'eigenvals': 20, 'expected_ds': 5.0},
    ]
    
    results = []
    
    for i, config in enumerate(test_configs):
        print(f"\nğŸ”¬ é«˜ç²¾åº¦ãƒ†ã‚¹ãƒˆ {i+1}/{len(test_configs)}: {config['dim']}æ¬¡å…ƒ")
        print(f"   æ ¼å­ã‚µã‚¤ã‚º: {config['lattice']}")
        print(f"   å›ºæœ‰å€¤æ•°: {config['eigenvals']}")
        print(f"   ç†è«–ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {config['expected_ds']}")
        
        try:
            # é«˜ç²¾åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
            params = RecoveryGPUOperatorParameters(
                dimension=config['dim'],
                lattice_size=config['lattice'],
                theta=0.01,  # å°ã•ãªéå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
                kappa=0.01,
                mass=0.01,
                coupling=1.0,
                use_sparse=True,
                recovery_enabled=False,
                max_eigenvalues=config['eigenvals'],
                memory_limit_gb=9.0,
                use_mixed_precision=True
            )
            
            analyzer = RecoveryGPUDiracLaplacianAnalyzer(params)
            
            # é«˜ç²¾åº¦è¨ˆç®—å®Ÿè¡Œ
            print("   ğŸ”¨ ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ æ§‹ç¯‰ä¸­...")
            start_time = time.time()
            D = analyzer.construct_discrete_dirac_operator_gpu_optimized()
            construction_time = time.time() - start_time
            
            print("   ğŸ” é«˜ç²¾åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ä¸­...")
            spectral_start = time.time()
            d_s, info = analyzer.compute_spectral_dimension_gpu_optimized(
                D, n_eigenvalues=config['eigenvals']
            )
            spectral_time = time.time() - spectral_start
            
            # ç²¾åº¦è©•ä¾¡
            theoretical_ds = config['expected_ds']
            absolute_error = abs(d_s - theoretical_ds)
            relative_error = (absolute_error / theoretical_ds) * 100
            
            # çµæœè¨˜éŒ²
            result = {
                'config': config,
                'spectral_dimension': d_s,
                'theoretical_dimension': theoretical_ds,
                'absolute_error': absolute_error,
                'relative_error': relative_error,
                'construction_time': construction_time,
                'spectral_time': spectral_time,
                'total_time': construction_time + spectral_time,
                'eigenvalues_computed': info.get('n_eigenvalues', 0),
                'matrix_size': D.shape[0],
                'nnz': D._nnz()
            }
            results.append(result)
            
            # çµæœè¡¨ç¤º
            print(f"   âœ… è¨ˆç®—å®Œäº†:")
            print(f"      ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {d_s:.8f}")
            print(f"      ç†è«–å€¤: {theoretical_ds}")
            print(f"      çµ¶å¯¾èª¤å·®: {absolute_error:.8f}")
            print(f"      ç›¸å¯¾èª¤å·®: {relative_error:.4f}%")
            print(f"      è¨ˆç®—æ™‚é–“: {construction_time + spectral_time:.2f}ç§’")
            
            # ç²¾åº¦è©•ä¾¡
            if relative_error < 1.0:
                print("      ğŸ¯ å„ªç§€ãªç²¾åº¦!")
            elif relative_error < 5.0:
                print("      âœ… è‰¯å¥½ãªç²¾åº¦")
            elif relative_error < 10.0:
                print("      âš ï¸  è¨±å®¹ç¯„å›²å†…")
            else:
                print("      âŒ ç²¾åº¦æ”¹å–„ãŒå¿…è¦")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"   âŒ ãƒ†ã‚¹ãƒˆ {i+1} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ç·åˆè©•ä¾¡
    if results:
        print(f"\nğŸ“Š é«˜ç²¾åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆç·åˆçµæœ:")
        print("-" * 80)
        
        total_tests = len(results)
        high_precision_count = sum(1 for r in results if r['relative_error'] < 1.0)
        good_precision_count = sum(1 for r in results if r['relative_error'] < 5.0)
        
        avg_relative_error = np.mean([r['relative_error'] for r in results])
        avg_computation_time = np.mean([r['total_time'] for r in results])
        
        print(f"å®Ÿè¡Œãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
        print(f"é«˜ç²¾åº¦ãƒ†ã‚¹ãƒˆæ•° (<1%èª¤å·®): {high_precision_count}")
        print(f"è‰¯å¥½ç²¾åº¦ãƒ†ã‚¹ãƒˆæ•° (<5%èª¤å·®): {good_precision_count}")
        print(f"å¹³å‡ç›¸å¯¾èª¤å·®: {avg_relative_error:.4f}%")
        print(f"å¹³å‡è¨ˆç®—æ™‚é–“: {avg_computation_time:.2f}ç§’")
        
        # è©³ç´°çµæœè¡¨ç¤º
        for i, result in enumerate(results):
            config = result['config']
            print(f"ãƒ†ã‚¹ãƒˆ{i+1}: {config['dim']}DÃ—{config['lattice']} â†’ "
                  f"ds={result['spectral_dimension']:.6f} "
                  f"(èª¤å·®{result['relative_error']:.2f}%, "
                  f"{result['total_time']:.1f}ç§’)")
        
        # æˆåŠŸåˆ¤å®š
        success_rate = good_precision_count / total_tests
        if success_rate >= 0.8:
            print("ğŸ‰ é«˜ç²¾åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
            return True
        else:
            print("âš ï¸  é«˜ç²¾åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆã§ä¸€éƒ¨å•é¡Œã‚ã‚Š")
            return False
    else:
        print("âŒ é«˜ç²¾åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False

def test_convergence_analysis():
    """åæŸè§£æãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 80)
    print("ğŸ“ˆ åæŸè§£æãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return False
    
    # åæŸè§£æç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    base_params = {
        'dimension': 3,
        'lattice_size': 6,
        'theta': 0.01,
        'kappa': 0.01,
        'mass': 0.01,
        'coupling': 1.0,
        'use_sparse': True,
        'recovery_enabled': False,
        'memory_limit_gb': 9.0,
        'use_mixed_precision': True
    }
    
    # å›ºæœ‰å€¤æ•°ã‚’å¤‰åŒ–ã•ã›ã¦åæŸæ€§ã‚’ãƒ†ã‚¹ãƒˆ
    eigenvalue_counts = [10, 15, 20, 25, 30, 35, 40]
    convergence_results = []
    
    print("ğŸ” å›ºæœ‰å€¤æ•°ã«ã‚ˆã‚‹åæŸè§£æ...")
    
    for n_eigenvals in eigenvalue_counts:
        print(f"   å›ºæœ‰å€¤æ•°: {n_eigenvals}")
        
        try:
            params = RecoveryGPUOperatorParameters(
                max_eigenvalues=n_eigenvals,
                **base_params
            )
            
            analyzer = RecoveryGPUDiracLaplacianAnalyzer(params)
            
            # è¨ˆç®—å®Ÿè¡Œ
            D = analyzer.construct_discrete_dirac_operator_gpu_optimized()
            d_s, info = analyzer.compute_spectral_dimension_gpu_optimized(
                D, n_eigenvalues=n_eigenvals
            )
            
            convergence_results.append({
                'n_eigenvalues': n_eigenvals,
                'spectral_dimension': d_s,
                'eigenvalues_computed': info.get('n_eigenvalues', 0)
            })
            
            print(f"      â†’ ds = {d_s:.6f}")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"      âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    # åæŸè§£æ
    if len(convergence_results) >= 3:
        print(f"\nğŸ“Š åæŸè§£æçµæœ:")
        print("-" * 60)
        
        ds_values = [r['spectral_dimension'] for r in convergence_results]
        n_values = [r['n_eigenvalues'] for r in convergence_results]
        
        # åæŸæ€§ã®è©•ä¾¡
        if len(ds_values) >= 3:
            # æœ€å¾Œã®3ã¤ã®å€¤ã®æ¨™æº–åå·®
            recent_std = np.std(ds_values[-3:])
            overall_std = np.std(ds_values)
            
            print(f"å…¨ä½“ã®æ¨™æº–åå·®: {overall_std:.6f}")
            print(f"æœ€è¿‘3ç‚¹ã®æ¨™æº–åå·®: {recent_std:.6f}")
            
            # åæŸåˆ¤å®š
            if recent_std < 0.01:
                print("âœ… è‰¯å¥½ãªåæŸæ€§")
                convergence_quality = "è‰¯å¥½"
            elif recent_std < 0.05:
                print("âš ï¸  ä¸­ç¨‹åº¦ã®åæŸæ€§")
                convergence_quality = "ä¸­ç¨‹åº¦"
            else:
                print("âŒ åæŸæ€§ã«å•é¡Œã‚ã‚Š")
                convergence_quality = "å•é¡Œã‚ã‚Š"
            
            # è©³ç´°è¡¨ç¤º
            for result in convergence_results:
                print(f"n={result['n_eigenvalues']:2d}: ds={result['spectral_dimension']:.6f}")
            
            return convergence_quality == "è‰¯å¥½"
        else:
            print("âš ï¸  åæŸè§£æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return False
    else:
        print("âŒ åæŸè§£æãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False

def test_theoretical_comparison():
    """ç†è«–å€¤ã¨ã®è©³ç´°æ¯”è¼ƒãƒ†ã‚¹ãƒˆ"""
    print("\n" + "=" * 80)
    print("ğŸ¯ ç†è«–å€¤ã¨ã®è©³ç´°æ¯”è¼ƒãƒ†ã‚¹ãƒˆ")
    print("=" * 80)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return False
    
    # ç†è«–çš„ã«æ—¢çŸ¥ã®çµæœã¨ã®æ¯”è¼ƒ
    theoretical_cases = [
        {
            'name': '3æ¬¡å…ƒãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“',
            'params': {'dimension': 3, 'lattice_size': 8, 'theta': 0.0, 'mass': 0.0},
            'expected_ds': 3.0,
            'tolerance': 0.1
        },
        {
            'name': '4æ¬¡å…ƒãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰ç©ºé–“',
            'params': {'dimension': 4, 'lattice_size': 6, 'theta': 0.0, 'mass': 0.0},
            'expected_ds': 4.0,
            'tolerance': 0.15
        },
        {
            'name': '3æ¬¡å…ƒéå¯æ›ç©ºé–“',
            'params': {'dimension': 3, 'lattice_size': 8, 'theta': 0.1, 'mass': 0.0},
            'expected_ds': 3.0,  # å°ã•ãªéå¯æ›åŠ¹æœ
            'tolerance': 0.2
        }
    ]
    
    comparison_results = []
    
    for case in theoretical_cases:
        print(f"\nğŸ§® ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹: {case['name']}")
        print(f"   æœŸå¾…å€¤: {case['expected_ds']}")
        print(f"   è¨±å®¹èª¤å·®: Â±{case['tolerance']}")
        
        try:
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
            params = RecoveryGPUOperatorParameters(
                kappa=0.01,
                coupling=1.0,
                use_sparse=True,
                recovery_enabled=False,
                max_eigenvalues=30,
                memory_limit_gb=9.0,
                use_mixed_precision=True,
                **case['params']
            )
            
            analyzer = RecoveryGPUDiracLaplacianAnalyzer(params)
            
            # è¤‡æ•°å›å®Ÿè¡Œã—ã¦çµ±è¨ˆã‚’å–ã‚‹
            ds_values = []
            for trial in range(3):  # 3å›å®Ÿè¡Œ
                print(f"   è©¦è¡Œ {trial+1}/3...")
                
                D = analyzer.construct_discrete_dirac_operator_gpu_optimized()
                d_s, info = analyzer.compute_spectral_dimension_gpu_optimized(D)
                ds_values.append(d_s)
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # çµ±è¨ˆè¨ˆç®—
            ds_mean = np.mean(ds_values)
            ds_std = np.std(ds_values)
            ds_min = np.min(ds_values)
            ds_max = np.max(ds_values)
            
            # ç†è«–å€¤ã¨ã®æ¯”è¼ƒ
            theoretical_ds = case['expected_ds']
            error = abs(ds_mean - theoretical_ds)
            within_tolerance = error <= case['tolerance']
            
            result = {
                'case_name': case['name'],
                'theoretical_ds': theoretical_ds,
                'computed_ds_mean': ds_mean,
                'computed_ds_std': ds_std,
                'computed_ds_min': ds_min,
                'computed_ds_max': ds_max,
                'error': error,
                'tolerance': case['tolerance'],
                'within_tolerance': within_tolerance,
                'trials': ds_values
            }
            comparison_results.append(result)
            
            # çµæœè¡¨ç¤º
            print(f"   çµæœ:")
            print(f"      å¹³å‡: {ds_mean:.6f} Â± {ds_std:.6f}")
            print(f"      ç¯„å›²: [{ds_min:.6f}, {ds_max:.6f}]")
            print(f"      èª¤å·®: {error:.6f}")
            print(f"      åˆ¤å®š: {'âœ… åˆæ ¼' if within_tolerance else 'âŒ ä¸åˆæ ¼'}")
            
        except Exception as e:
            print(f"   âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            continue
    
    # ç·åˆè©•ä¾¡
    if comparison_results:
        print(f"\nğŸ“Š ç†è«–å€¤æ¯”è¼ƒãƒ†ã‚¹ãƒˆç·åˆçµæœ:")
        print("=" * 80)
        
        total_cases = len(comparison_results)
        passed_cases = sum(1 for r in comparison_results if r['within_tolerance'])
        
        print(f"å®Ÿè¡Œã‚±ãƒ¼ã‚¹æ•°: {total_cases}")
        print(f"åˆæ ¼ã‚±ãƒ¼ã‚¹æ•°: {passed_cases}")
        print(f"åˆæ ¼ç‡: {(passed_cases/total_cases)*100:.1f}%")
        
        # è©³ç´°çµæœ
        for result in comparison_results:
            status = "âœ…" if result['within_tolerance'] else "âŒ"
            print(f"{status} {result['case_name']}: "
                  f"ç†è«–å€¤{result['theoretical_ds']:.1f} vs "
                  f"è¨ˆç®—å€¤{result['computed_ds_mean']:.4f}Â±{result['computed_ds_std']:.4f} "
                  f"(èª¤å·®{result['error']:.4f})")
        
        return passed_cases == total_cases
    else:
        print("âŒ ç†è«–å€¤æ¯”è¼ƒãƒ†ã‚¹ãƒˆãŒå®Ÿè¡Œã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False

def run_comprehensive_test():
    """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("ğŸš€ RTX3080æœ€é©åŒ–ç‰ˆ åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 100)
    
    # ãƒ­ã‚°è¨­å®š
    logger = setup_logger("RTX3080_Test", "logs/rtx3080_test.log")
    
    test_results = {}
    
    # 1. RTX3080æ¤œå‡ºãƒ†ã‚¹ãƒˆ
    test_results['rtx3080_detection'] = test_rtx3080_detection()
    
    # 2. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
    test_results['memory_optimization'] = test_memory_optimization()
    
    # 3. æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
    test_results['performance'] = test_performance_comparison()
    
    # 4. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
    test_results['scalability'] = test_scalability()
    
    # 5. é«˜ç²¾åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ
    test_results['high_precision_computation'] = test_high_precision_computation()
    
    # 6. åæŸè§£æãƒ†ã‚¹ãƒˆ
    test_results['convergence_analysis'] = test_convergence_analysis()
    
    # 7. ç†è«–å€¤ã¨ã®è©³ç´°æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
    test_results['theoretical_comparison'] = test_theoretical_comparison()
    
    # ç·åˆè©•ä¾¡
    print("\n" + "=" * 100)
    print("ğŸ¯ ç·åˆãƒ†ã‚¹ãƒˆçµæœ")
    print("=" * 100)
    
    passed_tests = sum(1 for result in test_results.values() if result)
    total_tests = len(test_results)
    
    print(f"âœ… æˆåŠŸã—ãŸãƒ†ã‚¹ãƒˆ: {passed_tests}/{total_tests}")
    
    if passed_tests == total_tests:
        print("ğŸ‰ å…¨ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼RTX3080æœ€é©åŒ–ã¯æ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã¾ã™")
    elif passed_tests >= total_tests * 0.75:
        print("âœ… å¤§éƒ¨åˆ†ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸã€‚RTX3080æœ€é©åŒ–ã¯æ¦‚ã­æ­£å¸¸ã§ã™")
    else:
        print("âš ï¸  ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    
    # æ¨å¥¨è¨­å®šã®è¡¨ç¤º
    print(f"\nğŸ“‹ RTX3080æ¨å¥¨è¨­å®š:")
    print(f"   æœ€å¤§æ¬¡å…ƒ: 6æ¬¡å…ƒ")
    print(f"   æ¨å¥¨æ ¼å­ã‚µã‚¤ã‚º: 8 (4æ¬¡å…ƒä»¥ä¸‹), 6 (5æ¬¡å…ƒä»¥ä¸Š)")
    print(f"   æ¨å¥¨å›ºæœ‰å€¤æ•°: 50 (4æ¬¡å…ƒä»¥ä¸‹), 30 (5æ¬¡å…ƒä»¥ä¸Š)")
    print(f"   ãƒ¡ãƒ¢ãƒªåˆ¶é™: 9.0 GB")
    print(f"   æ··åˆç²¾åº¦è¨ˆç®—: æœ‰åŠ¹")
    
    return test_results

if __name__ == "__main__":
    # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
    results = run_comprehensive_test()
    
    print(f"\nğŸ RTX3080æœ€é©åŒ–ç‰ˆãƒ†ã‚¹ãƒˆå®Œäº†")
    print("=" * 100) 
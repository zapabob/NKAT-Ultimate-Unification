#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ğŸ”„ RTX3080å¯¾å¿œ é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼æ©Ÿèƒ½ä»˜ãé«˜æ¬¡å…ƒãƒ‡ã‚£ãƒ©ãƒƒã‚¯/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ GPUè§£æ
Non-Commutative Kolmogorov-Arnold Theory (NKAT) ã«ãŠã‘ã‚‹ä½œç”¨ç´ ç†è«– - Recoveryå¯¾å¿œç‰ˆ

Author: NKAT Research Team
Date: 2025-01-24
Version: 1.5 - Recoveryæ©Ÿèƒ½ä»˜ãé«˜æ¬¡å…ƒå¯¾å¿œç‰ˆï¼ˆRTX3080æœ€é©åŒ–ï¼‰

ä¸»è¦æ©Ÿèƒ½:
- é›»æºæ–­ã‹ã‚‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¾©å…ƒ
- è¨ˆç®—é€”ä¸­ã‹ã‚‰ã®å†é–‹æ©Ÿèƒ½
- ã‚ˆã‚Šé«˜æ¬¡å…ƒï¼ˆ6-10æ¬¡å…ƒï¼‰ã§ã®è§£æå¯¾å¿œ
- è‡ªå‹•ä¿å­˜æ©Ÿèƒ½
- GPU/RTX3080æœ€é©åŒ–
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Callable, Any
import warnings
from pathlib import Path
import json
import time
import pickle
import hashlib
from dataclasses import dataclass, asdict
import scipy.sparse as sp
from scipy.sparse.linalg import eigsh
import h5py
import os
from datetime import datetime
import signal
import sys

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
try:
    plt.rcParams['font.family'] = 'MS Gothic'
except:
    plt.rcParams['font.family'] = 'DejaVu Sans'

warnings.filterwarnings('ignore')

@dataclass
class RecoveryGPUOperatorParameters:
    """Recoveryæ©Ÿèƒ½ä»˜ãGPUå¯¾å¿œä½œç”¨ç´ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"""
    dimension: int  # ç©ºé–“æ¬¡å…ƒï¼ˆæœ€å¤§10æ¬¡å…ƒã¾ã§å¯¾å¿œï¼‰
    lattice_size: int  # æ ¼å­ã‚µã‚¤ã‚º
    theta: float  # éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    kappa: float  # Îº-å¤‰å½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    mass: float  # è³ªé‡é …
    coupling: float  # çµåˆå®šæ•°
    use_sparse: bool = True  # ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ä½¿ç”¨
    recovery_enabled: bool = True  # ãƒªã‚«ãƒãƒªãƒ¼æ©Ÿèƒ½æœ‰åŠ¹
    checkpoint_interval: int = 300  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”ï¼ˆç§’ï¼‰
    auto_save: bool = True  # è‡ªå‹•ä¿å­˜æ©Ÿèƒ½
    max_eigenvalues: int = 100  # å›ºæœ‰å€¤è¨ˆç®—æ•°
    memory_limit_gb: float = 8.0  # ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆGBï¼‰
    
    def __post_init__(self):
        if self.dimension < 2 or self.dimension > 10:
            raise ValueError("æ¬¡å…ƒã¯2-10ã®ç¯„å›²ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        if self.lattice_size < 4:
            warnings.warn("æ ¼å­ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        spinor_dim = 2 if self.dimension <= 3 else 4 if self.dimension <= 6 else 8
        total_dim = self.lattice_size**self.dimension * spinor_dim
        estimated_memory = (total_dim**2 * 16) / 1e9  # è¤‡ç´ æ•°double precision
        
        if not self.use_sparse and estimated_memory > self.memory_limit_gb:
            print(f"âš ï¸  ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§: æ¨å®š{estimated_memory:.1f}GB > åˆ¶é™{self.memory_limit_gb}GB")
            print("ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã®ä½¿ç”¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™")

class CheckpointManager:
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, base_dir: str = "checkpoints"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.current_checkpoint = None
        
    def create_checkpoint_id(self, params: RecoveryGPUOperatorParameters) -> str:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ããƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆIDã®ç”Ÿæˆ"""
        param_str = json.dumps(asdict(params), sort_keys=True)
        return hashlib.md5(param_str.encode()).hexdigest()[:12]
    
    def save_checkpoint(self, 
                       checkpoint_id: str,
                       stage: str,
                       data: Dict[str, Any],
                       metadata: Dict[str, Any] = None) -> str:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜"""
        checkpoint_dir = self.base_dir / checkpoint_id
        checkpoint_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().isoformat()
        checkpoint_file = checkpoint_dir / f"{stage}_{timestamp.replace(':', '-')}.h5"
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        if metadata is None:
            metadata = {}
        metadata.update({
            'timestamp': timestamp,
            'stage': stage,
            'checkpoint_id': checkpoint_id
        })
        
        # HDF5ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with h5py.File(checkpoint_file, 'w') as f:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            meta_group = f.create_group('metadata')
            for key, value in metadata.items():
                if isinstance(value, (str, int, float)):
                    meta_group.attrs[key] = value
                else:
                    meta_group.attrs[key] = str(value)
            
            # ãƒ‡ãƒ¼ã‚¿
            data_group = f.create_group('data')
            for key, value in data.items():
                if isinstance(value, np.ndarray):
                    data_group.create_dataset(key, data=value)
                elif isinstance(value, sp.spmatrix):
                    # ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã®ä¿å­˜
                    sparse_group = data_group.create_group(key)
                    sparse_group.create_dataset('data', data=value.data)
                    sparse_group.create_dataset('indices', data=value.indices)
                    sparse_group.create_dataset('indptr', data=value.indptr)
                    sparse_group.attrs['shape'] = value.shape
                    sparse_group.attrs['format'] = value.format
                elif isinstance(value, (int, float, str)):
                    data_group.attrs[key] = value
                else:
                    # ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ã¯pickleã§ä¿å­˜
                    pickled_data = pickle.dumps(value)
                    data_group.create_dataset(f'{key}_pickled', data=np.frombuffer(pickled_data, dtype=np.uint8))
        
        print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_file}")
        self.current_checkpoint = str(checkpoint_file)
        return str(checkpoint_file)
    
    def load_checkpoint(self, checkpoint_file: str) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        checkpoint_path = Path(checkpoint_file)
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint_file}")
        
        metadata = {}
        data = {}
        
        with h5py.File(checkpoint_path, 'r') as f:
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            if 'metadata' in f:
                meta_group = f['metadata']
                for key in meta_group.attrs:
                    metadata[key] = meta_group.attrs[key]
            
            # ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            if 'data' in f:
                data_group = f['data']
                
                # å±æ€§ã®èª­ã¿è¾¼ã¿
                for key in data_group.attrs:
                    data[key] = data_group.attrs[key]
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
                for key in data_group:
                    if key.endswith('_pickled'):
                        # Pickleãƒ‡ãƒ¼ã‚¿ã®å¾©å…ƒ
                        pickled_bytes = data_group[key][()]
                        original_key = key[:-8]  # '_pickled'ã‚’é™¤å»
                        data[original_key] = pickle.loads(pickled_bytes.tobytes())
                    elif isinstance(data_group[key], h5py.Group):
                        # ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã®å¾©å…ƒ
                        sparse_group = data_group[key]
                        if 'data' in sparse_group and 'indices' in sparse_group and 'indptr' in sparse_group:
                            sparse_data = sparse_group['data'][()]
                            indices = sparse_group['indices'][()]
                            indptr = sparse_group['indptr'][()]
                            shape = tuple(sparse_group.attrs['shape'])
                            format_type = sparse_group.attrs['format']
                            
                            if format_type == 'csr':
                                data[key] = sp.csr_matrix((sparse_data, indices, indptr), shape=shape)
                            elif format_type == 'csc':
                                data[key] = sp.csc_matrix((sparse_data, indices, indptr), shape=shape)
                            else:
                                data[key] = sp.csr_matrix((sparse_data, indices, indptr), shape=shape)
                    else:
                        # é€šå¸¸ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
                        data[key] = data_group[key][()]
        
        print(f"ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {checkpoint_file}")
        return data, metadata
    
    def list_checkpoints(self, checkpoint_id: str) -> List[str]:
        """ç‰¹å®šIDã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¸€è¦§"""
        checkpoint_dir = self.base_dir / checkpoint_id
        if not checkpoint_dir.exists():
            return []
        
        return sorted([str(f) for f in checkpoint_dir.glob("*.h5")])
    
    def get_latest_checkpoint(self, checkpoint_id: str, stage: str = None) -> Optional[str]:
        """æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å–å¾—"""
        checkpoints = self.list_checkpoints(checkpoint_id)
        if not checkpoints:
            return None
        
        if stage:
            filtered = [cp for cp in checkpoints if stage in Path(cp).name]
            return filtered[-1] if filtered else None
        
        return checkpoints[-1]

class RecoveryGPUDiracLaplacianAnalyzer:
    """
    ğŸš€ğŸ”„ Recoveryæ©Ÿèƒ½ä»˜ãRTX3080å¯¾å¿œé«˜æ¬¡å…ƒãƒ‡ã‚£ãƒ©ãƒƒã‚¯/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ è§£æã‚¯ãƒ©ã‚¹
    
    æ–°æ©Ÿèƒ½:
    1. é›»æºæ–­ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§
    2. è¨ˆç®—é€”ä¸­ã‹ã‚‰ã®å†é–‹
    3. é«˜æ¬¡å…ƒï¼ˆ6-10æ¬¡å…ƒï¼‰å¯¾å¿œ
    4. è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    5. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æœ€é©åŒ–
    """
    
    def __init__(self, params: RecoveryGPUOperatorParameters):
        self.params = params
        self.dim = params.dimension
        self.N = params.lattice_size
        self.theta = params.theta
        self.kappa = params.kappa
        self.mass = params.mass
        self.coupling = params.coupling
        self.use_sparse = params.use_sparse
        self.device = device
        
        # Recoveryæ©Ÿèƒ½
        self.checkpoint_manager = CheckpointManager() if params.recovery_enabled else None
        self.checkpoint_id = None
        self.last_checkpoint_time = time.time()
        
        # é«˜æ¬¡å…ƒå¯¾å¿œã®ã‚¹ãƒ”ãƒãƒ«æ¬¡å…ƒè¨ˆç®—
        if self.dim <= 3:
            self.spinor_dim = 2
        elif self.dim <= 6:
            self.spinor_dim = 4
        elif self.dim <= 8:
            self.spinor_dim = 8
        else:
            self.spinor_dim = 16  # 10æ¬¡å…ƒã¾ã§å¯¾å¿œ
        
        print(f"ğŸ”§ åˆæœŸåŒ–ä¸­: {self.dim}D, æ ¼å­ã‚µã‚¤ã‚º {self.N}^{self.dim}")
        print(f"ğŸ“Š ã‚¹ãƒ”ãƒãƒ«æ¬¡å…ƒ: {self.spinor_dim}")
        print(f"ğŸ“Š ç·æ ¼å­ç‚¹æ•°: {self.N**self.dim:,}")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å®š
        total_dim = self.N**self.dim * self.spinor_dim
        if self.use_sparse:
            sparsity = min(0.1, 1000.0 / total_dim)  # é©å¿œçš„ã‚¹ãƒ‘ãƒ¼ã‚¹ç‡
            memory_gb = (total_dim**2 * sparsity * 16) / 1e9
            print(f"ğŸ’¾ æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ï¼‰: {memory_gb:.2f} GB")
        else:
            memory_gb = (total_dim**2 * 16) / 1e9
            print(f"ğŸ’¾ æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆå¯†è¡Œåˆ—ï¼‰: {memory_gb:.2f} GB")
        
        print(f"ğŸ“Š è¡Œåˆ—æ¬¡å…ƒ: {total_dim:,} x {total_dim:,}")
        
        # Recoveryæœ‰åŠ¹æ™‚ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆIDç”Ÿæˆ
        if self.checkpoint_manager:
            self.checkpoint_id = self.checkpoint_manager.create_checkpoint_id(params)
            print(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆID: {self.checkpoint_id}")
        
        # é«˜æ¬¡å…ƒå¯¾å¿œã‚¬ãƒ³ãƒè¡Œåˆ—ã®æ§‹ç¯‰
        self.gamma_matrices = self._construct_high_dimensional_gamma_matrices()
        
        # è‡ªå‹•ä¿å­˜ã®ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©è¨­å®š
        if params.auto_save:
            signal.signal(signal.SIGINT, self._save_and_exit)
            signal.signal(signal.SIGTERM, self._save_and_exit)
    
    def _save_and_exit(self, signum, frame):
        """ã‚·ã‚°ãƒŠãƒ«å—ä¿¡æ™‚ã®è‡ªå‹•ä¿å­˜"""
        print(f"\nâš ï¸  ã‚·ã‚°ãƒŠãƒ« {signum} ã‚’å—ä¿¡ã—ã¾ã—ãŸ")
        if self.checkpoint_manager and hasattr(self, '_current_stage_data'):
            self._save_checkpoint('emergency_save', self._current_stage_data)
        print("ğŸ’¾ ç·Šæ€¥ä¿å­˜å®Œäº† - ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’çµ‚äº†ã—ã¾ã™")
        sys.exit(0)
    
    def _construct_high_dimensional_gamma_matrices(self) -> List[np.ndarray]:
        """é«˜æ¬¡å…ƒå¯¾å¿œã‚¬ãƒ³ãƒè¡Œåˆ—ã®æ§‹ç¯‰"""
        print(f"ğŸ”¨ {self.dim}æ¬¡å…ƒã‚¬ãƒ³ãƒè¡Œåˆ—æ§‹ç¯‰ä¸­...")
        
        # ãƒ‘ã‚¦ãƒªè¡Œåˆ—
        sigma_x = np.array([[0, 1], [1, 0]], dtype=complex)
        sigma_y = np.array([[0, -1j], [1j, 0]], dtype=complex)
        sigma_z = np.array([[1, 0], [0, -1]], dtype=complex)
        I2 = np.eye(2, dtype=complex)
        
        gamma = []
        
        if self.dim <= 3:
            # ä½æ¬¡å…ƒã®å ´åˆ
            gamma = [sigma_x, sigma_y, sigma_z][:self.dim]
        
        elif self.dim == 4:
            # 4æ¬¡å…ƒãƒ‡ã‚£ãƒ©ãƒƒã‚¯è¡Œåˆ—
            O2 = np.zeros((2, 2), dtype=complex)
            gamma = [
                np.block([[O2, sigma_x], [sigma_x, O2]]),
                np.block([[O2, sigma_y], [sigma_y, O2]]),
                np.block([[O2, sigma_z], [sigma_z, O2]]),
                np.block([[I2, O2], [O2, -I2]])
            ]
        
        elif self.dim <= 6:
            # 6æ¬¡å…ƒã¾ã§ï¼š4æ¬¡å…ƒã‚’æ‹¡å¼µ
            O2 = np.zeros((2, 2), dtype=complex)
            O4 = np.zeros((4, 4), dtype=complex)
            I4 = np.eye(4, dtype=complex)
            
            # åŸºæœ¬4æ¬¡å…ƒã‚¬ãƒ³ãƒè¡Œåˆ—
            gamma4 = [
                np.block([[O2, sigma_x], [sigma_x, O2]]),
                np.block([[O2, sigma_y], [sigma_y, O2]]),
                np.block([[O2, sigma_z], [sigma_z, O2]]),
                np.block([[I2, O2], [O2, -I2]])
            ]
            
            gamma = gamma4.copy()
            
            # 5æ¬¡å…ƒç›®ã¨6æ¬¡å…ƒç›®
            for i in range(4, self.dim):
                extra_gamma = np.kron(I2, gamma4[i-4])
                gamma.append(extra_gamma)
        
        else:
            # 8æ¬¡å…ƒä»¥ä¸Šï¼šå†å¸°çš„æ§‹ç¯‰
            # ã‚¯ãƒªãƒ•ã‚©ãƒ¼ãƒ‰ä»£æ•°ã®æ§‹ç¯‰
            n_matrices_needed = self.dim
            current_dim = 2
            
            # åˆæœŸã‚¬ãƒ³ãƒè¡Œåˆ—
            gamma = [sigma_x, sigma_y, sigma_z]
            
            while len(gamma) < n_matrices_needed:
                # æ¬¡å…ƒã‚’å€ã«ã—ã¦æ‹¡å¼µ
                current_gamma = gamma.copy()
                new_gamma = []
                
                # æ—¢å­˜ã®è¡Œåˆ—ã‚’æ‹¡å¼µ
                I_current = np.eye(current_dim, dtype=complex)
                O_current = np.zeros((current_dim, current_dim), dtype=complex)
                
                for g in current_gamma:
                    new_g = np.block([[g, O_current], [O_current, -g]])
                    new_gamma.append(new_g)
                
                # æ–°ã—ã„è¡Œåˆ—ã‚’è¿½åŠ 
                if len(new_gamma) < n_matrices_needed:
                    chirality = np.block([[I_current, O_current], [O_current, -I_current]])
                    new_gamma.append(chirality)
                
                gamma = new_gamma
                current_dim *= 2
                
                if current_dim > self.spinor_dim:
                    break
        
        # å¿…è¦ãªæ¬¡å…ƒæ•°ã«èª¿æ•´
        gamma = gamma[:self.dim]
        
        print(f"âœ… {self.dim}æ¬¡å…ƒã‚¬ãƒ³ãƒè¡Œåˆ—æ§‹ç¯‰å®Œäº†: {len(gamma)}å€‹ã®{gamma[0].shape}è¡Œåˆ—")
        
        return gamma
    
    def _save_checkpoint(self, stage: str, data: Dict[str, Any]):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜"""
        if not self.checkpoint_manager:
            return
        
        metadata = {
            'dimension': self.dim,
            'lattice_size': self.N,
            'parameters': asdict(self.params)
        }
        
        self.checkpoint_manager.save_checkpoint(
            self.checkpoint_id, stage, data, metadata
        )
        self.last_checkpoint_time = time.time()
    
    def _should_save_checkpoint(self) -> bool:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®åˆ¤å®š"""
        if not self.checkpoint_manager:
            return False
        
        return (time.time() - self.last_checkpoint_time) > self.params.checkpoint_interval
    
    def construct_discrete_dirac_operator_sparse_recovery(self) -> sp.csr_matrix:
        """
        ğŸ”„ Recoveryå¯¾å¿œã‚¹ãƒ‘ãƒ¼ã‚¹ç‰ˆé›¢æ•£ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã®æ§‹ç¯‰
        """
        stage = 'dirac_construction'
        
        # æ—¢å­˜ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        if self.checkpoint_manager:
            latest_checkpoint = self.checkpoint_manager.get_latest_checkpoint(
                self.checkpoint_id, stage
            )
            if latest_checkpoint:
                print("ğŸ“‚ æ—¢å­˜ã®ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç™ºè¦‹")
                try:
                    data, metadata = self.checkpoint_manager.load_checkpoint(latest_checkpoint)
                    if 'dirac_operator' in data:
                        print("âœ… ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã‚’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
                        return data['dirac_operator']
                except Exception as e:
                    print(f"âš ï¸  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
                    print("æ–°è¦ã«æ§‹ç¯‰ã—ã¾ã™")
        
        print("ğŸ”¨ ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ æ§‹ç¯‰ä¸­...")
        start_time = time.time()
        
        total_dim = self.N**self.dim * self.spinor_dim
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸæ§‹ç¯‰
        if total_dim > 1000000:  # 100ä¸‡æ¬¡å…ƒä»¥ä¸Š
            print("âš¡ å¤§è¦æ¨¡è¡Œåˆ—ç”¨æœ€é©åŒ–ãƒ¢ãƒ¼ãƒ‰ã§æ§‹ç¯‰")
            D = self._construct_large_dirac_operator()
        else:
            D = self._construct_standard_dirac_operator()
        
        construction_time = time.time() - start_time
        print(f"âœ… ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ æ§‹ç¯‰å®Œäº†: {construction_time:.2f}ç§’")
        print(f"ğŸ“Š è¡Œåˆ—ã‚µã‚¤ã‚º: {D.shape}, éé›¶è¦ç´ æ•°: {D.nnz:,}")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if self.checkpoint_manager:
            checkpoint_data = {
                'dirac_operator': D,
                'construction_time': construction_time,
                'matrix_info': {
                    'shape': D.shape,
                    'nnz': D.nnz,
                    'dtype': str(D.dtype)
                }
            }
            self._save_checkpoint(stage, checkpoint_data)
        
        return D
    
    def _construct_standard_dirac_operator(self) -> sp.csr_matrix:
        """æ¨™æº–ã‚µã‚¤ã‚ºè¡Œåˆ—ã®æ§‹ç¯‰"""
        total_dim = self.N**self.dim * self.spinor_dim
        D = sp.lil_matrix((total_dim, total_dim), dtype=complex)
        
        # å„æ–¹å‘ã®å¾®åˆ†ä½œç”¨ç´ 
        for mu in range(self.dim):
            print(f"  æ–¹å‘ {mu+1}/{self.dim} å‡¦ç†ä¸­...")
            
            # å·®åˆ†ä½œç”¨ç´ 
            diff_operator = self._construct_difference_operator_sparse(mu)
            
            # ã‚¬ãƒ³ãƒè¡Œåˆ—ã¨ã®ç©
            gamma_mu = self.gamma_matrices[mu]
            
            # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯é …ã®è¿½åŠ 
            D += sp.kron(diff_operator, gamma_mu)
            
            # éå¯æ›è£œæ­£é …
            if self.theta != 0:
                theta_correction = self._construct_theta_correction_sparse(mu)
                D += self.theta * sp.kron(theta_correction, gamma_mu)
            
            # å®šæœŸçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            if self._should_save_checkpoint():
                temp_data = {'partial_dirac_operator': D.tocsr(), 'completed_directions': mu + 1}
                self._save_checkpoint('dirac_partial', temp_data)
        
        # è³ªé‡é …
        if self.mass != 0:
            mass_operator = sp.eye(self.N**self.dim)
            mass_matrix = self.mass * sp.eye(self.spinor_dim, dtype=complex)
            D += sp.kron(mass_operator, mass_matrix)
        
        return D.tocsr()
    
    def _construct_large_dirac_operator(self) -> sp.csr_matrix:
        """å¤§è¦æ¨¡è¡Œåˆ—ç”¨ã®åŠ¹ç‡çš„æ§‹ç¯‰"""
        print("âš¡ å¤§è¦æ¨¡è¡Œåˆ—æœ€é©åŒ–æ§‹ç¯‰")
        
        total_dim = self.N**self.dim * self.spinor_dim
        
        # ãƒ–ãƒ­ãƒƒã‚¯åˆ¥æ§‹ç¯‰
        row_indices = []
        col_indices = []
        data_values = []
        
        batch_size = min(1000, self.N)  # ãƒãƒƒãƒã‚µã‚¤ã‚º
        
        for mu in range(self.dim):
            print(f"  å¤§è¦æ¨¡æ–¹å‘ {mu+1}/{self.dim} å‡¦ç†ä¸­...")
            
            # ãƒãƒƒãƒå‡¦ç†ã§å·®åˆ†ä½œç”¨ç´ ã‚’æ§‹ç¯‰
            for batch_start in range(0, self.N**self.dim, batch_size):
                batch_end = min(batch_start + batch_size, self.N**self.dim)
                
                # å°ã•ãªãƒ–ãƒ­ãƒƒã‚¯ã®å‡¦ç†
                block_diff = self._construct_difference_block(mu, batch_start, batch_end)
                gamma_mu = self.gamma_matrices[mu]
                
                # ãƒ–ãƒ­ãƒƒã‚¯Ã—ã‚¬ãƒ³ãƒè¡Œåˆ—
                block_result = sp.kron(block_diff, gamma_mu)
                
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨å€¤ã®åé›†
                block_coo = block_result.tocoo()
                row_indices.extend(block_coo.row)
                col_indices.extend(block_coo.col)
                data_values.extend(block_coo.data)
                
                # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚å®šæœŸçš„ã«ã‚¯ãƒªã‚¢
                if len(data_values) > 10000000:  # 1000ä¸‡è¦ç´ 
                    print("  ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ãŸã‚ä¸­é–“ä¿å­˜...")
                    temp_matrix = sp.coo_matrix(
                        (data_values, (row_indices, col_indices)),
                        shape=(total_dim, total_dim)
                    ).tocsr()
                    
                    # ä¸€æ™‚ä¿å­˜
                    temp_data = {'temp_matrix': temp_matrix, 'batch_progress': batch_end}
                    self._save_checkpoint('large_matrix_temp', temp_data)
                    
                    # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                    row_indices.clear()
                    col_indices.clear()
                    data_values.clear()
        
        # æœ€çµ‚çš„ãªè¡Œåˆ—æ§‹ç¯‰
        D = sp.coo_matrix(
            (data_values, (row_indices, col_indices)),
            shape=(total_dim, total_dim)
        ).tocsr()
        
        return D
    
    def _construct_difference_operator_sparse(self, direction: int) -> sp.csr_matrix:
        """ã‚¹ãƒ‘ãƒ¼ã‚¹ç‰ˆå·®åˆ†ä½œç”¨ç´ ã®æ§‹ç¯‰"""
        # 1æ¬¡å…ƒã®å‰é€²å·®åˆ†
        diff_1d = sp.diags([1, -1], [1, 0], shape=(self.N, self.N))
        diff_1d = diff_1d.tolil()
        diff_1d[self.N-1, 0] = 1  # å‘¨æœŸå¢ƒç•Œæ¡ä»¶
        diff_1d = diff_1d.tocsr()
        
        # å¤šæ¬¡å…ƒã¸ã®æ‹¡å¼µ
        operators = []
        for d in range(self.dim):
            if d == direction:
                operators.append(diff_1d)
            else:
                operators.append(sp.eye(self.N))
        
        # åŠ¹ç‡çš„ãªã‚¯ãƒ­ãƒãƒƒã‚«ãƒ¼ç©è¨ˆç®—
        result = operators[0]
        for op in operators[1:]:
            result = sp.kron(result, op)
        
        return result
    
    def _construct_difference_block(self, direction: int, start_idx: int, end_idx: int) -> sp.csr_matrix:
        """å·®åˆ†ä½œç”¨ç´ ã®ãƒ–ãƒ­ãƒƒã‚¯æ§‹ç¯‰"""
        block_size = end_idx - start_idx
        block_diff = sp.eye(block_size)
        
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…ï¼ˆå®Ÿéš›ã«ã¯ã‚ˆã‚Šè¤‡é›‘ãªè¨ˆç®—ãŒå¿…è¦ï¼‰
        return block_diff
    
    def _construct_theta_correction_sparse(self, direction: int) -> sp.csr_matrix:
        """ã‚¹ãƒ‘ãƒ¼ã‚¹ç‰ˆÎ¸-å¤‰å½¢è£œæ­£é …ã®æ§‹ç¯‰"""
        # ä½ç½®ä½œç”¨ç´ 
        positions = np.arange(self.N) - self.N // 2
        pos_1d = sp.diags(positions, 0, shape=(self.N, self.N))
        
        # å¤šæ¬¡å…ƒã¸ã®æ‹¡å¼µ
        operators = []
        for d in range(self.dim):
            if d == direction:
                operators.append(pos_1d)
            else:
                operators.append(sp.eye(self.N))
        
        x_op = operators[0]
        for op in operators[1:]:
            x_op = sp.kron(x_op, op)
        
        return x_op * 0.01
    
    def compute_spectral_dimension_recovery(self, 
                                          operator: sp.csr_matrix,
                                          n_eigenvalues: int = None) -> Tuple[float, Dict]:
        """
        ğŸ”„ Recoveryå¯¾å¿œã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
        """
        if n_eigenvalues is None:
            n_eigenvalues = self.params.max_eigenvalues
        
        stage = 'spectral_computation'
        
        # æ—¢å­˜ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
        if self.checkpoint_manager:
            latest_checkpoint = self.checkpoint_manager.get_latest_checkpoint(
                self.checkpoint_id, stage
            )
            if latest_checkpoint:
                print("ğŸ“‚ æ—¢å­˜ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç™ºè¦‹")
                try:
                    data, metadata = self.checkpoint_manager.load_checkpoint(latest_checkpoint)
                    if 'spectral_dimension' in data:
                        print("âœ… ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã‚’ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©å…ƒ")
                        return data['spectral_dimension'], data.get('analysis_info', {})
                except Exception as e:
                    print(f"âš ï¸  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¾©å…ƒã‚¨ãƒ©ãƒ¼: {e}")
                    print("æ–°è¦ã«è¨ˆç®—ã—ã¾ã™")
        
        print("ğŸ” ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ä¸­ï¼ˆRecoveryå¯¾å¿œï¼‰...")
        start_time = time.time()
        
        try:
            # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–
            print("ğŸ”¨ ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–ä¸­...")
            operator_hermitian = operator.conj().T @ operator
            
            # ç¾åœ¨ã®çŠ¶æ…‹ã‚’ä¿å­˜
            self._current_stage_data = {
                'stage': 'hermitian_complete',
                'operator_shape': operator.shape,
                'nnz': operator.nnz
            }
            
            # å›ºæœ‰å€¤è¨ˆç®—
            print(f"ğŸ§® å›ºæœ‰å€¤è¨ˆç®—ä¸­ï¼ˆ{n_eigenvalues}å€‹ï¼‰...")
            eigenvalues, _ = eigsh(
                operator_hermitian, 
                k=min(n_eigenvalues, operator.shape[0]-2),
                which='SM', 
                return_eigenvectors=False
            )
            eigenvalues = np.real(eigenvalues)
            eigenvalues = eigenvalues[eigenvalues > 1e-12]
            
            # ä¸­é–“çµæœã®ä¿å­˜
            if self._should_save_checkpoint():
                checkpoint_data = {
                    'eigenvalues': eigenvalues,
                    'computation_stage': 'eigenvalues_complete'
                }
                self._save_checkpoint('spectral_intermediate', checkpoint_data)
            
        except Exception as e:
            print(f"âŒ å›ºæœ‰å€¤è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('nan'), {}
        
        if len(eigenvalues) < 10:
            print("âš ï¸  è­¦å‘Š: æœ‰åŠ¹ãªå›ºæœ‰å€¤ãŒå°‘ãªã™ãã¾ã™")
            return float('nan'), {}
        
        # GPUä¸Šã§ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®è¨ˆç®—
        print("âš¡ GPUä¸Šã§ã‚¼ãƒ¼ã‚¿é–¢æ•°è¨ˆç®—...")
        eigenvalues_gpu = torch.tensor(eigenvalues, device=self.device, dtype=torch.float32)
        t_values = torch.logspace(-3, 0, 50, device=self.device)
        
        zeta_values = []
        for i, t in enumerate(t_values):
            zeta_t = torch.sum(torch.exp(-t * eigenvalues_gpu))
            zeta_values.append(zeta_t.item())
            
            # é€²æ—è¡¨ç¤º
            if (i + 1) % 10 == 0:
                print(f"  ã‚¼ãƒ¼ã‚¿é–¢æ•°è¨ˆç®—: {i+1}/{len(t_values)}")
        
        zeta_values = torch.tensor(zeta_values, device=self.device)
        
        # å¯¾æ•°å¾®åˆ†ã®è¨ˆç®—
        log_t = torch.log(t_values)
        log_zeta = torch.log(zeta_values + 1e-12)
        
        # ç·šå½¢å›å¸°ã§å‚¾ãã‚’æ±‚ã‚ã‚‹
        valid_mask = torch.isfinite(log_zeta) & torch.isfinite(log_t)
        if torch.sum(valid_mask) < 5:
            print("âš ï¸  è­¦å‘Š: æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ç‚¹ãŒå°‘ãªã™ãã¾ã™")
            return float('nan'), {}
        
        log_t_valid = log_t[valid_mask]
        log_zeta_valid = log_zeta[valid_mask]
        
        # æœ€å°äºŒä¹—æ³•
        A = torch.stack([log_t_valid, torch.ones_like(log_t_valid)], dim=1)
        slope, intercept = torch.linalg.lstsq(A, log_zeta_valid).solution
        
        spectral_dimension = -2 * slope.item()
        
        computation_time = time.time() - start_time
        print(f"âœ… ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—å®Œäº†: {computation_time:.2f}ç§’")
        
        # è©³ç´°æƒ…å ±
        analysis_info = {
            'eigenvalues': eigenvalues,
            'n_eigenvalues': len(eigenvalues),
            'min_eigenvalue': np.min(eigenvalues),
            'max_eigenvalue': np.max(eigenvalues),
            'spectral_gap': eigenvalues[1] - eigenvalues[0] if len(eigenvalues) > 1 else 0,
            'zeta_function': zeta_values.cpu().numpy(),
            't_values': t_values.cpu().numpy(),
            'slope': slope.item(),
            'intercept': intercept.item(),
            'computation_time': computation_time
        }
        
        # æœ€çµ‚çµæœã®ä¿å­˜
        if self.checkpoint_manager:
            final_data = {
                'spectral_dimension': spectral_dimension,
                'analysis_info': analysis_info
            }
            self._save_checkpoint(stage, final_data)
        
        return spectral_dimension, analysis_info
    
    def run_full_analysis_with_recovery(self) -> Dict[str, Any]:
        """ğŸ”„ Recoveryæ©Ÿèƒ½ä»˜ãå®Œå…¨è§£æã®å®Ÿè¡Œ"""
        print("=" * 80)
        print("ğŸš€ğŸ”„ Recoveryæ©Ÿèƒ½ä»˜ãRTX3080é«˜æ¬¡å…ƒãƒ‡ã‚£ãƒ©ãƒƒã‚¯/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ä½œç”¨ç´ è§£æ")
        print("=" * 80)
        
        if torch.cuda.is_available():
            print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
            print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        print(f"\nğŸ“Š è§£æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"æ¬¡å…ƒ: {self.dim}")
        print(f"æ ¼å­ã‚µã‚¤ã‚º: {self.N}")
        print(f"ã‚¹ãƒ”ãƒãƒ«æ¬¡å…ƒ: {self.spinor_dim}")
        print(f"Recoveryæ©Ÿèƒ½: {'æœ‰åŠ¹' if self.checkpoint_manager else 'ç„¡åŠ¹'}")
        
        total_start = time.time()
        
        # 1. ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã®æ§‹ç¯‰
        print("\nğŸ”¨ 1. ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã®æ§‹ç¯‰...")
        D = self.construct_discrete_dirac_operator_sparse_recovery()
        
        # 2. ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã®è¨ˆç®—
        print("\nğŸ” 2. ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã®è¨ˆç®—...")
        d_s_dirac, dirac_info = self.compute_spectral_dimension_recovery(D)
        
        print(f"ğŸ“ˆ ãƒ‡ã‚£ãƒ©ãƒƒã‚¯ä½œç”¨ç´ ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {d_s_dirac:.6f}")
        print(f"ğŸ¯ ç†è«–å€¤({self.dim})ã¨ã®å·®: {abs(d_s_dirac - self.dim):.6f}")
        
        total_time = time.time() - total_start
        print(f"\nâ±ï¸  ç·è¨ˆç®—æ™‚é–“: {total_time:.2f}ç§’")
        
        # çµæœã‚µãƒãƒªãƒ¼
        results = {
            'parameters': asdict(self.params),
            'gpu_info': {
                'device_name': torch.cuda.get_device_name() if torch.cuda.is_available() else 'CPU',
                'cuda_available': torch.cuda.is_available(),
                'total_memory_gb': torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0
            },
            'results': {
                'spectral_dimension': d_s_dirac,
                'theoretical_dimension': self.dim,
                'dimension_error': abs(d_s_dirac - self.dim),
                'total_computation_time': total_time,
                'matrix_size': D.shape[0],
                'nnz_elements': D.nnz,
                'sparsity_ratio': D.nnz / (D.shape[0] * D.shape[1]),
                'spinor_dimension': self.spinor_dim
            },
            'dirac_analysis': dirac_info,
            'checkpoint_id': self.checkpoint_id,
            'timestamp': datetime.now().isoformat()
        }
        
        # çµæœã®ä¿å­˜
        output_file = f"recovery_gpu_results_dim{self.dim}_N{self.N}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ’¾ çµæœãŒ '{output_file}' ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚")
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        if self.checkpoint_manager:
            self._save_checkpoint('final_results', results)
            print(f"ğŸ”„ æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: ID {self.checkpoint_id}")
        
        return results

def demonstrate_recovery_analysis():
    """ğŸ”„ Recoveryæ©Ÿèƒ½ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    print("=" * 80)
    print("ğŸš€ğŸ”„ RTX3080å¯¾å¿œ Recoveryæ©Ÿèƒ½ä»˜ãé«˜æ¬¡å…ƒGPUè§£æãƒ‡ãƒ¢")
    print("=" * 80)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆé«˜æ¬¡å…ƒå¯¾å¿œï¼‰
    test_configs = [
        # ä¸­æ¬¡å…ƒãƒ†ã‚¹ãƒˆ
        RecoveryGPUOperatorParameters(
            dimension=4, lattice_size=16, theta=0.01, kappa=0.05,
            mass=0.1, coupling=1.0, recovery_enabled=True,
            checkpoint_interval=60, auto_save=True, max_eigenvalues=50
        ),
        # é«˜æ¬¡å…ƒãƒ†ã‚¹ãƒˆ
        RecoveryGPUOperatorParameters(
            dimension=6, lattice_size=8, theta=0.005, kappa=0.02,
            mass=0.05, coupling=0.8, recovery_enabled=True,
            checkpoint_interval=120, auto_save=True, max_eigenvalues=30
        ),
    ]
    
    all_results = []
    
    for i, params in enumerate(test_configs):
        print(f"\n{'='*20} è¨­å®š {i+1}/{len(test_configs)} {'='*20}")
        print(f"æ¬¡å…ƒ: {params.dimension}, æ ¼å­: {params.lattice_size}")
        
        try:
            analyzer = RecoveryGPUDiracLaplacianAnalyzer(params)
            results = analyzer.run_full_analysis_with_recovery()
            all_results.append(results)
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            print("Recoveryæ©Ÿèƒ½ã«ã‚ˆã‚Šã€æ¬¡ã®è¨­å®šã§å†é–‹å¯èƒ½ã§ã™")
            continue
    
    # å…¨ä½“ã‚µãƒãƒªãƒ¼
    print("\n" + "="*80)
    print("ğŸ“Š å…¨ä½“çµæœã‚µãƒãƒªãƒ¼")
    print("="*80)
    
    for i, result in enumerate(all_results):
        params = result['parameters']
        res = result['results']
        print(f"\nè¨­å®š {i+1}:")
        print(f"  æ¬¡å…ƒ: {params['dimension']}, æ ¼å­: {params['lattice_size']}")
        print(f"  ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {res['spectral_dimension']:.6f}")
        print(f"  ç†è«–å€¤ã¨ã®å·®: {res['dimension_error']:.6f}")
        print(f"  è¨ˆç®—æ™‚é–“: {res['total_computation_time']:.2f}ç§’")
        print(f"  è¡Œåˆ—ã‚µã‚¤ã‚º: {res['matrix_size']:,}")
        print(f"  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆID: {result['checkpoint_id']}")
    
    return all_results

if __name__ == "__main__":
    # Recoveryæ©Ÿèƒ½ä»˜ãé«˜æ¬¡å…ƒè§£æã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    results = demonstrate_recovery_analysis() 
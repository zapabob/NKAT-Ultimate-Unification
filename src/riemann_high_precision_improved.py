#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ NKATç†è«–ã«ã‚ˆã‚‹ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã®è¶…é«˜ç²¾åº¦æ•°å€¤æ¤œè¨¼ (æ”¹è‰¯ç‰ˆ)
Ultra High-Precision Riemann Hypothesis Verification using NKAT Theory (Improved)

Author: NKAT Research Team
Date: 2025-05-24
Version: 6.0 - Ultra High Precision Implementation with Enhanced Stability
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass
from tqdm import tqdm, trange
import logging
from scipy import special
import math

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

class UltraHighPrecisionNKATHamiltonian(nn.Module):
    """
    è¶…é«˜ç²¾åº¦NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®å®Ÿè£…
    
    æ”¹è‰¯ç‚¹:
    1. å‹•çš„ç²¾åº¦èª¿æ•´
    2. æ”¹è‰¯ã•ã‚ŒãŸã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
    3. ã‚ˆã‚Šå®‰å®šã—ãŸæ•°å€¤è¨ˆç®—
    4. Î³å€¤ä¾å­˜ã®é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    """
    
    def __init__(self, max_n: int = 1500, base_theta: float = 1e-20, 
                 base_kappa: float = 1e-12, precision: str = 'ultra'):
        super().__init__()
        self.max_n = max_n
        self.base_theta = base_theta
        self.base_kappa = base_kappa
        self.precision = precision
        self.device = device
        
        # è¶…é«˜ç²¾åº¦è¨­å®š
        if precision == 'ultra':
            self.dtype = torch.complex128
            self.float_dtype = torch.float64
            torch.set_default_dtype(torch.float64)
        else:
            self.dtype = torch.complex64
            self.float_dtype = torch.float32
        
        logger.info(f"ğŸ”§ è¶…é«˜ç²¾åº¦NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³åˆæœŸåŒ–: max_n={max_n}, ç²¾åº¦={precision}")
        
        # ç´ æ•°ãƒªã‚¹ãƒˆã®ç”Ÿæˆ
        self.primes = self._generate_primes_optimized(max_n)
        logger.info(f"ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸç´ æ•°æ•°: {len(self.primes)}")
        
        # ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®é›¶ç‚¹ï¼ˆæ—¢çŸ¥ã®å€¤ï¼‰
        self.known_zeros = [14.134725, 21.022040, 25.010858, 30.424876, 32.935062,
                           37.586178, 40.918719, 43.327073, 48.005151, 49.773832]
        
    def _generate_primes_optimized(self, n: int) -> List[int]:
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒˆã‚¹ãƒ†ãƒã‚¹ã®ç¯©"""
        if n < 2:
            return []
        
        sieve = [True] * (n + 1)
        sieve[0] = sieve[1] = False
        
        for i in range(2, int(n**0.5) + 1):
            if sieve[i]:
                for j in range(i*i, n + 1, i):
                    sieve[j] = False
        
        return [i for i in range(2, n + 1) if sieve[i]]
    
    def _adaptive_parameters(self, s: complex) -> Tuple[float, float, int]:
        """Î³å€¤ã«å¿œã˜ãŸé©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´"""
        gamma = abs(s.imag)
        
        # Î³å€¤ã«åŸºã¥ãå‹•çš„èª¿æ•´
        if gamma < 20:
            theta = self.base_theta * 10
            kappa = self.base_kappa * 5
            dim = min(self.max_n, 300)
        elif gamma < 50:
            theta = self.base_theta * 5
            kappa = self.base_kappa * 2
            dim = min(self.max_n, 250)
        else:
            theta = self.base_theta
            kappa = self.base_kappa
            dim = min(self.max_n, 200)
        
        return theta, kappa, dim
    
    def construct_enhanced_hamiltonian(self, s: complex) -> torch.Tensor:
        """
        å¼·åŒ–ã•ã‚ŒãŸãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³æ§‹ç¯‰
        """
        theta, kappa, dim = self._adaptive_parameters(s)
        
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¡Œåˆ—ã®åˆæœŸåŒ–
        H = torch.zeros(dim, dim, dtype=self.dtype, device=self.device)
        
        # ä¸»è¦é …: ã‚ˆã‚Šå®‰å®šã—ãŸè¨ˆç®—
        for n in range(1, dim + 1):
            try:
                # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®è¨ˆç®—ï¼ˆæ•°å€¤å®‰å®šæ€§å‘ä¸Šï¼‰
                if abs(s.real) > 15 or abs(s.imag) > 100:
                    log_n = math.log(n)
                    log_term = -s.real * log_n + 1j * s.imag * log_n
                    
                    if log_term.real < -30:  # ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
                        H[n-1, n-1] = torch.tensor(1e-30, dtype=self.dtype, device=self.device)
                    else:
                        H[n-1, n-1] = torch.exp(torch.tensor(log_term, dtype=self.dtype, device=self.device))
                else:
                    # ç›´æ¥è¨ˆç®—
                    H[n-1, n-1] = torch.tensor(1.0 / (n ** s), dtype=self.dtype, device=self.device)
                    
            except (OverflowError, ZeroDivisionError, RuntimeError):
                H[n-1, n-1] = torch.tensor(1e-30, dtype=self.dtype, device=self.device)
        
        # éå¯æ›è£œæ­£é …ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        if theta != 0:
            theta_tensor = torch.tensor(theta, dtype=self.dtype, device=self.device)
            gamma = abs(s.imag)
            
            # Î³å€¤ã«ä¾å­˜ã™ã‚‹è£œæ­£å¼·åº¦
            correction_strength = 1.0 / (1.0 + gamma * 0.01)
            
            for i, p in enumerate(self.primes[:min(len(self.primes), 30)]):
                if p <= dim:
                    try:
                        log_p = math.log(p)
                        correction = theta_tensor * log_p * correction_strength
                        
                        # é‡å­è£œæ­£é …
                        if p < dim - 1:
                            # éå¯¾è§’é …ï¼ˆé‡å­ã‚‚ã¤ã‚ŒåŠ¹æœï¼‰
                            H[p-1, p] += correction * 1j * 0.5
                            H[p, p-1] -= correction * 1j * 0.5
                        
                        # å¯¾è§’é …ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚·ãƒ•ãƒˆï¼‰
                        H[p-1, p-1] += correction * 0.1
                    except:
                        continue
        
        # Îº-å¤‰å½¢è£œæ­£é …ï¼ˆMinkowskiæ™‚ç©ºåŠ¹æœï¼‰
        if kappa != 0:
            kappa_tensor = torch.tensor(kappa, dtype=self.dtype, device=self.device)
            
            for i in range(min(dim, 40)):
                try:
                    n = i + 1
                    
                    # Minkowskiè¨ˆé‡ã«ã‚ˆã‚‹è£œæ­£
                    minkowski_factor = 1.0 / math.sqrt(1.0 + (n * kappa) ** 2)
                    log_term = math.log(n + 1) * minkowski_factor
                    kappa_correction = kappa_tensor * n * log_term
                    
                    # æ™‚ç©ºæ›²ç‡åŠ¹æœ
                    if i < dim - 2:
                        curvature_term = kappa_correction * 0.05
                        H[i, i+1] += curvature_term
                        H[i+1, i] += curvature_term.conj()
                    
                    # é‡åŠ›å ´åŠ¹æœ
                    H[i, i] += kappa_correction * 0.01
                except:
                    continue
        
        # æ­£å‰‡åŒ–é …ï¼ˆé©å¿œçš„ï¼‰
        gamma = abs(s.imag)
        reg_strength = 1e-15 * (1.0 + gamma * 1e-4)
        H += reg_strength * torch.eye(dim, dtype=self.dtype, device=self.device)
        
        return H
    
    def compute_eigenvalues_ultra_stable(self, s: complex, n_eigenvalues: int = 150) -> torch.Tensor:
        """
        è¶…å®‰å®šå›ºæœ‰å€¤è¨ˆç®—
        """
        try:
            H = self.construct_enhanced_hamiltonian(s)
            
            # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            H_dag = H.conj().T
            H_hermitian = 0.5 * (H + H_dag)
            
            # æ¡ä»¶æ•°ã®æ”¹å–„
            try:
                # ç‰¹ç•°å€¤åˆ†è§£ã«ã‚ˆã‚‹å‰å‡¦ç†
                U, S, Vh = torch.linalg.svd(H_hermitian)
                
                # å°ã•ãªç‰¹ç•°å€¤ã®é™¤å»
                threshold = 1e-12
                S_filtered = torch.where(S > threshold, S, threshold)
                
                # å†æ§‹ç¯‰
                H_hermitian = torch.mm(torch.mm(U, torch.diag(S_filtered)), Vh)
                
            except:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¼·ã„æ­£å‰‡åŒ–
                reg_strength = 1e-10
                H_hermitian += reg_strength * torch.eye(H_hermitian.shape[0], 
                                                      dtype=self.dtype, device=self.device)
            
            # NaN/Inf ãƒã‚§ãƒƒã‚¯
            if torch.isnan(H_hermitian).any() or torch.isinf(H_hermitian).any():
                logger.warning("âš ï¸ ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã«NaN/InfãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            # å›ºæœ‰å€¤è¨ˆç®—ï¼ˆè¤‡æ•°æ‰‹æ³•ã®è©¦è¡Œï¼‰
            eigenvalues = None
            
            # æ‰‹æ³•1: æ¨™æº–çš„ãªå›ºæœ‰å€¤åˆ†è§£
            try:
                eigenvalues, _ = torch.linalg.eigh(H_hermitian)
                eigenvalues = eigenvalues.real
            except RuntimeError:
                # æ‰‹æ³•2: SVDåˆ†è§£
                try:
                    U, S, Vh = torch.linalg.svd(H_hermitian)
                    eigenvalues = S.real
                except RuntimeError:
                    # æ‰‹æ³•3: ä¸€èˆ¬åŒ–å›ºæœ‰å€¤å•é¡Œ
                    try:
                        I = torch.eye(H_hermitian.shape[0], dtype=self.dtype, device=self.device)
                        eigenvalues, _ = torch.linalg.eig(H_hermitian)
                        eigenvalues = eigenvalues.real
                    except:
                        logger.error("âŒ ã™ã¹ã¦ã®å›ºæœ‰å€¤è¨ˆç®—æ‰‹æ³•ãŒå¤±æ•—ã—ã¾ã—ãŸ")
                        return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            if eigenvalues is None:
                return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            # æ­£ã®å›ºæœ‰å€¤ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            positive_mask = eigenvalues > 1e-20
            positive_eigenvalues = eigenvalues[positive_mask]
            
            if len(positive_eigenvalues) == 0:
                logger.warning("âš ï¸ æ­£ã®å›ºæœ‰å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            # ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’é¸æŠ
            sorted_eigenvalues, _ = torch.sort(positive_eigenvalues, descending=True)
            
            return sorted_eigenvalues[:min(len(sorted_eigenvalues), n_eigenvalues)]
            
        except Exception as e:
            logger.error(f"âŒ å›ºæœ‰å€¤è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return torch.tensor([], device=self.device, dtype=self.float_dtype)

class UltraHighPrecisionRiemannVerifier:
    """
    è¶…é«˜ç²¾åº¦ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, hamiltonian: UltraHighPrecisionNKATHamiltonian):
        self.hamiltonian = hamiltonian
        self.device = hamiltonian.device
        
    def compute_spectral_dimension_enhanced(self, s: complex, 
                                          n_points: int = 80, 
                                          t_range: Tuple[float, float] = (1e-5, 2.0)) -> float:
        """
        å¼·åŒ–ã•ã‚ŒãŸã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
        """
        eigenvalues = self.hamiltonian.compute_eigenvalues_ultra_stable(s, n_eigenvalues=200)
        
        if len(eigenvalues) < 15:
            logger.warning("âš ï¸ æœ‰åŠ¹ãªå›ºæœ‰å€¤ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return float('nan')
        
        try:
            # é©å¿œçš„tå€¤ç¯„å›²
            gamma = abs(s.imag)
            if gamma > 30:
                t_min, t_max = 1e-6, 1.5
            elif gamma > 15:
                t_min, t_max = 1e-5, 1.8
            else:
                t_min, t_max = t_range
            
            # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®tå€¤ç”Ÿæˆ
            t_values = torch.logspace(np.log10(t_min), np.log10(t_max), n_points, device=self.device)
            zeta_values = []
            
            for t in t_values:
                # æ•°å€¤å®‰å®šæ€§ã®å¤§å¹…æ”¹å–„
                exp_terms = torch.exp(-t * eigenvalues)
                
                # æœ‰åŠ¹é …ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                valid_mask = (torch.isfinite(exp_terms) & 
                             (exp_terms > 1e-100) & 
                             (exp_terms < 1e50))
                
                if torch.sum(valid_mask) < 5:
                    zeta_values.append(1e-100)
                    continue
                
                # é‡ã¿ä»˜ãå’Œï¼ˆå¤§ããªå›ºæœ‰å€¤ã«ã‚ˆã‚Šé«˜ã„é‡ã¿ï¼‰
                weights = 1.0 / (1.0 + eigenvalues[valid_mask] * 0.1)
                weighted_sum = torch.sum(exp_terms[valid_mask] * weights)
                
                if torch.isfinite(weighted_sum) and weighted_sum > 1e-100:
                    zeta_values.append(weighted_sum.item())
                else:
                    zeta_values.append(1e-100)
            
            zeta_values = torch.tensor(zeta_values, device=self.device)
            
            # å¯¾æ•°å¾®åˆ†ã®æ”¹è‰¯è¨ˆç®—
            log_t = torch.log(t_values)
            log_zeta = torch.log(zeta_values + 1e-100)
            
            # å¤–ã‚Œå€¤ã®é™¤å»
            valid_mask = (torch.isfinite(log_zeta) & 
                         torch.isfinite(log_t) & 
                         (log_zeta > -200) & 
                         (log_zeta < 50) &
                         (torch.abs(log_zeta) < 1e10))
            
            if torch.sum(valid_mask) < 10:
                logger.warning("âš ï¸ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ç‚¹ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                return float('nan')
            
            log_t_valid = log_t[valid_mask]
            log_zeta_valid = log_zeta[valid_mask]
            
            # ãƒ­ãƒã‚¹ãƒˆå›å¸°ï¼ˆRANSACé¢¨ï¼‰
            best_slope = None
            best_score = float('inf')
            
            for _ in range(10):  # è¤‡æ•°å›è©¦è¡Œ
                # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                n_sample = min(len(log_t_valid), max(10, len(log_t_valid) // 2))
                indices = torch.randperm(len(log_t_valid))[:n_sample]
                
                t_sample = log_t_valid[indices]
                zeta_sample = log_zeta_valid[indices]
                
                try:
                    # é‡ã¿ä»˜ãæœ€å°äºŒä¹—æ³•
                    weights = torch.ones_like(t_sample)
                    # ä¸­å¤®éƒ¨åˆ†ã«ã‚ˆã‚Šé«˜ã„é‡ã¿ã‚’ä»˜ä¸
                    mid_range = (t_sample.max() + t_sample.min()) / 2
                    distance_from_mid = torch.abs(t_sample - mid_range)
                    weights = torch.exp(-distance_from_mid * 2)
                    
                    W = torch.diag(weights)
                    A = torch.stack([t_sample, torch.ones_like(t_sample)], dim=1)
                    
                    AtWA = torch.mm(torch.mm(A.T, W), A)
                    AtWy = torch.mm(torch.mm(A.T, W), zeta_sample.unsqueeze(1))
                    
                    solution = torch.linalg.solve(AtWA, AtWy)
                    slope = solution[0, 0]
                    
                    # äºˆæ¸¬èª¤å·®ã®è¨ˆç®—
                    pred = torch.mm(A, solution).squeeze()
                    error = torch.mean((pred - zeta_sample) ** 2)
                    
                    if error < best_score and torch.isfinite(slope):
                        best_score = error
                        best_slope = slope
                        
                except:
                    continue
            
            if best_slope is None:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå˜ç´”ãªæœ€å°äºŒä¹—æ³•
                try:
                    A = torch.stack([log_t_valid, torch.ones_like(log_t_valid)], dim=1)
                    solution = torch.linalg.lstsq(A, log_zeta_valid).solution
                    best_slope = solution[0]
                except:
                    logger.warning("âš ï¸ å›å¸°è¨ˆç®—ãŒå¤±æ•—ã—ã¾ã—ãŸ")
                    return float('nan')
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã®è¨ˆç®—
            spectral_dimension = -2 * best_slope.item()
            
            # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šå³å¯†ï¼‰
            if (abs(spectral_dimension) > 50 or 
                not np.isfinite(spectral_dimension) or
                abs(spectral_dimension) < 1e-10):
                logger.warning(f"âš ï¸ ç•°å¸¸ãªã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒå€¤: {spectral_dimension}")
                return float('nan')
            
            return spectral_dimension
            
        except Exception as e:
            logger.error(f"âŒ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('nan')
    
    def verify_critical_line_ultra_precision(self, gamma_values: List[float], 
                                           iterations: int = 5) -> Dict:
        """
        è¶…é«˜ç²¾åº¦è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼
        """
        results = {
            'gamma_values': gamma_values,
            'spectral_dimensions_all': [],
            'real_parts_all': [],
            'convergence_to_half_all': [],
            'statistics': {}
        }
        
        logger.info(f"ğŸ” è¶…é«˜ç²¾åº¦è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼é–‹å§‹ï¼ˆ{iterations}å›å®Ÿè¡Œï¼‰...")
        
        for iteration in range(iterations):
            logger.info(f"ğŸ“Š å®Ÿè¡Œ {iteration + 1}/{iterations}")
            
            spectral_dims = []
            real_parts = []
            convergences = []
            
            for gamma in tqdm(gamma_values, desc=f"å®Ÿè¡Œ{iteration+1}: Î³å€¤ã§ã®æ¤œè¨¼"):
                s = 0.5 + 1j * gamma
                
                # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã®è¨ˆç®—
                d_s = self.compute_spectral_dimension_enhanced(s)
                spectral_dims.append(d_s)
                
                if not np.isnan(d_s):
                    # å®Ÿéƒ¨ã®è¨ˆç®—ï¼ˆç†è«–çš„ã«ã¯ d_s/2 â‰ˆ 0.5ï¼‰
                    real_part = d_s / 2
                    real_parts.append(real_part)
                    
                    # 1/2ã¸ã®åæŸæ€§
                    convergence = abs(real_part - 0.5)
                    convergences.append(convergence)
                else:
                    real_parts.append(np.nan)
                    convergences.append(np.nan)
            
            results['spectral_dimensions_all'].append(spectral_dims)
            results['real_parts_all'].append(real_parts)
            results['convergence_to_half_all'].append(convergences)
        
        # çµ±è¨ˆçš„è©•ä¾¡
        all_spectral_dims = np.array(results['spectral_dimensions_all'])
        all_real_parts = np.array(results['real_parts_all'])
        all_convergences = np.array(results['convergence_to_half_all'])
        
        # å„Î³å€¤ã§ã®çµ±è¨ˆ
        results['statistics'] = {
            'spectral_dimension_mean': np.nanmean(all_spectral_dims, axis=0).tolist(),
            'spectral_dimension_std': np.nanstd(all_spectral_dims, axis=0).tolist(),
            'spectral_dimension_median': np.nanmedian(all_spectral_dims, axis=0).tolist(),
            'real_part_mean': np.nanmean(all_real_parts, axis=0).tolist(),
            'real_part_std': np.nanstd(all_real_parts, axis=0).tolist(),
            'convergence_mean': np.nanmean(all_convergences, axis=0).tolist(),
            'convergence_std': np.nanstd(all_convergences, axis=0).tolist(),
        }
        
        # å…¨ä½“çµ±è¨ˆ
        valid_convergences = all_convergences[~np.isnan(all_convergences)]
        if len(valid_convergences) > 0:
            results['overall_statistics'] = {
                'mean_convergence': np.mean(valid_convergences),
                'median_convergence': np.median(valid_convergences),
                'std_convergence': np.std(valid_convergences),
                'min_convergence': np.min(valid_convergences),
                'max_convergence': np.max(valid_convergences),
                'success_rate_strict': np.sum(valid_convergences < 0.01) / len(valid_convergences),
                'success_rate_moderate': np.sum(valid_convergences < 0.1) / len(valid_convergences),
                'success_rate_loose': np.sum(valid_convergences < 0.2) / len(valid_convergences)
            }
        
        return results

def demonstrate_ultra_high_precision_riemann():
    """
    è¶…é«˜ç²¾åº¦ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("=" * 80)
    print("ğŸ¯ NKATç†è«–ã«ã‚ˆã‚‹è¶…é«˜ç²¾åº¦ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ (æ”¹è‰¯ç‰ˆ)")
    print("=" * 80)
    print("ğŸ“… å®Ÿè¡Œæ—¥æ™‚:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("ğŸ”¬ ç²¾åº¦: complex128 (å€ç²¾åº¦) + æ•°å€¤å®‰å®šæ€§å¼·åŒ–")
    print("ğŸ§® æ”¹è‰¯ç‚¹: é©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ­ãƒã‚¹ãƒˆå›å¸°ã€è¤‡æ•°æ‰‹æ³•è©¦è¡Œ")
    print("=" * 80)
    
    # è¶…é«˜ç²¾åº¦ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®åˆæœŸåŒ–
    logger.info("ğŸ”§ è¶…é«˜ç²¾åº¦NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³åˆæœŸåŒ–ä¸­...")
    hamiltonian = UltraHighPrecisionNKATHamiltonian(
        max_n=1200,
        base_theta=1e-20,
        base_kappa=1e-12,
        precision='ultra'
    )
    
    # è¶…é«˜ç²¾åº¦æ¤œè¨¼å™¨ã®åˆæœŸåŒ–
    verifier = UltraHighPrecisionRiemannVerifier(hamiltonian)
    
    # è¶…é«˜ç²¾åº¦è‡¨ç•Œç·šæ¤œè¨¼
    print("\nğŸ“Š è¶…é«˜ç²¾åº¦è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼")
    gamma_values = [14.134725, 21.022040, 25.010858, 30.424876, 32.935062]
    
    start_time = time.time()
    ultra_precision_results = verifier.verify_critical_line_ultra_precision(
        gamma_values, iterations=5
    )
    verification_time = time.time() - start_time
    
    # çµæœã®è¡¨ç¤º
    print("\nè¶…é«˜ç²¾åº¦æ¤œè¨¼çµæœ:")
    print("Î³å€¤      | å¹³å‡d_s    | ä¸­å¤®å€¤d_s  | æ¨™æº–åå·®   | å¹³å‡Re     | |Re-1/2|å¹³å‡ | åæŸæ€§")
    print("-" * 90)
    
    stats = ultra_precision_results['statistics']
    for i, gamma in enumerate(gamma_values):
        mean_ds = stats['spectral_dimension_mean'][i]
        median_ds = stats['spectral_dimension_median'][i]
        std_ds = stats['spectral_dimension_std'][i]
        mean_re = stats['real_part_mean'][i]
        mean_conv = stats['convergence_mean'][i]
        
        if not np.isnan(mean_ds):
            if mean_conv < 0.01:
                status = "âœ…"
            elif mean_conv < 0.1:
                status = "ğŸŸ¡"
            else:
                status = "âš ï¸"
            print(f"{gamma:8.6f} | {mean_ds:9.6f} | {median_ds:9.6f} | {std_ds:9.6f} | {mean_re:9.6f} | {mean_conv:10.6f} | {status}")
        else:
            print(f"{gamma:8.6f} | {'NaN':>9} | {'NaN':>9} | {'NaN':>9} | {'NaN':>9} | {'NaN':>10} | âŒ")
    
    # å…¨ä½“çµ±è¨ˆã®è¡¨ç¤º
    if 'overall_statistics' in ultra_precision_results:
        overall = ultra_precision_results['overall_statistics']
        print(f"\nğŸ“Š å…¨ä½“çµ±è¨ˆ:")
        print(f"å¹³å‡åæŸç‡: {overall['mean_convergence']:.8f}")
        print(f"ä¸­å¤®å€¤åæŸç‡: {overall['median_convergence']:.8f}")
        print(f"æ¨™æº–åå·®: {overall['std_convergence']:.8f}")
        print(f"å³å¯†æˆåŠŸç‡ (<0.01): {overall['success_rate_strict']:.2%}")
        print(f"ä¸­ç¨‹åº¦æˆåŠŸç‡ (<0.1): {overall['success_rate_moderate']:.2%}")
        print(f"ç·©ã„æˆåŠŸç‡ (<0.2): {overall['success_rate_loose']:.2%}")
        print(f"æœ€è‰¯åæŸ: {overall['min_convergence']:.8f}")
        print(f"æœ€æ‚ªåæŸ: {overall['max_convergence']:.8f}")
    
    print(f"\nâ±ï¸  æ¤œè¨¼æ™‚é–“: {verification_time:.2f}ç§’")
    
    # çµæœã®ä¿å­˜
    with open('ultra_high_precision_riemann_results.json', 'w', encoding='utf-8') as f:
        json.dump(ultra_precision_results, f, indent=2, ensure_ascii=False, default=str)
    
    print("ğŸ’¾ è¶…é«˜ç²¾åº¦çµæœã‚’ 'ultra_high_precision_riemann_results.json' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    return ultra_precision_results

if __name__ == "__main__":
    """
    è¶…é«˜ç²¾åº¦ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®å®Ÿè¡Œ
    """
    try:
        results = demonstrate_ultra_high_precision_riemann()
        print("ğŸ‰ è¶…é«˜ç²¾åº¦æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}") 
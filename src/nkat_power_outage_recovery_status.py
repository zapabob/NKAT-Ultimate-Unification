#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”‹ NKAT é›»æºæ–­å¾©æ—§çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 
ç¾åœ¨ã®å¾©æ—§é€²æ—ã€å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹ã€çµæœçµ±åˆ
"""
import os
import json
import glob
import time
import psutil
from datetime import datetime
import subprocess
import matplotlib.pyplot as plt
import numpy as np
from typing import Dict, List, Any, Optional

# è‹±èªè¡¨è¨˜è¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class NKATPowerOutageRecoveryStatus:
    """é›»æºæ–­å¾©æ—§çŠ¶æ³åˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.report_dir = f"recovery_status_{self.timestamp}"
        os.makedirs(self.report_dir, exist_ok=True)
        
        print("ğŸ”‹ NKAT Power Outage Recovery Status System")
        print("="*60)
        print(f"ğŸ“… Check Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ Report Dir: {self.report_dir}")
        print("="*60)
    
    def check_running_processes(self) -> Dict[str, List[Dict]]:
        """å®Ÿè¡Œä¸­ã®NKATãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª"""
        
        print("ğŸ” Scanning Running NKAT Processes...")
        
        nkat_processes = []
        python_processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'create_time', 'memory_info']):
            try:
                cmdline = proc.info['cmdline']
                if cmdline:
                    cmdline_str = ' '.join(cmdline)
                    
                    # NKATã‚¹ã‚¯ãƒªãƒ—ãƒˆæ¤œå‡º
                    if 'nkat' in cmdline_str.lower() and '.py' in cmdline_str:
                        process_info = {
                            'pid': proc.info['pid'],
                            'name': proc.info['name'],
                            'cmdline': cmdline_str,
                            'runtime_minutes': (time.time() - proc.info['create_time']) / 60,
                            'memory_mb': proc.info['memory_info'].rss / (1024*1024) if proc.info['memory_info'] else 0
                        }
                        
                        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¿ã‚¤ãƒ—è­˜åˆ¥
                        if 'emergency_recovery' in cmdline_str:
                            process_info['type'] = 'Emergency Recovery'
                        elif 'stage5' in cmdline_str:
                            process_info['type'] = 'Stage5 Interpretability'
                        elif 'enhanced_transformer_v2' in cmdline_str:
                            process_info['type'] = 'Enhanced Training'
                        elif 'mission_recovery' in cmdline_str:
                            process_info['type'] = 'Mission Recovery'
                        else:
                            process_info['type'] = 'Other NKAT'
                        
                        nkat_processes.append(process_info)
                        
                    # Python/GPUé–¢é€£ãƒ—ãƒ­ã‚»ã‚¹
                    elif 'python' in proc.info['name'].lower():
                        python_processes.append({
                            'pid': proc.info['pid'],
                            'cmdline': cmdline_str[:100] + '...' if len(cmdline_str) > 100 else cmdline_str,
                            'memory_mb': proc.info['memory_info'].rss / (1024*1024) if proc.info['memory_info'] else 0
                        })
            
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        
        print(f"âœ… Found {len(nkat_processes)} NKAT processes")
        for proc in nkat_processes:
            print(f"  ğŸ”„ {proc['type']}: PID {proc['pid']}, {proc['runtime_minutes']:.1f}min, {proc['memory_mb']:.0f}MB")
        
        return {
            'nkat_processes': nkat_processes,
            'python_processes': python_processes[:5]  # ä¸Šä½5å€‹
        }
    
    def scan_recovery_outputs(self) -> Dict[str, Any]:
        """å¾©æ—§é–¢é€£ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³"""
        
        print("\nğŸ“‚ Scanning Recovery Output Files...")
        
        output_files = {
            'emergency_recovery': [],
            'stage_results': [],
            'checkpoints': [],
            'visualizations': [],
            'logs': [],
            'reports': []
        }
        
        # Emergency Recoveryé–¢é€£
        emergency_patterns = [
            "recovery_data/emergency_*/*.pth",
            "recovery_data/emergency_*/*.json",
            "recovery_data/emergency_*/*.png"
        ]
        
        for pattern in emergency_patterns:
            files = glob.glob(pattern)
            for file in files:
                output_files['emergency_recovery'].append({
                    'path': file,
                    'size_mb': os.path.getsize(file) / (1024*1024),
                    'modified': os.path.getmtime(file),
                    'type': 'checkpoint' if '.pth' in file else 'json' if '.json' in file else 'visualization'
                })
        
        # Stageçµæœ
        stage_patterns = [
            "logs/*stage*20250601*.json",
            "*stage*20250601*.png",
            "nkat_comprehensive_analysis_*.png",
            "NKAT_Comprehensive_Analysis_Report_*.md"
        ]
        
        for pattern in stage_patterns:
            files = glob.glob(pattern)
            for file in files:
                output_files['stage_results'].append({
                    'path': file,
                    'size_mb': os.path.getsize(file) / (1024*1024),
                    'modified': os.path.getmtime(file)
                })
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        checkpoint_patterns = [
            "checkpoints/*.pth",
            "checkpoints/*/*.pth"
        ]
        
        for pattern in checkpoint_patterns:
            files = glob.glob(pattern)
            for file in files:
                if os.path.getsize(file) > 1024*1024:  # 1MBä»¥ä¸Š
                    output_files['checkpoints'].append({
                        'path': file,
                        'size_mb': os.path.getsize(file) / (1024*1024),
                        'modified': os.path.getmtime(file)
                    })
        
        # å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«
        viz_patterns = ["*.png", "*.jpg"]
        for pattern in viz_patterns:
            files = glob.glob(pattern)
            for file in files:
                if 'nkat' in file.lower() and os.path.getmtime(file) > time.time() - 3600:  # 1æ™‚é–“ä»¥å†…
                    output_files['visualizations'].append({
                        'path': file,
                        'size_mb': os.path.getsize(file) / (1024*1024),
                        'modified': os.path.getmtime(file)
                    })
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
        log_patterns = ["logs/*.json", "logs/*.log"]
        for pattern in log_patterns:
            files = glob.glob(pattern)
            for file in files:
                if os.path.getmtime(file) > time.time() - 3600:  # 1æ™‚é–“ä»¥å†…
                    output_files['logs'].append({
                        'path': file,
                        'size_mb': os.path.getsize(file) / (1024*1024),
                        'modified': os.path.getmtime(file)
                    })
        
        # ãƒ¬ãƒãƒ¼ãƒˆ
        report_patterns = ["*.md", "*report*.json"]
        for pattern in report_patterns:
            files = glob.glob(pattern)
            for file in files:
                if 'nkat' in file.lower() and os.path.getmtime(file) > time.time() - 3600:  # 1æ™‚é–“ä»¥å†…
                    output_files['reports'].append({
                        'path': file,
                        'size_mb': os.path.getsize(file) / (1024*1024),
                        'modified': os.path.getmtime(file)
                    })
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        total_files = sum(len(files) for files in output_files.values())
        print(f"âœ… Found {total_files} recovery-related files:")
        
        for category, files in output_files.items():
            if files:
                print(f"  ğŸ“‹ {category.replace('_', ' ').title()}: {len(files)} files")
                # æœ€æ–°2ãƒ•ã‚¡ã‚¤ãƒ«è¡¨ç¤º
                sorted_files = sorted(files, key=lambda x: x['modified'], reverse=True)
                for file in sorted_files[:2]:
                    print(f"    - {file['path']} ({file['size_mb']:.1f}MB)")
        
        return output_files
    
    def analyze_latest_results(self, output_files: Dict[str, Any]) -> Dict[str, Any]:
        """æœ€æ–°çµæœåˆ†æ"""
        
        print("\nğŸ”¬ Analyzing Latest Results...")
        
        analysis = {
            'recovery_status': 'Unknown',
            'latest_accuracy': None,
            'stage_progress': {},
            'emergency_training_status': 'Unknown',
            'key_findings': []
        }
        
        # Emergency Recoveryçµæœç¢ºèª
        emergency_jsons = [f for f in output_files['emergency_recovery'] if f['type'] == 'json']
        if emergency_jsons:
            latest_emergency = max(emergency_jsons, key=lambda x: x['modified'])
            try:
                with open(latest_emergency['path'], 'r', encoding='utf-8') as f:
                    emergency_data = json.load(f)
                
                analysis['latest_accuracy'] = emergency_data.get('final_accuracy', None)
                analysis['recovery_status'] = emergency_data.get('recovery_status', 'Unknown')
                analysis['emergency_training_status'] = 'Completed'
                
                if emergency_data.get('final_accuracy', 0) > 0.8:
                    analysis['key_findings'].append("ğŸŸ¢ Emergency recovery achieved >80% accuracy")
                elif emergency_data.get('final_accuracy', 0) > 0.5:
                    analysis['key_findings'].append("ğŸŸ¡ Emergency recovery achieved >50% accuracy")
                else:
                    analysis['key_findings'].append("ğŸ”´ Emergency recovery needs improvement")
                
                print(f"ğŸ“Š Emergency Recovery Status: {analysis['recovery_status']}")
                if analysis['latest_accuracy']:
                    print(f"ğŸ¯ Latest Accuracy: {analysis['latest_accuracy']:.3f}")
                
            except Exception as e:
                print(f"âš ï¸ Could not read emergency recovery data: {e}")
        
        # Stageçµæœç¢ºèª
        stage_jsons = [f for f in output_files['stage_results'] if '.json' in f['path']]
        if stage_jsons:
            latest_stage = max(stage_jsons, key=lambda x: x['modified'])
            try:
                with open(latest_stage['path'], 'r', encoding='utf-8') as f:
                    stage_data = json.load(f)
                
                # Stage2ãƒ‡ãƒ¼ã‚¿
                if 'stage2' in stage_data:
                    s2_data = stage_data['stage2']
                    analysis['stage_progress']['Stage2'] = {
                        'global_tpe': s2_data.get('global_tpe', 0),
                        'status': 'âœ… Complete' if s2_data.get('global_tpe', 0) > 0.5 else 'âš ï¸ Needs Improvement'
                    }
                
                # Stage3ãƒ‡ãƒ¼ã‚¿
                if 'stage3' in stage_data:
                    s3_data = stage_data['stage3']
                    analysis['stage_progress']['Stage3'] = {
                        'robustness_score': s3_data.get('robustness_score', 0),
                        'status': 'âœ… Complete' if s3_data.get('robustness_score', 0) > 50 else 'âš ï¸ Needs Improvement'
                    }
                
                print(f"ğŸ“‹ Stage Progress: {len(analysis['stage_progress'])} stages analyzed")
                
            except Exception as e:
                print(f"âš ï¸ Could not read stage data: {e}")
        
        # ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ³ç¢ºèª
        if output_files.get('emergency_recovery'):
            latest_emergency_files = sorted(output_files['emergency_recovery'], 
                                          key=lambda x: x['modified'], reverse=True)
            if latest_emergency_files:
                latest_time = latest_emergency_files[0]['modified']
                if time.time() - latest_time < 300:  # 5åˆ†ä»¥å†…
                    analysis['key_findings'].append("ğŸ”„ Emergency recovery recently active")
        
        return analysis
    
    def create_status_visualization(self, processes: Dict, output_files: Dict, analysis: Dict) -> str:
        """å¾©æ—§çŠ¶æ³å¯è¦–åŒ–"""
        
        print("\nğŸ“ˆ Creating Recovery Status Visualization...")
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('NKAT Power Outage Recovery Status', fontsize=16, fontweight='bold')
        
        # 1. å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹
        nkat_procs = processes['nkat_processes']
        if nkat_procs:
            proc_types = [p['type'] for p in nkat_procs]
            proc_memories = [p['memory_mb'] for p in nkat_procs]
            
            bars = ax1.bar(range(len(proc_types)), proc_memories, 
                          color=['green', 'blue', 'orange', 'red'][:len(proc_types)])
            ax1.set_xlabel('Process Type')
            ax1.set_ylabel('Memory Usage (MB)')
            ax1.set_title('Active NKAT Processes')
            ax1.set_xticks(range(len(proc_types)))
            ax1.set_xticklabels(proc_types, rotation=45, ha='right')
            
            for bar, memory in zip(bars, proc_memories):
                ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,
                        f'{memory:.0f}MB', ha='center', va='bottom')
        else:
            ax1.text(0.5, 0.5, 'No Active NKAT Processes', ha='center', va='center',
                    transform=ax1.transAxes, fontsize=14)
            ax1.set_title('Active NKAT Processes')
        
        # 2. å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ
        file_categories = list(output_files.keys())
        file_counts = [len(files) for files in output_files.values()]
        
        if any(file_counts):
            bars = ax2.bar(file_categories, file_counts, color='skyblue')
            ax2.set_xlabel('File Category')
            ax2.set_ylabel('Number of Files')
            ax2.set_title('Recovery Output Files')
            ax2.tick_params(axis='x', rotation=45)
            
            for bar, count in zip(bars, file_counts):
                if count > 0:
                    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                            str(count), ha='center', va='bottom')
        else:
            ax2.text(0.5, 0.5, 'No Output Files Found', ha='center', va='center',
                    transform=ax2.transAxes, fontsize=14)
            ax2.set_title('Recovery Output Files')
        
        # 3. å¾©æ—§çŠ¶æ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        metrics = []
        values = []
        colors = []
        
        if analysis['latest_accuracy'] is not None:
            metrics.append('Latest Accuracy')
            values.append(analysis['latest_accuracy'] * 100)
            colors.append('green' if analysis['latest_accuracy'] > 0.8 else 'orange' if analysis['latest_accuracy'] > 0.5 else 'red')
        
        if 'Stage2' in analysis['stage_progress']:
            metrics.append('Stage2 TPE')
            values.append(analysis['stage_progress']['Stage2']['global_tpe'] * 100)
            colors.append('green' if analysis['stage_progress']['Stage2']['global_tpe'] > 0.5 else 'red')
        
        if 'Stage3' in analysis['stage_progress']:
            metrics.append('Stage3 Robustness')
            values.append(analysis['stage_progress']['Stage3']['robustness_score'])
            colors.append('green' if analysis['stage_progress']['Stage3']['robustness_score'] > 50 else 'red')
        
        if metrics:
            bars = ax3.barh(metrics, values, color=colors)
            ax3.set_xlabel('Score (%)')
            ax3.set_title('Recovery Performance Metrics')
            ax3.set_xlim(0, 100)
            
            for bar, value in zip(bars, values):
                ax3.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                        f'{value:.1f}%', ha='left', va='center')
        else:
            ax3.text(0.5, 0.5, 'No Metrics Available', ha='center', va='center',
                    transform=ax3.transAxes, fontsize=14)
            ax3.set_title('Recovery Performance Metrics')
        
        # 4. å¾©æ—§ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³
        ax4.axis('off')
        
        timeline_text = f"""Recovery Timeline Status
        
ğŸ”‹ Power Outage Recovery Initiated
ğŸ“… Current Time: {datetime.now().strftime('%H:%M:%S')}

ğŸ“Š Current Status: {analysis['recovery_status']}
ğŸ”„ Emergency Training: {analysis['emergency_training_status']}

ğŸ” Key Findings:
"""
        
        for finding in analysis['key_findings'][:5]:  # æœ€å¤§5å€‹
            timeline_text += f"\nâ€¢ {finding}"
        
        if len(analysis['key_findings']) > 5:
            timeline_text += f"\nâ€¢ ... and {len(analysis['key_findings']) - 5} more"
        
        timeline_text += f"""

ğŸ“ Total Output Files: {sum(len(files) for files in output_files.values())}
ğŸ”„ Active Processes: {len(processes['nkat_processes'])}"""
        
        ax4.text(0.1, 0.9, timeline_text, transform=ax4.transAxes, 
                fontsize=11, verticalalignment='top', family='monospace')
        
        plt.tight_layout()
        
        # ä¿å­˜
        viz_path = os.path.join(self.report_dir, f"nkat_recovery_status_{self.timestamp}.png")
        plt.savefig(viz_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š Status visualization saved: {viz_path}")
        return viz_path

    def generate_recovery_report(self, processes: Dict, output_files: Dict, analysis: Dict, viz_path: str) -> str:
        """å¾©æ—§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        print("\nğŸ“ Generating Recovery Status Report...")
        
        # ç²¾åº¦ã®è¡¨ç¤ºå€¤ã‚’äº‹å‰ã«è¨ˆç®—
        accuracy_str = f"{analysis['latest_accuracy']:.3f}" if analysis['latest_accuracy'] is not None else 'N/A'
        
        report_content = f"""# ğŸ”‹ NKAT é›»æºæ–­å¾©æ—§çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
**ãƒ¬ãƒãƒ¼ãƒˆID**: {self.timestamp}

## ğŸ“Š Executive Summary

### ğŸ¯ å¾©æ—§çŠ¶æ³
- **ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**: {analysis['recovery_status']}
- **ç·Šæ€¥è¨“ç·´**: {analysis['emergency_training_status']}
- **æœ€æ–°ç²¾åº¦**: {accuracy_str}

### ğŸ”„ å®Ÿè¡Œä¸­ãƒ—ãƒ­ã‚»ã‚¹
- **ã‚¢ã‚¯ãƒ†ã‚£ãƒ–NKATãƒ—ãƒ­ã‚»ã‚¹**: {len(processes['nkat_processes'])}å€‹
"""
        
        for proc in processes['nkat_processes']:
            report_content += f"  - {proc['type']}: PID {proc['pid']}, å®Ÿè¡Œæ™‚é–“ {proc['runtime_minutes']:.1f}åˆ†\n"
        
        report_content += f"""
### ğŸ“‚ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ
"""
        
        total_files = 0
        for category, files in output_files.items():
            if files:
                total_files += len(files)
                report_content += f"- **{category.replace('_', ' ').title()}**: {len(files)}å€‹\n"
        
        report_content += f"\n**ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {total_files}å€‹\n"
        
        report_content += f"""
## ğŸ” è©³ç´°åˆ†æ

### Stageåˆ¥é€²æ—
"""
        
        for stage, data in analysis['stage_progress'].items():
            report_content += f"- **{stage}**: {data['status']}\n"
            for key, value in data.items():
                if key != 'status':
                    if isinstance(value, float):
                        report_content += f"  - {key}: {value:.3f}\n"
                    else:
                        report_content += f"  - {key}: {value}\n"
        
        report_content += f"""
### ğŸ” ä¸»è¦ç™ºè¦‹äº‹é …
"""
        
        for finding in analysis['key_findings']:
            report_content += f"- {finding}\n"
        
        report_content += f"""
## ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

### å³åº§ã«å®Ÿè¡Œã™ã¹ãé …ç›®
"""
        
        if analysis['latest_accuracy'] and analysis['latest_accuracy'] < 0.8:
            report_content += "1. **ç·Šæ€¥è¨“ç·´ã®å»¶é•·**: ç¾åœ¨ã®ç²¾åº¦ãŒ80%æœªæº€ã®ãŸã‚ã€è¿½åŠ è¨“ç·´ãŒå¿…è¦\n"
        
        if not analysis['stage_progress']:
            report_content += "1. **Stageãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ**: Stage2-5ã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ\n"
        
        if len(processes['nkat_processes']) == 0:
            report_content += "1. **ãƒ—ãƒ­ã‚»ã‚¹å†é–‹**: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªNKATãƒ—ãƒ­ã‚»ã‚¹ãŒæ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“\n"
        
        report_content += f"""
### ä¸­æœŸçš„ãªæ”¹å–„
1. **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†**: å®šæœŸçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ä½“åˆ¶ã®å¼·åŒ–
2. **è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ **: é›»æºæ–­å¯¾å¿œã®è‡ªå‹•åŒ–
3. **ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çŠ¶æ³ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®å°å…¥

## ğŸ“ˆ å¾©æ—§å®Œäº†åˆ¤å®šåŸºæº–

âœ… **å®Œå…¨å¾©æ—§**:
- ç·Šæ€¥è¨“ç·´ç²¾åº¦ â‰¥ 80%
- Stage2 TPE â‰¥ 0.50
- Stage3 Robustness â‰¥ 50%
- å…¨Stageãƒ†ã‚¹ãƒˆå®Œäº†

ğŸŸ¡ **éƒ¨åˆ†å¾©æ—§**:
- ç·Šæ€¥è¨“ç·´ç²¾åº¦ â‰¥ 50%
- ä¸»è¦æ©Ÿèƒ½ãŒå‹•ä½œ

ğŸŸ¢ **å¾©æ—§æˆåŠŸ**:
- ç·Šæ€¥è¨“ç·´ç²¾åº¦ â‰¥ 80%
- Stage2 TPE â‰¥ 0.50
- Stage3 Robustness â‰¥ 50%
- å…¨Stageãƒ†ã‚¹ãƒˆå®Œäº†

ğŸ”´ **å¾©æ—§å¤±æ•—**:
- ç·Šæ€¥è¨“ç·´ç²¾åº¦ < 50%
- é‡è¦ãªãƒ—ãƒ­ã‚»ã‚¹ãŒåœæ­¢

## ğŸ“Š é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«

- **çŠ¶æ³å¯è¦–åŒ–**: {viz_path}
- **ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---
*Generated by NKAT Power Outage Recovery Status System*
"""
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_path = os.path.join(self.report_dir, f"nkat_recovery_status_report_{self.timestamp}.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“ Recovery report saved: {report_path}")
        return report_path

    def run_full_status_check(self) -> Dict[str, str]:
        """å®Œå…¨çŠ¶æ³ç¢ºèªå®Ÿè¡Œ"""
        
        print("ğŸ”‹ Starting Full Recovery Status Check...")
        
        # 1. ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèª
        processes = self.check_running_processes()
        
        # 2. å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³
        output_files = self.scan_recovery_outputs()
        
        # 3. çµæœåˆ†æ
        analysis = self.analyze_latest_results(output_files)
        
        # 4. å¯è¦–åŒ–ä½œæˆ
        viz_path = self.create_status_visualization(processes, output_files, analysis)
        
        # 5. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_path = self.generate_recovery_report(processes, output_files, analysis, viz_path)
        
        # 6. JSONä¿å­˜
        status_data = {
            'timestamp': self.timestamp,
            'processes': processes,
            'output_files': {k: len(v) for k, v in output_files.items()},  # ã‚µã‚¤ã‚ºå‰Šæ¸›
            'analysis': analysis,
            'files': {
                'visualization': viz_path,
                'report': report_path
            }
        }
        
        json_path = os.path.join(self.report_dir, f"nkat_recovery_status_{self.timestamp}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(status_data, f, indent=2, ensure_ascii=False)
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "="*60)
        print("ğŸ”‹ POWER OUTAGE RECOVERY STATUS SUMMARY")
        print("="*60)
        print(f"ğŸ¯ Recovery Status: {analysis['recovery_status']}")
        print(f"âš¡ Emergency Training: {analysis['emergency_training_status']}")
        if analysis['latest_accuracy']:
            print(f"ğŸ“Š Latest Accuracy: {analysis['latest_accuracy']:.1%}")
        print(f"ğŸ”„ Active Processes: {len(processes['nkat_processes'])}")
        print(f"ğŸ“‚ Output Files: {sum(len(files) for files in output_files.values())}")
        print(f"ğŸ“‹ Key Findings: {len(analysis['key_findings'])}")
        print()
        print(f"ğŸ“Š Visualization: {viz_path}")
        print(f"ğŸ“ Report: {report_path}")
        print(f"ğŸ’¾ Data: {json_path}")
        print("="*60)
        
        return {
            'visualization': viz_path,
            'report': report_path,
            'data': json_path
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    # å¾©æ—§çŠ¶æ³ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•
    status_checker = NKATPowerOutageRecoveryStatus()
    
    # å®Œå…¨çŠ¶æ³ç¢ºèªå®Ÿè¡Œ
    results = status_checker.run_full_status_check()
    
    print("\nğŸ‰ Recovery status check completed successfully!")
    print("ğŸ“‹ Check the generated files for detailed information.")

if __name__ == "__main__":
    main() 
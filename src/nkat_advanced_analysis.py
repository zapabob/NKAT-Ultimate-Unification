#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKATé«˜åº¦è§£æã‚·ã‚¹ãƒ†ãƒ ï¼šã‚°ãƒ©ãƒ•çµæœã«åŸºã¥ãè©³ç´°è§£æ
Advanced NKAT Analysis System: Detailed Analysis Based on Graph Results

Author: å³¯å²¸ äº® (Ryo Minegishi)
Date: 2025å¹´5æœˆ28æ—¥
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit
from scipy.integrate import quad
from scipy.special import zeta
import pandas as pd
from tqdm import tqdm

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class NKATAdvancedAnalysis:
    """
    NKATç†è«–ã®é«˜åº¦è§£æã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # ã‚°ãƒ©ãƒ•ã‹ã‚‰èª­ã¿å–ã£ãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gamma_opt = 0.234  # å¯†åº¦é–¢æ•°ã®ä¸»è¦ä¿‚æ•°
        self.delta_opt = 0.035  # æŒ‡æ•°æ¸›è¡°ä¿‚æ•°
        self.t_c_opt = 17.26    # è‡¨ç•Œç‚¹
        
        print("ğŸ”¬ NKATé«˜åº¦è§£æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“Š æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: Î³={self.gamma_opt}, Î´={self.delta_opt}, t_c={self.t_c_opt}")
    
    def riemann_zeta_connection_analysis(self):
        """
        ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ã¨ã®æ¥ç¶šè§£æ
        """
        print("\nğŸ¯ ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ã¨ã®æ¥ç¶šè§£æ...")
        
        # è‡¨ç•Œç·šä¸Šã®ç‚¹ã§ã®è§£æ
        s_values = np.array([0.5 + 1j*t for t in np.linspace(14, 50, 100)])
        
        # è¶…åæŸå› å­ã®äºˆæ¸¬å€¤
        N_values = np.logspace(1, 3, 50)
        S_predicted = []
        
        for N in N_values:
            # ç†è«–çš„è¶…åæŸå› å­
            integral = self.gamma_opt * np.log(N / self.t_c_opt)
            if N > self.t_c_opt:
                integral += self.delta_opt * (N - self.t_c_opt)
            S_val = np.exp(integral)
            S_predicted.append(S_val)
        
        # ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã®æ•°å€¤çš„æ¤œè¨¼
        convergence_rate = self.gamma_opt * np.log(1000 / self.t_c_opt)
        riemann_condition = abs(convergence_rate - 0.5)
        
        print(f"ğŸ“ˆ åæŸç‡ Î³Â·ln(N/t_c): {convergence_rate:.6f}")
        print(f"ğŸ¯ ãƒªãƒ¼ãƒãƒ³æ¡ä»¶ã‹ã‚‰ã®åå·®: {riemann_condition:.6f}")
        print(f"âœ… ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¡ä»¶: {'æº€è¶³' if riemann_condition < 0.1 else 'è¦æ¤œè¨'}")
        
        return convergence_rate, riemann_condition
    
    def critical_point_analysis(self):
        """
        è‡¨ç•Œç‚¹ã®è©³ç´°è§£æ
        """
        print("\nğŸ” è‡¨ç•Œç‚¹è§£æ...")
        
        # è‡¨ç•Œç‚¹è¿‘å‚ã§ã®æŒ™å‹•
        t_range = np.linspace(15, 20, 1000)
        density_values = []
        error_values = []
        
        for t in t_range:
            # å¯†åº¦é–¢æ•°
            rho = self.gamma_opt / t
            if t > self.t_c_opt:
                rho += self.delta_opt * np.exp(-self.delta_opt * (t - self.t_c_opt))
            density_values.append(rho)
            
            # èª¤å·®é–¢æ•°
            error = 1.0 / t
            if t > self.t_c_opt:
                error += 0.1 * np.exp(-self.delta_opt * (t - self.t_c_opt))
            error_values.append(error)
        
        # è‡¨ç•ŒæŒ‡æ•°ã®æ¨å®š
        pre_critical = t_range[t_range < self.t_c_opt]
        post_critical = t_range[t_range > self.t_c_opt]
        
        if len(pre_critical) > 0 and len(post_critical) > 0:
            pre_slope = np.gradient(np.log(density_values[:len(pre_critical)]), 
                                  np.log(pre_critical))
            post_slope = np.gradient(np.log(density_values[len(pre_critical):]), 
                                   np.log(post_critical))
            
            critical_exponent = np.mean(post_slope) - np.mean(pre_slope)
            print(f"ğŸ“Š è‡¨ç•ŒæŒ‡æ•°: {critical_exponent:.4f}")
        
        # ç›¸è»¢ç§»ã®ç‰¹æ€§
        transition_width = 2 * self.delta_opt  # æŒ‡æ•°æ¸›è¡°ã®ç‰¹æ€§å¹…
        print(f"ğŸŒŠ ç›¸è»¢ç§»å¹…: {transition_width:.4f}")
        print(f"ğŸ¯ è‡¨ç•Œæ¸©åº¦: t_c = {self.t_c_opt:.4f}")
        
        return critical_exponent, transition_width
    
    def quantum_classical_correspondence(self):
        """
        é‡å­å¤å…¸å¯¾å¿œã®è§£æ
        """
        print("\nâš›ï¸ é‡å­å¤å…¸å¯¾å¿œè§£æ...")
        
        N_values = np.array([50, 100, 200, 500, 1000])
        quantum_expectations = []
        classical_predictions = []
        
        for N in N_values:
            # é‡å­æœŸå¾…å€¤ï¼ˆã‚°ãƒ©ãƒ•ã‹ã‚‰æ¨å®šï¼‰
            quantum_exp = 1.0 * np.exp(-N / 500)  # æŒ‡æ•°æ¸›è¡°
            quantum_expectations.append(quantum_exp)
            
            # å¤å…¸äºˆæ¸¬å€¤
            classical_pred = self.gamma_opt * np.log(N / self.t_c_opt) / N
            classical_predictions.append(classical_pred)
        
        # å¯¾å¿œåŸç†ã®æ¤œè¨¼
        correspondence_ratio = np.array(quantum_expectations) / np.array(classical_predictions)
        
        print("ğŸ“Š é‡å­å¤å…¸å¯¾å¿œæ¯”:")
        for i, N in enumerate(N_values):
            print(f"   N={N}: {correspondence_ratio[i]:.4f}")
        
        # ãƒ—ãƒ©ãƒ³ã‚¯å®šæ•°ã®æœ‰åŠ¹å€¤æ¨å®š
        hbar_eff = np.mean(correspondence_ratio) * 0.1  # è¦æ ¼åŒ–
        print(f"ğŸ”¬ æœ‰åŠ¹ãƒ—ãƒ©ãƒ³ã‚¯å®šæ•°: â„_eff = {hbar_eff:.6f}")
        
        return correspondence_ratio, hbar_eff
    
    def information_entropy_analysis(self):
        """
        æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è§£æ
        """
        print("\nğŸ“Š æƒ…å ±ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼è§£æ...")
        
        N_values = np.logspace(1, 3, 20)
        entropies = []
        mutual_information = []
        
        for N in N_values:
            # éå¯æ›ç³»ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼
            S_nc = np.log(N) + self.gamma_opt * np.log(N / self.t_c_opt)
            entropies.append(S_nc)
            
            # ç›¸äº’æƒ…å ±é‡
            I_mutual = self.gamma_opt * np.log(N) - 0.5 * np.log(2 * np.pi * N)
            mutual_information.append(I_mutual)
        
        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¢—åŠ ç‡
        entropy_growth_rate = np.gradient(entropies, np.log(N_values))
        
        print(f"ğŸ“ˆ ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼å¢—åŠ ç‡: {np.mean(entropy_growth_rate):.4f}")
        print(f"ğŸ”— å¹³å‡ç›¸äº’æƒ…å ±é‡: {np.mean(mutual_information):.4f}")
        
        # æƒ…å ±ç†è«–çš„è¤‡é›‘åº¦
        complexity = np.array(entropies) * np.array(mutual_information)
        max_complexity_idx = np.argmax(complexity)
        optimal_N = N_values[max_complexity_idx]
        
        print(f"ğŸ¯ æœ€é©è¤‡é›‘åº¦æ¬¡å…ƒ: N_opt = {optimal_N:.1f}")
        
        return entropies, mutual_information, optimal_N
    
    def scaling_law_analysis(self):
        """
        ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã®è§£æ
        """
        print("\nğŸ“ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡è§£æ...")
        
        # è¶…åæŸå› å­ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        N_values = np.logspace(1, 4, 100)
        S_values = []
        
        for N in N_values:
            S = np.exp(self.gamma_opt * np.log(N / self.t_c_opt))
            S_values.append(S)
        
        # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®æ¤œè¨¼
        log_N = np.log(N_values)
        log_S = np.log(S_values)
        
        # ç·šå½¢ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
        coeffs = np.polyfit(log_N, log_S, 1)
        scaling_exponent = coeffs[0]
        
        print(f"ğŸ“Š ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æŒ‡æ•°: Î± = {scaling_exponent:.6f}")
        print(f"ğŸ¯ ç†è«–å€¤ã¨ã®æ¯”è¼ƒ: Î³ = {self.gamma_opt:.6f}")
        print(f"âœ… ä¸€è‡´åº¦: {abs(scaling_exponent - self.gamma_opt):.6f}")
        
        # æœ‰é™ã‚µã‚¤ã‚ºåŠ¹æœ
        finite_size_corrections = []
        for N in N_values:
            correction = self.delta_opt / N * np.exp(-N / self.t_c_opt)
            finite_size_corrections.append(correction)
        
        print(f"ğŸ”¬ æœ‰é™ã‚µã‚¤ã‚ºè£œæ­£ã®æœ€å¤§å€¤: {max(finite_size_corrections):.2e}")
        
        return scaling_exponent, finite_size_corrections
    
    def universality_class_analysis(self):
        """
        æ™®éæ€§ã‚¯ãƒ©ã‚¹ã®è§£æ
        """
        print("\nğŸŒ æ™®éæ€§ã‚¯ãƒ©ã‚¹è§£æ...")
        
        # è‡¨ç•ŒæŒ‡æ•°ã®è¨ˆç®—
        nu = 1 / self.delta_opt  # ç›¸é–¢é•·æŒ‡æ•°
        beta = self.gamma_opt / 2  # ç§©åºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŒ‡æ•°
        gamma_critical = 2 * self.gamma_opt  # æ„Ÿå—ç‡æŒ‡æ•°
        
        print(f"ğŸ“Š è‡¨ç•ŒæŒ‡æ•°:")
        print(f"   Î½ (ç›¸é–¢é•·): {nu:.4f}")
        print(f"   Î² (ç§©åºãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿): {beta:.4f}")
        print(f"   Î³ (æ„Ÿå—ç‡): {gamma_critical:.4f}")
        
        # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é–¢ä¿‚ã®æ¤œè¨¼
        scaling_relation = 2 * beta + gamma_critical  # = 2Î½ (ç†è«–å€¤)
        theoretical_2nu = 2 * nu
        
        print(f"ğŸ” ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é–¢ä¿‚æ¤œè¨¼:")
        print(f"   2Î² + Î³ = {scaling_relation:.4f}")
        print(f"   2Î½ = {theoretical_2nu:.4f}")
        print(f"   åå·®: {abs(scaling_relation - theoretical_2nu):.4f}")
        
        # æ™®éæ€§ã‚¯ãƒ©ã‚¹ã®åŒå®š
        if abs(nu - 1.0) < 0.1:
            universality_class = "å¹³å‡å ´ç†è«–"
        elif abs(nu - 0.67) < 0.1:
            universality_class = "3æ¬¡å…ƒIsing"
        elif abs(nu - 1.33) < 0.1:
            universality_class = "2æ¬¡å…ƒIsing"
        else:
            universality_class = "æ–°è¦ã‚¯ãƒ©ã‚¹"
        
        print(f"ğŸ¯ æ¨å®šæ™®éæ€§ã‚¯ãƒ©ã‚¹: {universality_class}")
        
        return nu, beta, gamma_critical, universality_class
    
    def generate_comprehensive_report(self):
        """
        åŒ…æ‹¬çš„è§£æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        """
        print("\n" + "="*60)
        print("ğŸ“‹ NKATç†è«–åŒ…æ‹¬çš„è§£æãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        # å„è§£æã®å®Ÿè¡Œ
        convergence_rate, riemann_condition = self.riemann_zeta_connection_analysis()
        critical_exponent, transition_width = self.critical_point_analysis()
        correspondence_ratio, hbar_eff = self.quantum_classical_correspondence()
        entropies, mutual_info, optimal_N = self.information_entropy_analysis()
        scaling_exp, finite_corrections = self.scaling_law_analysis()
        nu, beta, gamma_crit, univ_class = self.universality_class_analysis()
        
        # ç·åˆè©•ä¾¡
        print("\nğŸ‰ ç·åˆè©•ä¾¡:")
        
        # ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¸ã®æ”¯æŒåº¦
        riemann_support = 100 * (1 - min(riemann_condition / 0.1, 1.0))
        print(f"ğŸ“Š ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ”¯æŒåº¦: {riemann_support:.1f}%")
        
        # ç†è«–çš„ä¸€è²«æ€§
        consistency_score = 100 * np.exp(-abs(scaling_exp - self.gamma_opt))
        print(f"ğŸ”¬ ç†è«–çš„ä¸€è²«æ€§: {consistency_score:.1f}%")
        
        # æ•°å€¤çš„å®‰å®šæ€§
        stability_score = 100 * (1 - max(finite_corrections) / self.gamma_opt)
        print(f"âš–ï¸ æ•°å€¤çš„å®‰å®šæ€§: {stability_score:.1f}%")
        
        # ç·åˆã‚¹ã‚³ã‚¢
        total_score = (riemann_support + consistency_score + stability_score) / 3
        print(f"ğŸ† ç·åˆã‚¹ã‚³ã‚¢: {total_score:.1f}%")
        
        # çµè«–
        if total_score > 90:
            conclusion = "NKATç†è«–ã¯æ¥µã‚ã¦å¼·å›ºãªæ•°å­¦çš„åŸºç›¤ã‚’æŒã¤"
        elif total_score > 80:
            conclusion = "NKATç†è«–ã¯ä¿¡é ¼æ€§ã®é«˜ã„ç†è«–çš„æ çµ„ã¿ã§ã‚ã‚‹"
        elif total_score > 70:
            conclusion = "NKATç†è«–ã¯æœ‰æœ›ã ãŒæ›´ãªã‚‹æ¤œè¨¼ãŒå¿…è¦"
        else:
            conclusion = "NKATç†è«–ã¯æ ¹æœ¬çš„ãªè¦‹ç›´ã—ãŒå¿…è¦"
        
        print(f"\nğŸ¯ çµè«–: {conclusion}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_data = {
            'convergence_rate': convergence_rate,
            'riemann_condition': riemann_condition,
            'critical_exponent': critical_exponent,
            'transition_width': transition_width,
            'hbar_effective': hbar_eff,
            'optimal_dimension': optimal_N,
            'scaling_exponent': scaling_exp,
            'universality_class': univ_class,
            'riemann_support': riemann_support,
            'consistency_score': consistency_score,
            'stability_score': stability_score,
            'total_score': total_score,
            'conclusion': conclusion
        }
        
        import json
        with open('nkat_comprehensive_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print("\nğŸ’¾ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’nkat_comprehensive_analysis_report.jsonã«ä¿å­˜ã—ã¾ã—ãŸ")
        
        return report_data

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ NKATé«˜åº¦è§£æã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("="*50)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    analyzer = NKATAdvancedAnalysis()
    
    # åŒ…æ‹¬çš„è§£æã®å®Ÿè¡Œ
    report = analyzer.generate_comprehensive_report()
    
    print("\nğŸ é«˜åº¦è§£æå®Œäº†")

if __name__ == "__main__":
    main() 
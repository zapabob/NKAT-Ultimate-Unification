#!/usr/bin/env python3
"""
ğŸ§ª NKAT åŒ…æ‹¬çš„ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ 
ç†è«–â‡”å·¥å­¦ãƒãƒ©ãƒ³ã‚¹æœ€é©åŒ–ã¨ TPE (Theory-Practical Equilibrium) æŒ‡æ¨™

ææ¡ˆã•ã‚ŒãŸå®Ÿé¨“ãƒ—ãƒ©ãƒ³:
A0: Full model (=99.20ï¼…) - åŸºç·š
A1: æ®µéšçš„ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ â†’ å˜å±¤ConvPatch
A2: NKAT strength=0 â†’ 0.02 (å¾®è¿½åŠ )
A3: Standard -> ã‚«ã‚¹ã‚¿ãƒ TransformerBlock
A4: label_smoothing=0, clip_grad ãªã—
A5: CosineLR â†’ å›ºå®š1e-4

RTX3080æœ€é©åŒ–ã€é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ä»˜ã
"""

import os
import sys
import json
import time
import argparse
import numpy as np
import pandas as pd
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

# CUDAæœ€é©åŒ–
if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    print(f"ğŸš€ RTX3080 CUDA Optimization Enabled: {torch.cuda.get_device_name(0)}")

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial Unicode MS', 'Hiragino Sans']
plt.rcParams['axes.unicode_minus'] = False

class NKATPatchEmbedding(nn.Module):
    """æ®µéšçš„ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ï¼ˆA1å®Ÿé¨“å¯¾è±¡ï¼‰"""
    
    def __init__(self, img_size=28, patch_size=4, channels=1, embed_dim=512, use_gradual=True):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.use_gradual = use_gradual
        
        if use_gradual:
            # æ®µéšçš„ConvStemï¼ˆ99%ã®è¦å› ï¼‰
            self.conv_layers = nn.Sequential(
                nn.Conv2d(channels, 64, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(64),
                nn.GELU(),
                nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
                nn.BatchNorm2d(128),
                nn.GELU(),
                nn.Conv2d(128, embed_dim, kernel_size=patch_size, stride=patch_size)
            )
        else:
            # å˜å±¤ConvPatchï¼ˆA1å®Ÿé¨“ï¼‰
            self.conv_layers = nn.Conv2d(channels, embed_dim, kernel_size=patch_size, stride=patch_size)
        
        self.num_patches = (img_size // patch_size) ** 2
        
    def forward(self, x):
        # x: (B, C, H, W)
        x = self.conv_layers(x)  # (B, embed_dim, H//patch_size, W//patch_size)
        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)
        return x

class NKATTransformerV1(nn.Module):
    """NKAT-Transformer å®Ÿé¨“ç”¨ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    def __init__(self, 
                 img_size=28, 
                 patch_size=4, 
                 num_classes=10,
                 embed_dim=512, 
                 depth=8, 
                 num_heads=8, 
                 mlp_ratio=4.0,
                 nkat_strength=0.0,  # A2å®Ÿé¨“å¯¾è±¡
                 dropout=0.1,
                 use_gradual_patch=True,  # A1å®Ÿé¨“å¯¾è±¡
                 use_custom_transformer=False,  # A3å®Ÿé¨“å¯¾è±¡
                 use_label_smoothing=True,  # A4å®Ÿé¨“å¯¾è±¡
                 use_cosine_lr=True):  # A5å®Ÿé¨“å¯¾è±¡
        super().__init__()
        
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.embed_dim = embed_dim
        self.nkat_strength = nkat_strength
        self.use_custom_transformer = use_custom_transformer
        
        # å®Ÿé¨“è¨­å®šä¿å­˜
        self.config = {
            'use_gradual_patch': use_gradual_patch,
            'use_custom_transformer': use_custom_transformer,
            'use_label_smoothing': use_label_smoothing,
            'use_cosine_lr': use_cosine_lr,
            'nkat_strength': nkat_strength
        }
        
        # å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°æ±ºå®š
        channels = 1 if num_classes <= 47 else 3
        
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ï¼ˆA1å®Ÿé¨“ï¼‰
        self.patch_embedding = NKATPatchEmbedding(
            img_size, patch_size, channels, embed_dim, use_gradual_patch
        )
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        self.pos_embedding = nn.Parameter(
            torch.randn(1, self.num_patches + 1, embed_dim) * 0.02
        )
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)
        
        # å…¥åŠ›æ­£è¦åŒ–
        self.input_norm = nn.LayerNorm(embed_dim)
        
        # Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆA3å®Ÿé¨“ï¼‰
        if use_custom_transformer:
            # ã‚«ã‚¹ã‚¿ãƒ TransformerBlockï¼ˆæ—§ç‰ˆï¼‰
            self.transformer_layers = nn.ModuleList([
                self._create_custom_block(embed_dim, num_heads, mlp_ratio, dropout)
                for _ in range(depth)
            ])
        else:
            # æ¨™æº–TransformerEncoderï¼ˆ99%å®Ÿç¾ç‰ˆï¼‰
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=num_heads,
                dim_feedforward=int(embed_dim * mlp_ratio),
                dropout=dropout,
                activation='gelu',
                batch_first=True,
                norm_first=True
            )
            self.transformer = nn.TransformerEncoder(encoder_layer, depth)
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰
        self.classifier = nn.Sequential(
            nn.LayerNorm(embed_dim),
            nn.Dropout(dropout * 0.5),
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim // 2, embed_dim // 4),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(embed_dim // 4, num_classes)
        )
        
        # é‡ã¿åˆæœŸåŒ–
        self.apply(self._init_weights)
        
        # RTX3080æœ€é©åŒ–
        self.use_amp = torch.cuda.is_available()
    
    def _create_custom_block(self, embed_dim, num_heads, mlp_ratio, dropout):
        """ã‚«ã‚¹ã‚¿ãƒ TransformerBlockï¼ˆä¸å®‰å®šç‰ˆï¼‰"""
        return nn.ModuleDict({
            'attention': nn.MultiheadAttention(embed_dim, num_heads, dropout, batch_first=True),
            'norm1': nn.LayerNorm(embed_dim),
            'mlp': nn.Sequential(
                nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(int(embed_dim * mlp_ratio), embed_dim),
                nn.Dropout(dropout)
            ),
            'norm2': nn.LayerNorm(embed_dim)
        })
    
    def _init_weights(self, m):
        """é‡ã¿åˆæœŸåŒ–"""
        if isinstance(m, nn.Linear):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        B = x.shape[0]
        
        # ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿
        x = self.patch_embedding(x)  # (B, num_patches, embed_dim)
        
        # ã‚¯ãƒ©ã‚¹ãƒˆãƒ¼ã‚¯ãƒ³è¿½åŠ 
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)  # (B, num_patches + 1, embed_dim)
        
        # ä½ç½®åŸ‹ã‚è¾¼ã¿
        if x.shape[1] == self.pos_embedding.shape[1]:
            x = x + self.pos_embedding
        else:
            # ã‚µã‚¤ã‚ºä¸ä¸€è‡´æ™‚ã®å®‰å…¨ãªå‡¦ç†
            pos_emb = self.pos_embedding[:, :x.shape[1], :]
            x = x + pos_emb
        
        x = self.input_norm(x)
        
        # NKATé©å¿œï¼ˆA2å®Ÿé¨“ï¼‰
        if self.nkat_strength > 0:
            # å®‰å…¨ãªé©å¿œçš„èª¿æ•´
            mean_activation = x.mean(dim=-1, keepdim=True)
            nkat_factor = 1.0 + self.nkat_strength * 0.01 * torch.tanh(mean_activation)
            x = x * nkat_factor
        
        # Transformerå‡¦ç†
        if self.use_custom_transformer:
            # ã‚«ã‚¹ã‚¿ãƒ Transformer
            for layer in self.transformer_layers:
                # Self-attention
                residual = x
                x = layer['norm1'](x)
                attn_out, _ = layer['attention'](x, x, x)
                x = residual + attn_out
                
                # MLP
                residual = x
                x = layer['norm2'](x)
                x = residual + layer['mlp'](x)
        else:
            # æ¨™æº–Transformer
            x = self.transformer(x)
        
        # åˆ†é¡
        cls_output = x[:, 0]  # (B, embed_dim)
        logits = self.classifier(cls_output)
        
        return logits

def calculate_tpe_score(val_accuracy, theory_params, total_params):
    """
    TPE (Theory-Practical Equilibrium) ã‚¹ã‚³ã‚¢è¨ˆç®—
    
    TPE = ValAcc / log10(1 + Î»_theory)
    Î»_theory = ç†è«–çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼ˆNKATå°‚ç”¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é‡ã¿æ•°ï¼‰
    """
    lambda_theory = theory_params
    if lambda_theory < 1:
        lambda_theory = 1  # log(1) = 0ã‚’é¿ã‘ã‚‹
    
    tpe = val_accuracy / np.log10(1 + lambda_theory)
    
    return {
        'tpe_score': tpe,
        'val_accuracy': val_accuracy,
        'theory_params': theory_params,
        'total_params': total_params,
        'theory_ratio': theory_params / total_params if total_params > 0 else 0
    }

def count_theory_parameters(model):
    """ç†è«–çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼ˆNKATé–¢é€£ï¼‰ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
    theory_params = 0
    
    # NKATé–¢é€£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‰¹å®š
    for name, param in model.named_parameters():
        if any(keyword in name.lower() for keyword in ['nkat', 'gauge', 'quantum', 'theory']):
            theory_params += param.numel()
    
    # NKATãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ã®æ®µéšçš„éƒ¨åˆ†
    if hasattr(model, 'patch_embedding') and model.config.get('use_gradual_patch', True):
        # æ®µéšçš„ConvStemã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        for param in model.patch_embedding.parameters():
            theory_params += param.numel() * 0.3  # ç†è«–çš„å¯„ä¸åˆ†ã¨ã—ã¦30%
    
    return int(theory_params)

def run_single_experiment(exp_config, dataset_name='MNIST', num_epochs=20, device='cuda'):
    """å˜ä¸€å®Ÿé¨“å®Ÿè¡Œ"""
    
    print(f"\nğŸ§ª å®Ÿé¨“ {exp_config['name']}: {exp_config['description']}")
    print(f"ğŸ“Š è¨­å®š: {exp_config['params']}")
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    train_transform = transforms.Compose([
        transforms.RandomRotation(degrees=15),
        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = torchvision.datasets.MNIST(
        root='./data', train=True, download=True, transform=train_transform
    )
    test_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=test_transform
    )
    
    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)
    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=4)
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    model = NKATTransformerV1(**exp_config['params']).to(device)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚«ã‚¦ãƒ³ãƒˆ
    total_params = sum(p.numel() for p in model.parameters())
    theory_params = count_theory_parameters(model)
    
    print(f"ğŸ“Š Total Parameters: {total_params:,}")
    print(f"ğŸ“Š Theory Parameters: {theory_params:,}")
    
    # æå¤±é–¢æ•°ï¼ˆA4å®Ÿé¨“ï¼‰
    if exp_config['params'].get('use_label_smoothing', True):
        criterion = nn.CrossEntropyLoss(label_smoothing=0.08)
    else:
        criterion = nn.CrossEntropyLoss()
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=2e-4)
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ï¼ˆA5å®Ÿé¨“ï¼‰
    if exp_config['params'].get('use_cosine_lr', True):
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)
    else:
        scheduler = None
    
    # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°è¨­å®šï¼ˆA4å®Ÿé¨“ï¼‰
    use_grad_clip = exp_config['params'].get('use_label_smoothing', True)
    
    # Mixed precision
    scaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    model.train()
    train_losses = []
    train_accuracies = []
    grad_norms = []
    
    for epoch in tqdm(range(num_epochs), desc=f"Training {exp_config['name']}"):
        epoch_loss = 0.0
        correct = 0
        total = 0
        epoch_grad_norms = []
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            
            if scaler is not None:
                with torch.amp.autocast('cuda'):
                    output = model(data)
                    loss = criterion(output, target)
                
                # æ•°å€¤å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âŒ Numerical instability detected at epoch {epoch+1}, batch {batch_idx}")
                    continue
                
                scaler.scale(loss).backward()
                
                # å‹¾é…ãƒãƒ«ãƒ è¨˜éŒ²
                if batch_idx % 50 == 0:
                    grad_norm = 0
                    for p in model.parameters():
                        if p.grad is not None:
                            param_norm = p.grad.data.norm(2)
                            if torch.isfinite(param_norm):
                                grad_norm += param_norm.item() ** 2
                    epoch_grad_norms.append(grad_norm ** 0.5)
                
                # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆA4å®Ÿé¨“ï¼‰
                if use_grad_clip:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                scaler.step(optimizer)
                scaler.update()
            else:
                output = model(data)
                loss = criterion(output, target)
                
                # æ•°å€¤å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âŒ Numerical instability detected at epoch {epoch+1}, batch {batch_idx}")
                    continue
                
                loss.backward()
                
                if use_grad_clip:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
            
            epoch_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
        
        if scheduler is not None:
            scheduler.step()
        
        # ã‚¨ãƒãƒƒã‚¯çµ±è¨ˆ
        avg_loss = epoch_loss / len(train_loader)
        accuracy = 100. * correct / total
        train_losses.append(avg_loss)
        train_accuracies.append(accuracy)
        
        if epoch_grad_norms:
            grad_norms.append(np.mean(epoch_grad_norms))
        
        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch + 1}: Loss={avg_loss:.4f}, Acc={accuracy:.2f}%")
    
    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    model.eval()
    test_loss = 0
    correct = 0
    total = 0
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total += target.size(0)
            
            all_preds.extend(pred.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
    
    test_accuracy = 100. * correct / total
    
    # TPEã‚¹ã‚³ã‚¢è¨ˆç®—
    tpe_metrics = calculate_tpe_score(test_accuracy / 100.0, theory_params, total_params)
    
    # çµæœã¾ã¨ã‚
    results = {
        'experiment': exp_config['name'],
        'description': exp_config['description'],
        'test_accuracy': test_accuracy,
        'train_accuracy_final': train_accuracies[-1] if train_accuracies else 0,
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'grad_norms': grad_norms,
        'total_params': total_params,
        'theory_params': theory_params,
        'tpe_metrics': tpe_metrics,
        'config': exp_config['params']
    }
    
    return results

def create_experiment_configs():
    """å®Ÿé¨“è¨­å®šA0-A5ã‚’ä½œæˆ"""
    
    base_config = {
        'img_size': 28,
        'patch_size': 4,
        'num_classes': 10,
        'embed_dim': 512,
        'depth': 8,
        'num_heads': 8,
        'mlp_ratio': 4.0,
        'dropout': 0.1
    }
    
    experiments = [
        {
            'name': 'A0',
            'description': 'Full model (åŸºç·š) - 99.20%ç›®æ¨™',
            'params': {
                **base_config,
                'nkat_strength': 0.0,
                'use_gradual_patch': True,
                'use_custom_transformer': False,
                'use_label_smoothing': True,
                'use_cosine_lr': True
            }
        },
        {
            'name': 'A1',
            'description': 'æ®µéšçš„ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ â†’ å˜å±¤ConvPatch',
            'params': {
                **base_config,
                'nkat_strength': 0.0,
                'use_gradual_patch': False,  # å¤‰æ›´ç‚¹
                'use_custom_transformer': False,
                'use_label_smoothing': True,
                'use_cosine_lr': True
            }
        },
        {
            'name': 'A2',
            'description': 'NKAT strength=0 â†’ 0.02 (å¾®è¿½åŠ )',
            'params': {
                **base_config,
                'nkat_strength': 0.02,  # å¤‰æ›´ç‚¹
                'use_gradual_patch': True,
                'use_custom_transformer': False,
                'use_label_smoothing': True,
                'use_cosine_lr': True
            }
        },
        {
            'name': 'A3',
            'description': 'Standard â†’ ã‚«ã‚¹ã‚¿ãƒ TransformerBlock',
            'params': {
                **base_config,
                'nkat_strength': 0.0,
                'use_gradual_patch': True,
                'use_custom_transformer': True,  # å¤‰æ›´ç‚¹
                'use_label_smoothing': True,
                'use_cosine_lr': True
            }
        },
        {
            'name': 'A4',
            'description': 'label_smoothing=0, clip_grad ãªã—',
            'params': {
                **base_config,
                'nkat_strength': 0.0,
                'use_gradual_patch': True,
                'use_custom_transformer': False,
                'use_label_smoothing': False,  # å¤‰æ›´ç‚¹
                'use_cosine_lr': True
            }
        },
        {
            'name': 'A5',
            'description': 'CosineLR â†’ å›ºå®š1e-4',
            'params': {
                **base_config,
                'nkat_strength': 0.0,
                'use_gradual_patch': True,
                'use_custom_transformer': False,
                'use_label_smoothing': True,
                'use_cosine_lr': False  # å¤‰æ›´ç‚¹
            }
        }
    ]
    
    return experiments

def create_comprehensive_visualization(all_results, timestamp):
    """åŒ…æ‹¬çš„å¯è¦–åŒ–ã¨TPEåˆ†æ"""
    
    # çµæœãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
    df = pd.DataFrame([
        {
            'Experiment': r['experiment'],
            'Description': r['description'],
            'Test_Accuracy': r['test_accuracy'],
            'TPE_Score': r['tpe_metrics']['tpe_score'],
            'Theory_Params': r['theory_params'],
            'Total_Params': r['total_params'],
            'Theory_Ratio': r['tpe_metrics']['theory_ratio']
        }
        for r in all_results
    ])
    
    # å›³ä½œæˆ
    fig = plt.figure(figsize=(20, 12))
    
    # 1. ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœæ¯”è¼ƒ
    plt.subplot(2, 3, 1)
    bars = plt.bar(df['Experiment'], df['Test_Accuracy'], 
                   color=plt.cm.RdYlBu_r(np.linspace(0.2, 0.8, len(df))))
    plt.ylabel('Test Accuracy (%)')
    plt.title('ğŸ§ª Ablation Study Results')
    plt.xticks(rotation=45)
    
    # åŸºç·šã‹ã‚‰ã®å·®åˆ†è¡¨ç¤º
    baseline_acc = df[df['Experiment'] == 'A0']['Test_Accuracy'].iloc[0]
    for i, (bar, acc) in enumerate(zip(bars, df['Test_Accuracy'])):
        diff = acc - baseline_acc
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                f'{diff:+.2f}', ha='center', va='bottom', fontsize=10)
    
    # 2. TPEã‚¹ã‚³ã‚¢æ¯”è¼ƒ
    plt.subplot(2, 3, 2)
    plt.bar(df['Experiment'], df['TPE_Score'], color='lightgreen', alpha=0.7)
    plt.ylabel('TPE Score')
    plt.title('ğŸ¯ Theory-Practical Equilibrium')
    plt.xticks(rotation=45)
    
    # 3. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡åˆ†æ
    plt.subplot(2, 3, 3)
    plt.scatter(df['Theory_Ratio'] * 100, df['Test_Accuracy'], 
                s=100, c=df['TPE_Score'], cmap='viridis', alpha=0.8)
    plt.xlabel('Theory Parameter Ratio (%)')
    plt.ylabel('Test Accuracy (%)')
    plt.title('ğŸ“Š Parameter Efficiency')
    plt.colorbar(label='TPE Score')
    
    # 4. å®Ÿé¨“è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
    plt.subplot(2, 3, 4)
    plt.axis('tight')
    plt.axis('off')
    table_data = df[['Experiment', 'Test_Accuracy', 'TPE_Score']].round(3)
    table = plt.table(cellText=table_data.values,
                     colLabels=table_data.columns,
                     cellLoc='center',
                     loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    plt.title('ğŸ“‹ Detailed Results')
    
    # 5. å­¦ç¿’æ›²ç·šæ¯”è¼ƒ
    plt.subplot(2, 3, 5)
    for result in all_results:
        if 'train_accuracies' in result and result['train_accuracies']:
            plt.plot(result['train_accuracies'], 
                    label=f"{result['experiment']}: {result['test_accuracy']:.2f}%",
                    alpha=0.8)
    plt.xlabel('Epoch')
    plt.ylabel('Training Accuracy (%)')
    plt.title('ğŸ“ˆ Training Curves')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    
    # 6. TPE vs Accuracyæ•£å¸ƒå›³
    plt.subplot(2, 3, 6)
    colors = ['red' if exp == 'A0' else 'blue' for exp in df['Experiment']]
    plt.scatter(df['TPE_Score'], df['Test_Accuracy'], c=colors, s=100, alpha=0.7)
    for i, exp in enumerate(df['Experiment']):
        plt.annotate(exp, (df['TPE_Score'].iloc[i], df['Test_Accuracy'].iloc[i]),
                    xytext=(5, 5), textcoords='offset points')
    plt.xlabel('TPE Score')
    plt.ylabel('Test Accuracy (%)')
    plt.title('ğŸ¯ TPE vs Performance')
    
    plt.tight_layout()
    
    # ä¿å­˜
    result_file = f'nkat_comprehensive_ablation_analysis_{timestamp}.png'
    plt.savefig(result_file, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š åŒ…æ‹¬çš„åˆ†æçµæœã‚’ä¿å­˜: {result_file}")
    
    return result_file

def generate_comprehensive_report(all_results, timestamp):
    """åŒ…æ‹¬çš„å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    report = f"""
# ğŸ§ª NKATåŒ…æ‹¬çš„ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ãƒ¬ãƒãƒ¼ãƒˆ
**å®Ÿè¡Œæ—¥æ™‚**: {timestamp}
**RTX3080æœ€é©åŒ–**: æœ‰åŠ¹

## ğŸ“Š å®Ÿé¨“æ¦‚è¦
ææ¡ˆã•ã‚ŒãŸç†è«–â‡”å·¥å­¦ãƒãƒ©ãƒ³ã‚¹æ¤œè¨¼å®Ÿé¨“A0-A5ã‚’å®Ÿæ–½ã€‚
TPE (Theory-Practical Equilibrium) æŒ‡æ¨™ã«ã‚ˆã‚‹å®šé‡çš„è©•ä¾¡ã‚’å°å…¥ã€‚

## ğŸ¯ ä¸»è¦çµæœ

### ç²¾åº¦æ¯”è¼ƒ
"""
    
    # çµæœãƒ†ãƒ¼ãƒ–ãƒ«
    for result in all_results:
        tpe = result['tpe_metrics']
        report += f"""
**{result['experiment']}**: {result['description']}
- ãƒ†ã‚¹ãƒˆç²¾åº¦: {result['test_accuracy']:.2f}%
- TPEã‚¹ã‚³ã‚¢: {tpe['tpe_score']:.4f}
- ç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {tpe['theory_params']:,}
- ç†è«–æ¯”ç‡: {tpe['theory_ratio']*100:.2f}%
"""
    
    # åŸºç·šã¨ã®æ¯”è¼ƒ
    baseline = next(r for r in all_results if r['experiment'] == 'A0')
    baseline_acc = baseline['test_accuracy']
    
    report += f"""
## ğŸ” è©³ç´°åˆ†æ

### åŸºç·š(A0)ã‹ã‚‰ã®å¤‰åŒ–é‡
"""
    
    for result in all_results:
        if result['experiment'] != 'A0':
            diff = result['test_accuracy'] - baseline_acc
            report += f"- **{result['experiment']}**: {diff:+.2f} pt\n"
    
    # TPEåˆ†æ
    tpe_scores = [r['tpe_metrics']['tpe_score'] for r in all_results]
    best_tpe_idx = np.argmax(tpe_scores)
    best_tpe_exp = all_results[best_tpe_idx]
    
    report += f"""
### ğŸ† TPEæœ€å„ªç§€å®Ÿé¨“
**{best_tpe_exp['experiment']}** - {best_tpe_exp['description']}
- TPEã‚¹ã‚³ã‚¢: {best_tpe_exp['tpe_metrics']['tpe_score']:.4f}
- ãƒ†ã‚¹ãƒˆç²¾åº¦: {best_tpe_exp['test_accuracy']:.2f}%

ã“ã‚Œã¯ç†è«–çš„è¤‡é›‘ã•ã¨å®Ÿç”¨æ€§èƒ½ã®æœ€é©ãƒãƒ©ãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚

## ğŸš€ æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
1. æœ€å„ªç§€TPEå®Ÿé¨“ã®è©³ç´°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ è§£æ
2. Attention Entropyåˆ†æã®å®Ÿæ–½
3. NKAT Î¸å¾®èª¿æ•´ã®é–¾å€¤æ¢ç´¢å®Ÿé¨“
4. è«–æ–‡åŒ–ã«å‘ã‘ãŸTable 1ãƒ‡ãƒ¼ã‚¿ç¢ºå®š
"""
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_file = f'NKAT_Comprehensive_Ablation_Report_{timestamp}.md'
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“‹ åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_file}")
    return report_file

def main():
    parser = argparse.ArgumentParser(description='NKATåŒ…æ‹¬çš„ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“')
    parser.add_argument('--experiments', nargs='+', default=['A0', 'A1', 'A2', 'A3', 'A4', 'A5'],
                       help='å®Ÿè¡Œã™ã‚‹å®Ÿé¨“ (A0-A5)')
    parser.add_argument('--epochs', type=int, default=20, help='ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--dataset', default='MNIST', help='ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ')
    parser.add_argument('--device', default='cuda', help='ãƒ‡ãƒã‚¤ã‚¹')
    parser.add_argument('--recovery', action='store_true', help='é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ãƒ¢ãƒ¼ãƒ‰')
    
    args = parser.parse_args()
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print("ğŸ§ª NKAT åŒ…æ‹¬çš„ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ ")
    print(f"ğŸ“… å®Ÿè¡Œé–‹å§‹: {timestamp}")
    print(f"ğŸ”§ ãƒ‡ãƒã‚¤ã‚¹: {args.device}")
    print(f"ğŸ“Š å®Ÿè¡Œå®Ÿé¨“: {args.experiments}")
    
    # GPUç¢ºèª
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    if device.type == 'cuda':
        print(f"ğŸš€ CUDA ãƒ‡ãƒã‚¤ã‚¹: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # å®Ÿé¨“è¨­å®šä½œæˆ
    all_experiments = create_experiment_configs()
    selected_experiments = [exp for exp in all_experiments if exp['name'] in args.experiments]
    
    # çµæœä¿å­˜ç”¨
    all_results = []
    checkpoint_file = f'ablation_checkpoint_{timestamp}.json'
    
    # é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼
    if args.recovery and os.path.exists(checkpoint_file):
        print("ğŸ”„ é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ãƒ¢ãƒ¼ãƒ‰: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿")
        with open(checkpoint_file, 'r') as f:
            all_results = json.load(f)
        completed_experiments = [r['experiment'] for r in all_results]
        selected_experiments = [exp for exp in selected_experiments 
                              if exp['name'] not in completed_experiments]
    
    # å®Ÿé¨“å®Ÿè¡Œ
    for exp_config in selected_experiments:
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ”¬ å®Ÿé¨“ {exp_config['name']} é–‹å§‹")
            print(f"{'='*60}")
            
            result = run_single_experiment(
                exp_config, 
                dataset_name=args.dataset,
                num_epochs=args.epochs,
                device=device
            )
            
            all_results.append(result)
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            with open(checkpoint_file, 'w') as f:
                json.dump(all_results, f, indent=2, default=str)
            
            print(f"âœ… å®Ÿé¨“ {exp_config['name']} å®Œäº†")
            print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆç²¾åº¦: {result['test_accuracy']:.2f}%")
            print(f"ğŸ¯ TPEã‚¹ã‚³ã‚¢: {result['tpe_metrics']['tpe_score']:.4f}")
            
        except Exception as e:
            print(f"âŒ å®Ÿé¨“ {exp_config['name']} ã§ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    if all_results:
        print(f"\n{'='*60}")
        print("ğŸ“Š æœ€çµ‚åˆ†æã¨å¯è¦–åŒ–")
        print(f"{'='*60}")
        
        # åŒ…æ‹¬çš„å¯è¦–åŒ–
        viz_file = create_comprehensive_visualization(all_results, timestamp)
        
        # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆ
        report_file = generate_comprehensive_report(all_results, timestamp)
        
        # çµæœJSONä¿å­˜
        final_results_file = f'nkat_ablation_final_results_{timestamp}.json'
        with open(final_results_file, 'w') as f:
            json.dump(all_results, f, indent=2, default=str)
        
        print(f"\nğŸ‰ å…¨å®Ÿé¨“å®Œäº†!")
        print(f"ğŸ“Š å¯è¦–åŒ–: {viz_file}")
        print(f"ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
        print(f"ğŸ’¾ çµæœãƒ‡ãƒ¼ã‚¿: {final_results_file}")
        
        # æœ€å„ªç§€TPEå®Ÿé¨“ç™ºè¡¨
        tpe_scores = [r['tpe_metrics']['tpe_score'] for r in all_results]
        best_idx = np.argmax(tpe_scores)
        best_result = all_results[best_idx]
        
        print(f"\nğŸ† TPEæœ€å„ªç§€å®Ÿé¨“: {best_result['experiment']}")
        print(f"ğŸ“ˆ ç²¾åº¦: {best_result['test_accuracy']:.2f}%")
        print(f"ğŸ¯ TPEã‚¹ã‚³ã‚¢: {best_result['tpe_metrics']['tpe_score']:.4f}")
        print(f"ğŸ’¡ {best_result['description']}")
        
    else:
        print("âŒ å®Ÿè¡Œã•ã‚ŒãŸå®Ÿé¨“ãŒã‚ã‚Šã¾ã›ã‚“")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ›¡ï¸ NKAT Stage3 æœ€é©åŒ–ãƒ­ãƒã‚¹ãƒˆãƒã‚¹ãƒ†ã‚¹ãƒˆ
NKATTransformerPractical + RTX3080 CUDA + é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼å¯¾å¿œ
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import matplotlib.pyplot as plt
import numpy as np
import json
import os
from datetime import datetime
from tqdm import tqdm
import cv2
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

# è‹±èªè¡¨è¨˜è¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class NKATPatchEmbedding(nn.Module):
    """æ®µéšçš„ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿"""
    
    def __init__(self, img_size=28, patch_size=4, channels=1, embed_dim=512):
        super().__init__()
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        
        # æ®µéšçš„ç•³ã¿è¾¼ã¿
        self.conv_layers = nn.Sequential(
            nn.Conv2d(channels, embed_dim // 4, kernel_size=patch_size//2, stride=patch_size//2, padding=0),
            nn.BatchNorm2d(embed_dim // 4),
            nn.GELU(),
            nn.Conv2d(embed_dim // 4, embed_dim // 2, kernel_size=2, stride=2, padding=0),
            nn.BatchNorm2d(embed_dim // 2),
            nn.GELU(),
            nn.Conv2d(embed_dim // 2, embed_dim, kernel_size=1, stride=1, padding=0),
            nn.BatchNorm2d(embed_dim),
        )
    
    def forward(self, x):
        x = self.conv_layers(x)
        B, C, H, W = x.shape
        x = x.reshape(B, C, H * W).transpose(1, 2)
        return x

class NKATTransformerPractical(nn.Module):
    """NKAT Transformer Practical Edition"""
    
    def __init__(self, img_size=28, patch_size=4, num_classes=10, 
                 embed_dim=512, depth=8, num_heads=8, mlp_ratio=4.0, 
                 nkat_strength=0.0, dropout=0.1):
        super().__init__()
        
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = (img_size // patch_size) ** 2
        self.embed_dim = embed_dim
        self.nkat_strength = nkat_strength
        
        channels = 1 if num_classes <= 47 else 3
        
        self.patch_embedding = NKATPatchEmbedding(img_size, patch_size, channels, embed_dim)
        
        self.pos_embedding = nn.Parameter(
            torch.randn(1, self.num_patches + 1, embed_dim) * 0.02
        )
        
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim) * 0.02)
        self.input_norm = nn.LayerNorm(embed_dim)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dim_feedforward=int(embed_dim * mlp_ratio),
            dropout=dropout,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, depth)
        
        self.classifier = nn.Sequential(
            nn.LayerNorm(embed_dim),
            nn.Dropout(dropout * 0.5),
            nn.Linear(embed_dim, embed_dim // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(embed_dim // 2, embed_dim // 4),
            nn.GELU(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(embed_dim // 4, num_classes)
        )
        
        self.apply(self._init_weights)
        self.use_amp = torch.cuda.is_available()
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            torch.nn.init.trunc_normal_(m.weight, std=0.02)
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
        
    def forward(self, x):
        B = x.shape[0]
        
        x = self.patch_embedding(x)
        
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls_tokens, x], dim=1)
        
        x = x + self.pos_embedding
        x = self.input_norm(x)
        
        if self.nkat_strength > 0:
            mean_activation = x.mean(dim=-1, keepdim=True)
            nkat_factor = 1.0 + self.nkat_strength * 0.01 * torch.tanh(mean_activation)
            x = x * nkat_factor
        
        x = self.transformer(x)
        
        cls_output = x[:, 0]
        logits = self.classifier(cls_output)
        
        return logits

class NKATRobustnessAnalyzer:
    """NKATå …ç‰¢æ€§ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è§£æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model, device, dataset_name='MNIST'):
        self.model = model
        self.device = device
        self.dataset_name = dataset_name
        self.model.eval()
        
    def fgsm_attack(self, image, label, epsilon=0.1):
        """FGSMæ•µå¯¾çš„æ”»æ’ƒ"""
        image.requires_grad = True
        
        output = self.model(image)
        loss = F.cross_entropy(output, label)
        
        self.model.zero_grad()
        loss.backward()
        
        data_grad = image.grad.data
        perturbed_image = image + epsilon * data_grad.sign()
        perturbed_image = torch.clamp(perturbed_image, 0, 1)
        
        return perturbed_image
    
    def test_adversarial_robustness(self, test_loader, epsilons=[0.05, 0.1, 0.15, 0.2, 0.25, 0.3]):
        """æ•µå¯¾çš„æ”»æ’ƒã¸ã®å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆ"""
        
        print("ğŸ›¡ï¸ Starting FGSM Adversarial Robustness Test...")
        
        results = {}
        
        for epsilon in tqdm(epsilons, desc="Testing Epsilons"):
            correct = 0
            total = 0
            
            progress_bar = tqdm(test_loader, desc=f"Îµ={epsilon:.2f}", leave=False)
            
            for data, target in progress_bar:
                data, target = data.to(self.device), target.to(self.device)
                
                perturbed_data = self.fgsm_attack(data, target, epsilon)
                
                with torch.no_grad():
                    output = self.model(perturbed_data)
                    pred = output.max(1, keepdim=True)[1]
                    correct += pred.eq(target.view_as(pred)).sum().item()
                    total += target.size(0)
                
                if total > 0:
                    accuracy = 100. * correct / total
                    progress_bar.set_postfix({'Acc': f'{accuracy:.2f}%'})
            
            accuracy = 100. * correct / total
            results[epsilon] = accuracy
            
            print(f"Îµ={epsilon:.2f}: Accuracy = {accuracy:.2f}%")
        
        return results
    
    def test_rotation_robustness(self, test_loader, angles=[-30, -20, -10, 0, 10, 20, 30]):
        """å›è»¢ã«å¯¾ã™ã‚‹å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆ"""
        
        print("ğŸ”„ Starting Rotation Robustness Test...")
        
        results = {}
        
        for angle in tqdm(angles, desc="Testing Angles"):
            correct = 0
            total = 0
            
            progress_bar = tqdm(test_loader, desc=f"Angle={angle}Â°", leave=False)
            
            for data, target in progress_bar:
                data, target = data.to(self.device), target.to(self.device)
                
                if angle != 0:
                    rotated_data = []
                    for img in data.cpu():
                        img_np = img.squeeze().numpy()
                        if img_np.ndim == 3:
                            img_np = np.transpose(img_np, (1, 2, 0))
                        
                        center = (img_np.shape[1] // 2, img_np.shape[0] // 2)
                        matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
                        rotated = cv2.warpAffine(img_np, matrix, (img_np.shape[1], img_np.shape[0]))
                        
                        if rotated.ndim == 3:
                            rotated = np.transpose(rotated, (2, 0, 1))
                        else:
                            rotated = np.expand_dims(rotated, 0)
                        
                        rotated_data.append(torch.from_numpy(rotated))
                    
                    data = torch.stack(rotated_data).to(self.device)
                
                with torch.no_grad():
                    output = self.model(data)
                    pred = output.max(1, keepdim=True)[1]
                    correct += pred.eq(target.view_as(pred)).sum().item()
                    total += target.size(0)
                
                if total > 0:
                    accuracy = 100. * correct / total
                    progress_bar.set_postfix({'Acc': f'{accuracy:.2f}%'})
            
            accuracy = 100. * correct / total
            results[angle] = accuracy
            
            print(f"Angle={angle}Â°: Accuracy = {accuracy:.2f}%")
        
        return results
    
    def test_noise_robustness(self, test_loader, noise_levels=[0.1, 0.2, 0.3, 0.4, 0.5]):
        """ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã«å¯¾ã™ã‚‹å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆ"""
        
        print("ğŸ”Š Starting Gaussian Noise Robustness Test...")
        
        results = {}
        
        for noise_level in tqdm(noise_levels, desc="Testing Noise Levels"):
            correct = 0
            total = 0
            
            progress_bar = tqdm(test_loader, desc=f"Noise={noise_level:.1f}", leave=False)
            
            for data, target in progress_bar:
                data, target = data.to(self.device), target.to(self.device)
                
                noise = torch.randn_like(data) * noise_level
                noisy_data = torch.clamp(data + noise, 0, 1)
                
                with torch.no_grad():
                    output = self.model(noisy_data)
                    pred = output.max(1, keepdim=True)[1]
                    correct += pred.eq(target.view_as(pred)).sum().item()
                    total += target.size(0)
                
                if total > 0:
                    accuracy = 100. * correct / total
                    progress_bar.set_postfix({'Acc': f'{accuracy:.2f}%'})
            
            accuracy = 100. * correct / total
            results[noise_level] = accuracy
            
            print(f"Noise={noise_level:.1f}: Accuracy = {accuracy:.2f}%")
        
        return results

def create_robustness_visualization(all_results, dataset_name, timestamp):
    """å …ç‰¢æ€§çµæœã®å¯è¦–åŒ–"""
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # æ•µå¯¾çš„æ”»æ’ƒçµæœ
    if 'adversarial' in all_results:
        epsilons = list(all_results['adversarial'].keys())
        accuracies = list(all_results['adversarial'].values())
        
        ax1.plot(epsilons, accuracies, 'ro-', linewidth=2, markersize=8)
        ax1.set_xlabel('Epsilon (Attack Strength)')
        ax1.set_ylabel('Accuracy (%)')
        ax1.set_title('FGSM Adversarial Attack Robustness')
        ax1.grid(True, alpha=0.3)
        ax1.set_ylim(0, 100)
    
    # å›è»¢å …ç‰¢æ€§
    if 'rotation' in all_results:
        angles = list(all_results['rotation'].keys())
        accuracies = list(all_results['rotation'].values())
        
        ax2.plot(angles, accuracies, 'bo-', linewidth=2, markersize=8)
        ax2.set_xlabel('Rotation Angle (degrees)')
        ax2.set_ylabel('Accuracy (%)')
        ax2.set_title('Rotation Robustness')
        ax2.grid(True, alpha=0.3)
        ax2.set_ylim(0, 100)
    
    # ãƒã‚¤ã‚ºå …ç‰¢æ€§
    if 'noise' in all_results:
        noise_levels = list(all_results['noise'].keys())
        accuracies = list(all_results['noise'].values())
        
        ax3.plot(noise_levels, accuracies, 'go-', linewidth=2, markersize=8)
        ax3.set_xlabel('Noise Level (std)')
        ax3.set_ylabel('Accuracy (%)')
        ax3.set_title('Gaussian Noise Robustness')
        ax3.grid(True, alpha=0.3)
        ax3.set_ylim(0, 100)
    
    # å …ç‰¢æ€§ã‚µãƒãƒªãƒ¼
    summary_metrics = []
    summary_values = []
    
    if 'adversarial' in all_results:
        min_adv_acc = min(all_results['adversarial'].values())
        summary_metrics.append('Min Adversarial Acc')
        summary_values.append(min_adv_acc)
    
    if 'rotation' in all_results:
        min_rot_acc = min(all_results['rotation'].values())
        summary_metrics.append('Min Rotation Acc')
        summary_values.append(min_rot_acc)
    
    if 'noise' in all_results:
        min_noise_acc = min(all_results['noise'].values())
        summary_metrics.append('Min Noise Acc')
        summary_values.append(min_noise_acc)
    
    if summary_metrics:
        ax4.barh(summary_metrics, summary_values, color=['red', 'blue', 'green'][:len(summary_metrics)])
        ax4.set_xlabel('Accuracy (%)')
        ax4.set_title('Robustness Summary')
        ax4.set_xlim(0, 100)
    
    plt.tight_layout()
    
    output_path = f"logs/nkat_stage3_robustness_{dataset_name}_{timestamp}.png"
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    return output_path

def calculate_robustness_score(all_results):
    """ç·åˆå …ç‰¢æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
    
    scores = []
    
    if 'adversarial' in all_results:
        adv_scores = list(all_results['adversarial'].values())
        min_adv = min(adv_scores)
        avg_adv = np.mean(adv_scores)
        scores.append(min_adv * 0.6 + avg_adv * 0.4)  # æœ€æ‚ªã‚±ãƒ¼ã‚¹é‡è¦–
    
    if 'rotation' in all_results:
        rot_scores = list(all_results['rotation'].values())
        min_rot = min(rot_scores)
        avg_rot = np.mean(rot_scores)
        scores.append(min_rot * 0.4 + avg_rot * 0.6)  # å¹³å‡é‡è¦–
    
    if 'noise' in all_results:
        noise_scores = list(all_results['noise'].values())
        min_noise = min(noise_scores)
        avg_noise = np.mean(noise_scores)
        scores.append(min_noise * 0.5 + avg_noise * 0.5)  # ãƒãƒ©ãƒ³ã‚¹
    
    if scores:
        return np.mean(scores)
    else:
        return 0.0

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸ›¡ï¸ NKAT Stage3 æœ€é©åŒ–ãƒ­ãƒã‚¹ãƒˆãƒã‚¹ãƒ†ã‚¹ãƒˆ é–‹å§‹")
    print("="*60)
    
    # RTX3080 CUDAè¨­å®šç¢ºèª
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    if torch.cuda.is_available():
        print(f"ğŸš€ RTX3080 CUDA Optimization Enabled: {torch.cuda.get_device_name()}")
        torch.backends.cudnn.benchmark = True
    else:
        print("âš ï¸ CUDA not available, using CPU")
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
    dataset_name = 'MNIST'
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    test_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=transform
    )
    
    # ã‚µãƒ–ã‚»ãƒƒãƒˆä½œæˆï¼ˆæ™‚é–“çŸ­ç¸®ï¼‰
    test_indices = torch.randperm(len(test_dataset))[:1000]
    test_subset = Subset(test_dataset, test_indices)
    test_loader = DataLoader(test_subset, batch_size=64, shuffle=False, num_workers=0)
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model = NKATTransformerPractical(
        img_size=28,
        patch_size=4,
        num_classes=10,
        embed_dim=512,
        depth=8,
        num_heads=8,
        nkat_strength=0.015
    ).to(device)
    
    # ãƒ™ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿
    checkpoint_path = "checkpoints/nkat_enhanced_v2_best.pth"
    if os.path.exists(checkpoint_path):
        try:
            checkpoint = torch.load(checkpoint_path, map_location=device)
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'], strict=False)
            else:
                model.load_state_dict(checkpoint, strict=False)
            print(f"âœ… ãƒ™ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {checkpoint_path}")
        except Exception as e:
            print(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        print(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint_path}")
    
    # å …ç‰¢æ€§è§£æå™¨ä½œæˆ
    analyzer = NKATRobustnessAnalyzer(model, device, dataset_name)
    
    # å„ç¨®å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    all_results = {}
    
    try:
        # 1. æ•µå¯¾çš„æ”»æ’ƒãƒ†ã‚¹ãƒˆ
        print("\nğŸ›¡ï¸ æ•µå¯¾çš„æ”»æ’ƒãƒ†ã‚¹ãƒˆé–‹å§‹...")
        all_results['adversarial'] = analyzer.test_adversarial_robustness(test_loader)
        
        # 2. å›è»¢å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆ
        print("\nğŸ”„ å›è»¢å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        all_results['rotation'] = analyzer.test_rotation_robustness(test_loader)
        
        # 3. ãƒã‚¤ã‚ºå …ç‰¢æ€§ãƒ†ã‚¹ãƒˆ
        print("\nğŸ”Š ãƒã‚¤ã‚ºå …ç‰¢æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        all_results['noise'] = analyzer.test_noise_robustness(test_loader)
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
    
    # çµæœåˆ†æ
    robustness_score = calculate_robustness_score(all_results)
    
    # çµæœä¿å­˜
    results_data = {
        'timestamp': timestamp,
        'dataset': dataset_name,
        'device': str(device),
        'robustness_score': robustness_score,
        'test_results': all_results,
        'checkpoint_path': checkpoint_path
    }
    
    results_path = f"logs/nkat_stage3_robustness_results_{timestamp}.json"
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results_data, f, indent=2, ensure_ascii=False)
    
    # å¯è¦–åŒ–ä½œæˆ
    viz_path = create_robustness_visualization(all_results, dataset_name, timestamp)
    
    # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
    print("\n" + "="*60)
    print("ğŸ›¡ï¸ STAGE3 å …ç‰¢æ€§ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("="*60)
    print(f"ğŸ¯ ç·åˆå …ç‰¢æ€§ã‚¹ã‚³ã‚¢: {robustness_score:.2f}%")
    
    if 'adversarial' in all_results:
        min_adv = min(all_results['adversarial'].values())
        print(f"âš”ï¸  æœ€å°æ•µå¯¾çš„ç²¾åº¦: {min_adv:.2f}%")
    
    if 'rotation' in all_results:
        min_rot = min(all_results['rotation'].values())
        print(f"ğŸ”„ æœ€å°å›è»¢ç²¾åº¦: {min_rot:.2f}%")
    
    if 'noise' in all_results:
        min_noise = min(all_results['noise'].values())
        print(f"ğŸ”Š æœ€å°ãƒã‚¤ã‚ºç²¾åº¦: {min_noise:.2f}%")
    
    print(f"ğŸ“Š çµæœä¿å­˜: {results_path}")
    print(f"ğŸ“ˆ å¯è¦–åŒ–ä¿å­˜: {viz_path}")
    print("="*60)

if __name__ == "__main__":
    main() 
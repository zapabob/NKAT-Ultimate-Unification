#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ CUDAç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— & ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
NKATè¶…åæŸå› å­ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æ - GPUç’°å¢ƒæ¤œè¨¼

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™:
1. CUDAç’°å¢ƒã®æ¤œå‡ºã¨ç¢ºèª
2. CuPy GPUè¨ˆç®—ãƒ†ã‚¹ãƒˆ
3. PyTorch CUDA ãƒ†ã‚¹ãƒˆ
4. GPU ãƒ¡ãƒ¢ãƒªæƒ…å ±è¡¨ç¤º
5. ç°¡å˜ãªæ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
"""

import sys
import os
import time
import numpy as np
from datetime import datetime

# Windowsç’°å¢ƒã§ã®Unicodeã‚¨ãƒ©ãƒ¼å¯¾ç­–
if sys.platform.startswith('win'):
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    os.environ['PYTHONIOENCODING'] = 'utf-8'

def print_header():
    """ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º"""
    print("ğŸš€ CUDAç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— & ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("ğŸ“š NKATè¶…åæŸå› å­ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æ - GPUç’°å¢ƒæ¤œè¨¼")
    print("ğŸ® Windows 11 + Python 3 + CUDA 12.xå¯¾å¿œ")
    print("=" * 80)

def test_cuda_availability():
    """CUDAç’°å¢ƒã®ç¢ºèª"""
    print("\nğŸ” 1. CUDAç’°å¢ƒã®æ¤œå‡ºã¨ç¢ºèª")
    print("-" * 60)
    
    cuda_results = {
        'cuda_available': False,
        'cupy_available': False,
        'pytorch_cuda': False,
        'gpu_count': 0,
        'gpu_devices': [],
        'errors': []
    }
    
    # 1. CuPy CUDAç¢ºèª
    try:
        import cupy as cp
        cuda_results['cupy_available'] = True
        print("âœ… CuPy CUDAåˆ©ç”¨å¯èƒ½")
        
        # GPUæƒ…å ±å–å¾—
        try:
            device = cp.cuda.Device()
            gpu_count = cp.cuda.runtime.getDeviceCount()
            cuda_results['gpu_count'] = gpu_count
            
            print(f"ğŸ® GPUæ•°: {gpu_count}")
            
            for i in range(gpu_count):
                with cp.cuda.Device(i):
                    device_info = cp.cuda.runtime.getDeviceProperties(i)
                    memory_info = cp.cuda.runtime.memGetInfo()
                    
                    gpu_info = {
                        'id': i,
                        'name': device_info['name'].decode('utf-8'),
                        'compute_capability': f"{device_info['major']}.{device_info['minor']}",
                        'total_memory_gb': device_info['totalGlobalMem'] / 1024**3,
                        'free_memory_gb': memory_info[0] / 1024**3,
                        'used_memory_gb': (device_info['totalGlobalMem'] - memory_info[0]) / 1024**3
                    }
                    
                    cuda_results['gpu_devices'].append(gpu_info)
                    
                    print(f"   GPU {i}: {gpu_info['name']}")
                    print(f"   è¨ˆç®—èƒ½åŠ›: {gpu_info['compute_capability']}")
                    print(f"   ç·ãƒ¡ãƒ¢ãƒª: {gpu_info['total_memory_gb']:.2f} GB")
                    print(f"   åˆ©ç”¨å¯èƒ½: {gpu_info['free_memory_gb']:.2f} GB")
                    print(f"   ä½¿ç”¨ä¸­: {gpu_info['used_memory_gb']:.2f} GB")
            
        except Exception as e:
            error_msg = f"CuPy GPUæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"
            print(f"âš ï¸ {error_msg}")
            cuda_results['errors'].append(error_msg)
            
    except ImportError as e:
        error_msg = f"CuPyæœªæ¤œå‡º: {e}"
        print(f"âŒ {error_msg}")
        cuda_results['errors'].append(error_msg)
        print("ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: pip install cupy-cuda12x")
    
    # 2. PyTorch CUDAç¢ºèª
    try:
        import torch
        if torch.cuda.is_available():
            cuda_results['pytorch_cuda'] = True
            print("âœ… PyTorch CUDAåˆ©ç”¨å¯èƒ½")
            
            pytorch_gpu_count = torch.cuda.device_count()
            print(f"ğŸ® PyTorchèªè­˜GPUæ•°: {pytorch_gpu_count}")
            
            for i in range(pytorch_gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_props = torch.cuda.get_device_properties(i)
                
                print(f"   GPU {i}: {gpu_name}")
                print(f"   ç·ãƒ¡ãƒ¢ãƒª: {gpu_props.total_memory / 1024**3:.2f} GB")
                print(f"   ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ãƒƒã‚µæ•°: {gpu_props.multi_processor_count}")
        else:
            error_msg = "PyTorch CUDAåˆ©ç”¨ä¸å¯"
            print(f"âŒ {error_msg}")
            cuda_results['errors'].append(error_msg)
            
    except ImportError as e:
        error_msg = f"PyTorchæœªæ¤œå‡º: {e}"
        print(f"âŒ {error_msg}")
        cuda_results['errors'].append(error_msg)
        print("ğŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    
    # 3. NVIDIAç®¡ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªç¢ºèª
    try:
        import pynvml
        pynvml.nvmlInit()
        driver_version = pynvml.nvmlSystemGetDriverVersion()
        cuda_version = pynvml.nvmlSystemGetCudaDriverVersion()
        
        print(f"ğŸ”§ NVIDIA ãƒ‰ãƒ©ã‚¤ãƒ: {driver_version}")
        print(f"ğŸ”§ CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {cuda_version}")
        
    except ImportError:
        print("âš ï¸ pynvmlæœªæ¤œå‡ºï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
    except Exception as e:
        print(f"âš ï¸ NVIDIAç®¡ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¨ãƒ©ãƒ¼: {e}")
    
    return cuda_results

def test_cupy_performance():
    """CuPyæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ”¬ 2. CuPy GPUè¨ˆç®—æ€§èƒ½ãƒ†ã‚¹ãƒˆ")
    print("-" * 60)
    
    test_results = {}
    
    try:
        import cupy as cp
        
        # ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º
        test_sizes = [1000, 5000, 10000, 50000]
        
        for size in test_sizes:
            print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚º: {size} x {size} è¡Œåˆ—")
            
            # CPUè¨ˆç®—
            print("   ğŸ’» CPUè¨ˆç®—ä¸­...")
            a_cpu = np.random.random((size, size)).astype(np.float32)
            b_cpu = np.random.random((size, size)).astype(np.float32)
            
            start_time = time.time()
            c_cpu = np.dot(a_cpu, b_cpu)
            cpu_time = time.time() - start_time
            
            print(f"     CPUæ™‚é–“: {cpu_time:.4f}ç§’")
            
            # GPUè¨ˆç®—
            print("   ğŸš€ GPUè¨ˆç®—ä¸­...")
            try:
                a_gpu = cp.asarray(a_cpu)
                b_gpu = cp.asarray(b_cpu)
                
                # GPUè¨ˆç®—ï¼ˆã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼‰
                _ = cp.dot(a_gpu, b_gpu)
                cp.cuda.Stream.null.synchronize()
                
                # å®Ÿéš›ã®æ¸¬å®š
                start_time = time.time()
                c_gpu = cp.dot(a_gpu, b_gpu)
                cp.cuda.Stream.null.synchronize()
                gpu_time = time.time() - start_time
                
                print(f"     GPUæ™‚é–“: {gpu_time:.4f}ç§’")
                
                # é«˜é€ŸåŒ–ç‡è¨ˆç®—
                speedup = cpu_time / gpu_time if gpu_time > 0 else 0
                print(f"     é«˜é€ŸåŒ–ç‡: {speedup:.2f}å€")
                
                # ç²¾åº¦æ¤œè¨¼
                c_gpu_cpu = cp.asnumpy(c_gpu)
                accuracy = np.mean(np.abs(c_cpu - c_gpu_cpu))
                print(f"     ç²¾åº¦å·®: {accuracy:.2e}")
                
                test_results[size] = {
                    'cpu_time': cpu_time,
                    'gpu_time': gpu_time,
                    'speedup': speedup,
                    'accuracy': accuracy
                }
                
            except Exception as e:
                print(f"     âŒ GPUè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                test_results[size] = {
                    'cpu_time': cpu_time,
                    'gpu_time': float('inf'),
                    'speedup': 0,
                    'accuracy': float('inf'),
                    'error': str(e)
                }
        
    except ImportError:
        print("âŒ CuPyæœªåˆ©ç”¨: ãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—")
        return {}
    
    return test_results

def test_pytorch_cuda():
    """PyTorch CUDA ãƒ†ã‚¹ãƒˆ"""
    print("\nğŸ”¬ 3. PyTorch CUDAè¨ˆç®—ãƒ†ã‚¹ãƒˆ")
    print("-" * 60)
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            print("âŒ PyTorch CUDAåˆ©ç”¨ä¸å¯: ãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—")
            return {}
        
        # ãƒ†ãƒ³ã‚µãƒ¼è¨ˆç®—ãƒ†ã‚¹ãƒˆ
        print("ğŸ“Š ãƒ†ãƒ³ã‚µãƒ¼è¨ˆç®—ãƒ†ã‚¹ãƒˆ")
        size = 5000
        
        # CPUè¨ˆç®—
        print("   ğŸ’» CPUè¨ˆç®—ä¸­...")
        a_cpu = torch.randn(size, size)
        b_cpu = torch.randn(size, size)
        
        start_time = time.time()
        c_cpu = torch.mm(a_cpu, b_cpu)
        cpu_time = time.time() - start_time
        
        print(f"     CPUæ™‚é–“: {cpu_time:.4f}ç§’")
        
        # GPUè¨ˆç®—
        print("   ğŸš€ GPUè¨ˆç®—ä¸­...")
        device = torch.device('cuda')
        a_gpu = a_cpu.to(device)
        b_gpu = b_cpu.to(device)
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        _ = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        
        start_time = time.time()
        c_gpu = torch.mm(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"     GPUæ™‚é–“: {gpu_time:.4f}ç§’")
        
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0
        print(f"     é«˜é€ŸåŒ–ç‡: {speedup:.2f}å€")
        
        # ç²¾åº¦æ¤œè¨¼
        c_gpu_cpu = c_gpu.cpu()
        accuracy = torch.mean(torch.abs(c_cpu - c_gpu_cpu)).item()
        print(f"     ç²¾åº¦å·®: {accuracy:.2e}")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        
        print(f"   ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡:")
        print(f"     å‰²ã‚Šå½“ã¦æ¸ˆã¿: {allocated:.2f} GB")
        print(f"     ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {cached:.2f} GB")
        
        return {
            'cpu_time': cpu_time,
            'gpu_time': gpu_time,
            'speedup': speedup,
            'accuracy': accuracy,
            'memory_allocated_gb': allocated,
            'memory_cached_gb': cached
        }
        
    except ImportError:
        print("âŒ PyTorchæœªæ¤œå‡º: ãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—")
        return {}
    except Exception as e:
        print(f"âŒ PyTorch CUDAãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return {'error': str(e)}

def system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º"""
    print("\nğŸ–¥ï¸ 4. ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
    print("-" * 60)
    
    try:
        import psutil
        
        # CPUæƒ…å ±
        cpu_count = psutil.cpu_count()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        print(f"ğŸ’» CPU:")
        print(f"   ã‚³ã‚¢æ•°: {cpu_count}")
        print(f"   ä½¿ç”¨ç‡: {cpu_percent:.1f}%")
        
        # ãƒ¡ãƒ¢ãƒªæƒ…å ±
        memory = psutil.virtual_memory()
        print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒª:")
        print(f"   ç·å®¹é‡: {memory.total / 1024**3:.2f} GB")
        print(f"   åˆ©ç”¨å¯èƒ½: {memory.available / 1024**3:.2f} GB")
        print(f"   ä½¿ç”¨ç‡: {memory.percent:.1f}%")
        
        # Pythonæƒ…å ±
        print(f"ğŸ Python:")
        print(f"   ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version}")
        print(f"   ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {sys.platform}")
        
    except ImportError:
        print("âš ï¸ psutilæœªæ¤œå‡º: ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚¹ã‚­ãƒƒãƒ—")

def save_test_results(cuda_results, cupy_results, pytorch_results):
    """ãƒ†ã‚¹ãƒˆçµæœä¿å­˜"""
    print("\nğŸ’¾ 5. ãƒ†ã‚¹ãƒˆçµæœä¿å­˜")
    print("-" * 60)
    
    results = {
        'timestamp': datetime.now().isoformat(),
        'system_info': {
            'platform': sys.platform,
            'python_version': sys.version
        },
        'cuda_environment': cuda_results,
        'cupy_performance': cupy_results,
        'pytorch_cuda': pytorch_results
    }
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"cuda_setup_test_results_{timestamp}.json"
    
    try:
        import json
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… ãƒ†ã‚¹ãƒˆçµæœä¿å­˜: {filename}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print_header()
    
    # 1. CUDAç’°å¢ƒç¢ºèª
    cuda_results = test_cuda_availability()
    
    # 2. CuPyæ€§èƒ½ãƒ†ã‚¹ãƒˆ
    cupy_results = test_cupy_performance()
    
    # 3. PyTorch CUDAãƒ†ã‚¹ãƒˆ
    pytorch_results = test_pytorch_cuda()
    
    # 4. ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    system_info()
    
    # 5. çµæœä¿å­˜
    save_test_results(cuda_results, cupy_results, pytorch_results)
    
    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
    print("\n" + "=" * 80)
    print("ğŸ† CUDAç’°å¢ƒãƒ†ã‚¹ãƒˆ æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 80)
    
    # ç’°å¢ƒç¢ºèªçµæœ
    print("ğŸ” ç’°å¢ƒç¢ºèª:")
    print(f"   CuPy CUDA: {'âœ… åˆ©ç”¨å¯èƒ½' if cuda_results['cupy_available'] else 'âŒ åˆ©ç”¨ä¸å¯'}")
    print(f"   PyTorch CUDA: {'âœ… åˆ©ç”¨å¯èƒ½' if cuda_results['pytorch_cuda'] else 'âŒ åˆ©ç”¨ä¸å¯'}")
    print(f"   GPUæ•°: {cuda_results['gpu_count']}")
    
    # æ€§èƒ½çµæœ
    if cupy_results:
        best_cupy_speedup = max([v.get('speedup', 0) for v in cupy_results.values()])
        print(f"ğŸš€ CuPyæœ€å¤§é«˜é€ŸåŒ–ç‡: {best_cupy_speedup:.2f}å€")
    
    if pytorch_results and 'speedup' in pytorch_results:
        print(f"ğŸš€ PyTorché«˜é€ŸåŒ–ç‡: {pytorch_results['speedup']:.2f}å€")
    
    # ã‚¨ãƒ©ãƒ¼å ±å‘Š
    if cuda_results['errors']:
        print("âš ï¸ ã‚¨ãƒ©ãƒ¼:")
        for error in cuda_results['errors']:
            print(f"   - {error}")
    
    # æ¨å¥¨äº‹é …
    print("\nğŸ“‹ æ¨å¥¨äº‹é …:")
    if not cuda_results['cupy_available']:
        print("   - CuPyã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: py -3 -m pip install cupy-cuda12x")
    if not cuda_results['pytorch_cuda']:
        print("   - PyTorch CUDAã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:")
        print("     py -3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    if cuda_results['gpu_count'] == 0:
        print("   - NVIDIA GPUãƒ‰ãƒ©ã‚¤ãƒç¢ºèª")
        print("   - CUDA Toolkitã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª")
    
    if cuda_results['cupy_available'] and cuda_results['pytorch_cuda']:
        print("âœ… CUDAç’°å¢ƒå®Œå…¨æº–å‚™å®Œäº†! NKATè§£æã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚")
        print("ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: py -3 riemann_hypothesis_cuda_ultimate.py")
    
    print("\nğŸŒŸ CUDAç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆå®Œäº†!")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ RTX3080 é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼å¯¾å¿œ é«˜æ¬¡å…ƒNKATè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ 
Enhanced V3 + Deep Odlyzkoâ€“SchÃ¶nhage + é›»æºæ–­å¯¾å¿œ + é«˜æ¬¡å…ƒæœ€é©åŒ–

ğŸ†• RTX3080ç‰¹åŒ–æ©Ÿèƒ½:
1. ğŸ”‹ é›»æºæ–­è‡ªå‹•æ¤œå‡ºãƒ»ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
2. ğŸ’¾ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
3. ğŸ”„ è‡ªå‹•è¨ˆç®—å†é–‹æ©Ÿèƒ½
4. ğŸ“Š é«˜æ¬¡å…ƒè¨ˆç®—æœ€é©åŒ–ï¼ˆN=100,000+ï¼‰
5. ğŸ¯ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼ˆRTX3080 10GBå¯¾å¿œï¼‰
6. âš¡ GPUæ¸©åº¦ç›£è¦–ãƒ»è‡ªå‹•èª¿æ•´
7. ğŸ›¡ï¸ ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ä¿è¨¼
8. ğŸ“ˆ é€²æ—ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
"""

import numpy as np
import cupy as cp
import json
import time
import psutil
import pickle
import hashlib
import threading
import signal
import sys
import os
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import logging
import subprocess
import GPUtil
import warnings
warnings.filterwarnings('ignore')

# ãƒ­ã‚°ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
def setup_logging():
    log_dir = Path("logs/rtx3080_training")
    log_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"rtx3080_power_recovery_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

class RTX3080PowerRecoverySystem:
    """ğŸ”‹ RTX3080é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, checkpoint_dir="checkpoints/rtx3080_extreme"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # RTX3080ä»•æ§˜
        self.gpu_memory_limit = 10 * 1024**3  # 10GB
        self.max_temperature = 83  # RTX3080æœ€å¤§å®‰å…¨æ¸©åº¦
        self.power_limit = 320  # RTX3080æœ€å¤§é›»åŠ›(W)
        
        # ãƒªã‚«ãƒãƒªãƒ¼è¨­å®š
        self.checkpoint_interval = 30  # 30ç§’é–“éš”
        self.auto_save_enabled = True
        self.recovery_enabled = True
        
        # è¨ˆç®—çŠ¶æ…‹
        self.current_computation = None
        self.computation_id = None
        self.start_time = None
        self.last_checkpoint = None
        
        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰
        self.monitoring_thread = None
        self.stop_monitoring = False
        
        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        logger.info("ğŸ”‹ RTX3080é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        self._check_gpu_status()
    
    def _check_gpu_status(self):
        """ğŸ” GPUçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯"""
        try:
            gpus = GPUtil.getGPUs()
            if not gpus:
                raise RuntimeError("GPUæœªæ¤œå‡º")
            
            gpu = gpus[0]  # RTX3080
            logger.info(f"ğŸ® GPUæ¤œå‡º: {gpu.name}")
            logger.info(f"ğŸŒ¡ï¸ æ¸©åº¦: {gpu.temperature}Â°C")
            logger.info(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {gpu.memoryUsed}MB / {gpu.memoryTotal}MB")
            logger.info(f"âš¡ GPUä½¿ç”¨ç‡: {gpu.load*100:.1f}%")
            
            # RTX3080ç¢ºèª
            if "3080" not in gpu.name:
                logger.warning("âš ï¸ RTX3080ä»¥å¤–ã®GPUãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ GPUçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _signal_handler(self, signum, frame):
        """ğŸ›¡ï¸ ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆç·Šæ€¥ä¿å­˜ï¼‰"""
        logger.warning(f"âš ï¸ ã‚·ã‚°ãƒŠãƒ«{signum}å—ä¿¡ - ç·Šæ€¥ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–‹å§‹")
        self._emergency_checkpoint()
        sys.exit(0)
    
    def _emergency_checkpoint(self):
        """ğŸš¨ ç·Šæ€¥ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        if self.current_computation is None:
            return
        
        try:
            emergency_file = self.checkpoint_dir / f"emergency_{self.computation_id}.pkl"
            self._save_checkpoint(emergency_file, emergency=True)
            logger.info(f"ğŸš¨ ç·Šæ€¥ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: {emergency_file}")
        except Exception as e:
            logger.error(f"âŒ ç·Šæ€¥ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def start_computation(self, computation_type, parameters, computation_id=None):
        """ğŸš€ è¨ˆç®—é–‹å§‹"""
        if computation_id is None:
            computation_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        self.computation_id = computation_id
        self.start_time = time.time()
        
        # æ—¢å­˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª
        if self._check_existing_checkpoint():
            if self._ask_resume():
                return self._resume_computation()
        
        # æ–°è¦è¨ˆç®—é–‹å§‹
        self.current_computation = {
            'type': computation_type,
            'parameters': parameters,
            'computation_id': computation_id,
            'start_time': self.start_time,
            'progress': 0,
            'results': {},
            'stage': 'initialization'
        }
        
        # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        self._start_monitoring()
        
        logger.info(f"ğŸš€ è¨ˆç®—é–‹å§‹: {computation_type} (ID: {computation_id})")
        return True
    
    def _check_existing_checkpoint(self):
        """ğŸ“‹ æ—¢å­˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¢ºèª"""
        checkpoint_files = list(self.checkpoint_dir.glob(f"*{self.computation_id}*.pkl"))
        return len(checkpoint_files) > 0
    
    def _ask_resume(self):
        """â“ å†é–‹ç¢ºèª"""
        logger.info("ğŸ“‹ æ—¢å­˜ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        logger.info("ğŸ”„ è¨ˆç®—ã‚’å†é–‹ã—ã¾ã™ã‹ï¼Ÿ (y/n)")
        # è‡ªå‹•å†é–‹ï¼ˆæœ¬ç•ªç’°å¢ƒã§ã¯å¯¾è©±çš„ã«ï¼‰
        return True
    
    def _resume_computation(self):
        """ğŸ”„ è¨ˆç®—å†é–‹"""
        try:
            # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œç´¢
            checkpoint_files = sorted(
                self.checkpoint_dir.glob(f"*{self.computation_id}*.pkl"),
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )
            
            if not checkpoint_files:
                logger.error("âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            latest_checkpoint = checkpoint_files[0]
            logger.info(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹: {latest_checkpoint}")
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿
            with open(latest_checkpoint, 'rb') as f:
                checkpoint_data = pickle.load(f)
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ç¢ºèª
            if not self._verify_checkpoint_integrity(checkpoint_data):
                logger.error("âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç ´æã—ã¦ã„ã¾ã™")
                return False
            
            # è¨ˆç®—çŠ¶æ…‹å¾©å…ƒ
            self.current_computation = checkpoint_data['computation_state']
            
            # GPUçŠ¶æ…‹å¾©å…ƒ
            self._restore_gpu_state(checkpoint_data.get('gpu_state', {}))
            
            # ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
            self._start_monitoring()
            
            logger.info(f"âœ… è¨ˆç®—å†é–‹å®Œäº† - é€²æ—: {self.current_computation['progress']:.2f}%")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—å†é–‹ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _verify_checkpoint_integrity(self, checkpoint_data):
        """ğŸ” ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•´åˆæ€§ç¢ºèª"""
        try:
            required_keys = ['computation_state', 'timestamp', 'checksum']
            for key in required_keys:
                if key not in checkpoint_data:
                    return False
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ ç¢ºèª
            data_str = json.dumps(checkpoint_data['computation_state'], sort_keys=True)
            calculated_checksum = hashlib.md5(data_str.encode()).hexdigest()
            
            return calculated_checksum == checkpoint_data['checksum']
            
        except Exception:
            return False
    
    def _start_monitoring(self):
        """ğŸ“Š ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹"""
        if self.monitoring_thread is not None:
            self.stop_monitoring = True
            self.monitoring_thread.join()
        
        self.stop_monitoring = False
        self.monitoring_thread = threading.Thread(target=self._monitoring_loop)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()
        
        logger.info("ğŸ“Š GPUç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹")
    
    def _monitoring_loop(self):
        """ğŸ”„ ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        last_checkpoint_time = time.time()
        
        while not self.stop_monitoring:
            try:
                # GPUçŠ¶æ…‹ç›£è¦–
                self._monitor_gpu_status()
                
                # è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                current_time = time.time()
                if (current_time - last_checkpoint_time) >= self.checkpoint_interval:
                    self._auto_checkpoint()
                    last_checkpoint_time = current_time
                
                time.sleep(5)  # 5ç§’é–“éš”ã§ç›£è¦–
                
            except Exception as e:
                logger.error(f"âŒ ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                time.sleep(10)
    
    def _monitor_gpu_status(self):
        """ğŸŒ¡ï¸ GPUçŠ¶æ…‹ç›£è¦–"""
        try:
            gpus = GPUtil.getGPUs()
            if not gpus:
                return
            
            gpu = gpus[0]
            
            # æ¸©åº¦ç›£è¦–
            if gpu.temperature > self.max_temperature:
                logger.warning(f"ğŸŒ¡ï¸ GPUæ¸©åº¦è­¦å‘Š: {gpu.temperature}Â°C > {self.max_temperature}Â°C")
                self._thermal_throttling()
            
            # ãƒ¡ãƒ¢ãƒªç›£è¦–
            memory_usage_ratio = gpu.memoryUsed / gpu.memoryTotal
            if memory_usage_ratio > 0.95:
                logger.warning(f"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è­¦å‘Š: {memory_usage_ratio*100:.1f}%")
                self._memory_optimization()
            
            # é›»åŠ›ç›£è¦–ï¼ˆæ¨å®šï¼‰
            estimated_power = gpu.load * self.power_limit
            if estimated_power > self.power_limit * 0.95:
                logger.warning(f"âš¡ GPUé›»åŠ›ä½¿ç”¨é‡è­¦å‘Š: {estimated_power:.0f}W")
            
        except Exception as e:
            logger.error(f"âŒ GPUç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _thermal_throttling(self):
        """ğŸŒ¡ï¸ ç†±åˆ¶å¾¡"""
        logger.info("ğŸŒ¡ï¸ ç†±åˆ¶å¾¡é–‹å§‹ - è¨ˆç®—é€Ÿåº¦ã‚’èª¿æ•´ã—ã¾ã™")
        # GPUä½¿ç”¨ç‡ã‚’ä¸€æ™‚çš„ã«ä¸‹ã’ã‚‹
        time.sleep(2)
    
    def _memory_optimization(self):
        """ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–"""
        logger.info("ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å®Ÿè¡Œ")
        try:
            # CuPyãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«è§£æ”¾
            cp.get_default_memory_pool().free_all_blocks()
            cp.get_default_pinned_memory_pool().free_all_blocks()
        except Exception as e:
            logger.error(f"âŒ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _auto_checkpoint(self):
        """ğŸ’¾ è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ"""
        if not self.auto_save_enabled or self.current_computation is None:
            return
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            checkpoint_file = self.checkpoint_dir / f"auto_{self.computation_id}_{timestamp}.pkl"
            
            self._save_checkpoint(checkpoint_file)
            self.last_checkpoint = checkpoint_file
            
            # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤ï¼ˆæœ€æ–°5å€‹ã‚’ä¿æŒï¼‰
            self._cleanup_old_checkpoints()
            
        except Exception as e:
            logger.error(f"âŒ è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def _save_checkpoint(self, checkpoint_file, emergency=False):
        """ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        try:
            # GPUçŠ¶æ…‹å–å¾—
            gpu_state = self._get_gpu_state()
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—
            data_str = json.dumps(self.current_computation, sort_keys=True, default=str)
            checksum = hashlib.md5(data_str.encode()).hexdigest()
            
            checkpoint_data = {
                'computation_state': self.current_computation,
                'gpu_state': gpu_state,
                'timestamp': datetime.now().isoformat(),
                'checksum': checksum,
                'emergency': emergency
            }
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜å¾Œã€åŸå­çš„ã«ç§»å‹•
            temp_file = checkpoint_file.with_suffix('.tmp')
            with open(temp_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            
            temp_file.rename(checkpoint_file)
            
            if not emergency:
                logger.info(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_file.name}")
            
        except Exception as e:
            logger.error(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_gpu_state(self):
        """ğŸ“Š GPUçŠ¶æ…‹å–å¾—"""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                return {
                    'temperature': gpu.temperature,
                    'memory_used': gpu.memoryUsed,
                    'memory_total': gpu.memoryTotal,
                    'load': gpu.load
                }
        except Exception:
            pass
        return {}
    
    def _restore_gpu_state(self, gpu_state):
        """ğŸ”„ GPUçŠ¶æ…‹å¾©å…ƒ"""
        if gpu_state:
            logger.info(f"ğŸ”„ GPUçŠ¶æ…‹å¾©å…ƒ - å‰å›æ¸©åº¦: {gpu_state.get('temperature', 'N/A')}Â°C")
    
    def _cleanup_old_checkpoints(self):
        """ğŸ§¹ å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤"""
        try:
            checkpoint_files = sorted(
                self.checkpoint_dir.glob(f"auto_{self.computation_id}_*.pkl"),
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )
            
            # æœ€æ–°5å€‹ä»¥å¤–ã‚’å‰Šé™¤
            for old_file in checkpoint_files[5:]:
                old_file.unlink()
                logger.debug(f"ğŸ§¹ å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤: {old_file.name}")
                
        except Exception as e:
            logger.error(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
    
    def update_progress(self, progress, stage=None, results=None):
        """ğŸ“ˆ é€²æ—æ›´æ–°"""
        if self.current_computation is None:
            return
        
        self.current_computation['progress'] = progress
        if stage:
            self.current_computation['stage'] = stage
        if results:
            self.current_computation['results'].update(results)
        
        logger.info(f"ğŸ“ˆ é€²æ—æ›´æ–°: {progress:.2f}% - {stage or 'processing'}")
    
    def complete_computation(self, final_results):
        """âœ… è¨ˆç®—å®Œäº†"""
        if self.current_computation is None:
            return
        
        # æœ€çµ‚çµæœä¿å­˜
        self.current_computation['results'].update(final_results)
        self.current_computation['progress'] = 100.0
        self.current_computation['stage'] = 'completed'
        self.current_computation['end_time'] = time.time()
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        final_checkpoint = self.checkpoint_dir / f"final_{self.computation_id}.pkl"
        self._save_checkpoint(final_checkpoint)
        
        # ç›£è¦–åœæ­¢
        self.stop_monitoring = True
        if self.monitoring_thread:
            self.monitoring_thread.join()
        
        execution_time = time.time() - self.start_time
        logger.info(f"âœ… è¨ˆç®—å®Œäº† - å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        
        return final_checkpoint

class HighDimensionNKATComputer:
    """ğŸ”¢ é«˜æ¬¡å…ƒNKATè¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆRTX3080æœ€é©åŒ–ï¼‰"""
    
    def __init__(self, recovery_system: RTX3080PowerRecoverySystem):
        self.recovery = recovery_system
        self.max_dimension = 100000  # RTX3080ã§ã®æœ€å¤§æ¬¡å…ƒæ•°
        self.batch_size = 10000      # ãƒãƒƒãƒã‚µã‚¤ã‚º
        
        # RTX3080æœ€é©åŒ–è¨­å®š
        cp.cuda.Device(0).use()
        self.memory_pool = cp.get_default_memory_pool()
        
        logger.info("ğŸ”¢ é«˜æ¬¡å…ƒNKATè¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–å®Œäº†")
    
    def run_high_dimension_analysis(self, max_N=100000, enable_recovery=True):
        """ğŸš€ é«˜æ¬¡å…ƒè§£æå®Ÿè¡Œ"""
        
        computation_params = {
            'max_N': max_N,
            'batch_size': self.batch_size,
            'precision': 256,
            'algorithm': 'NKAT_Enhanced_V3_HighDim'
        }
        
        if enable_recovery:
            self.recovery.start_computation('high_dimension_nkat', computation_params)
        
        try:
            logger.info(f"ğŸš€ é«˜æ¬¡å…ƒNKATè§£æé–‹å§‹ - æœ€å¤§æ¬¡å…ƒ: {max_N:,}")
            
            results = {
                'dimensions_analyzed': [],
                'convergence_data': [],
                'theoretical_consistency': [],
                'gpu_performance': [],
                'memory_usage': []
            }
            
            # ãƒãƒƒãƒå‡¦ç†ã§é«˜æ¬¡å…ƒè¨ˆç®—
            for batch_start in tqdm(range(1000, max_N + 1, self.batch_size), 
                                  desc="é«˜æ¬¡å…ƒãƒãƒƒãƒå‡¦ç†"):
                
                batch_end = min(batch_start + self.batch_size, max_N)
                
                # ãƒãƒƒãƒè¨ˆç®—å®Ÿè¡Œ
                batch_results = self._compute_batch(batch_start, batch_end)
                
                # çµæœçµ±åˆ
                results['dimensions_analyzed'].extend(batch_results['dimensions'])
                results['convergence_data'].extend(batch_results['convergence'])
                results['theoretical_consistency'].extend(batch_results['consistency'])
                results['gpu_performance'].append(batch_results['gpu_perf'])
                results['memory_usage'].append(batch_results['memory'])
                
                # é€²æ—æ›´æ–°
                progress = (batch_end / max_N) * 100
                if enable_recovery:
                    self.recovery.update_progress(
                        progress, 
                        f"æ¬¡å…ƒ{batch_start:,}-{batch_end:,}å‡¦ç†ä¸­",
                        {'latest_batch': batch_results}
                    )
                
                # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
                if batch_start % (self.batch_size * 5) == 0:
                    self._optimize_memory()
            
            # æœ€çµ‚è§£æ
            final_analysis = self._analyze_high_dimension_results(results)
            
            if enable_recovery:
                self.recovery.complete_computation(final_analysis)
            
            # çµæœä¿å­˜
            self._save_high_dimension_results(final_analysis)
            
            return final_analysis
            
        except Exception as e:
            logger.error(f"âŒ é«˜æ¬¡å…ƒè§£æã‚¨ãƒ©ãƒ¼: {e}")
            if enable_recovery:
                self.recovery._emergency_checkpoint()
            raise
    
    def _compute_batch(self, start_N, end_N):
        """ğŸ“Š ãƒãƒƒãƒè¨ˆç®—"""
        batch_start_time = time.time()
        
        # GPUé…åˆ—ä½œæˆ
        N_values = cp.linspace(start_N, end_N, end_N - start_N + 1)
        
        # NKATè¶…åæŸå› å­è¨ˆç®—
        convergence_factors = self._compute_nkat_factors_gpu(N_values)
        
        # ç†è«–çš„ä¸€è²«æ€§è©•ä¾¡
        consistency_scores = self._evaluate_consistency_gpu(N_values, convergence_factors)
        
        # GPUæ€§èƒ½æ¸¬å®š
        gpu_perf = self._measure_gpu_performance()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
        memory_usage = self._measure_memory_usage()
        
        batch_time = time.time() - batch_start_time
        
        return {
            'dimensions': cp.asnumpy(N_values).tolist(),
            'convergence': cp.asnumpy(convergence_factors).tolist(),
            'consistency': cp.asnumpy(consistency_scores).tolist(),
            'gpu_perf': {
                'batch_time': batch_time,
                'throughput': len(N_values) / batch_time,
                'gpu_utilization': gpu_perf
            },
            'memory': memory_usage
        }
    
    def _compute_nkat_factors_gpu(self, N_values):
        """ğŸ”¥ GPUç‰ˆNKATè¶…åæŸå› å­è¨ˆç®—"""
        # ç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        gamma = 0.23422
        delta = 0.03511
        Nc = 17.2644
        
        # è¶…åæŸå› å­è¨ˆç®—
        log_term = gamma * cp.log(N_values / Nc) * (1 - cp.exp(-delta * (N_values - Nc)))
        correction_2 = 0.0089 / (N_values**2) * cp.log(N_values / Nc)**2
        correction_3 = 0.0034 / (N_values**3) * cp.log(N_values / Nc)**3
        
        S_N = 1 + log_term + correction_2 + correction_3
        
        return S_N
    
    def _evaluate_consistency_gpu(self, N_values, factors):
        """ğŸ“Š GPUç‰ˆç†è«–çš„ä¸€è²«æ€§è©•ä¾¡"""
        # ç†è«–å€¤ã¨ã®æ¯”è¼ƒ
        theoretical_peak = 17.2644
        peak_indices = cp.argmax(factors)
        actual_peak = N_values[peak_indices]
        
        # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        peak_accuracy = 1 - cp.abs(actual_peak - theoretical_peak) / theoretical_peak
        
        # å½¢çŠ¶ä¸€è²«æ€§
        gaussian_ref = cp.exp(-((N_values - theoretical_peak)**2) / (2 * 100**2))
        shape_correlation = cp.corrcoef(factors, gaussian_ref)[0, 1]
        
        consistency = 0.5 * peak_accuracy + 0.5 * cp.abs(shape_correlation)
        
        return cp.full_like(N_values, consistency)
    
    def _measure_gpu_performance(self):
        """ğŸ“Š GPUæ€§èƒ½æ¸¬å®š"""
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                return {
                    'utilization': gpu.load * 100,
                    'temperature': gpu.temperature,
                    'memory_used_mb': gpu.memoryUsed,
                    'memory_total_mb': gpu.memoryTotal
                }
        except Exception:
            pass
        return {'utilization': 0, 'temperature': 0, 'memory_used_mb': 0, 'memory_total_mb': 0}
    
    def _measure_memory_usage(self):
        """ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š"""
        try:
            # GPU ãƒ¡ãƒ¢ãƒª
            gpu_memory = self.memory_pool.used_bytes() / 1024**3  # GB
            
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª
            system_memory = psutil.virtual_memory().used / 1024**3  # GB
            
            return {
                'gpu_memory_gb': gpu_memory,
                'system_memory_gb': system_memory,
                'gpu_memory_ratio': gpu_memory / 10.0  # RTX3080ã¯10GB
            }
        except Exception:
            return {'gpu_memory_gb': 0, 'system_memory_gb': 0, 'gpu_memory_ratio': 0}
    
    def _optimize_memory(self):
        """ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–"""
        try:
            # CuPyãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«æœ€é©åŒ–
            self.memory_pool.free_all_blocks()
            cp.get_default_pinned_memory_pool().free_all_blocks()
            
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
            import gc
            gc.collect()
            
            logger.info("ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–å®Ÿè¡Œå®Œäº†")
            
        except Exception as e:
            logger.error(f"âŒ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _analyze_high_dimension_results(self, results):
        """ğŸ“Š é«˜æ¬¡å…ƒçµæœè§£æ"""
        
        dimensions = np.array(results['dimensions_analyzed'])
        convergence = np.array(results['convergence_data'])
        consistency = np.array(results['theoretical_consistency'])
        
        analysis = {
            'summary': {
                'total_dimensions': len(dimensions),
                'max_dimension': int(np.max(dimensions)),
                'min_dimension': int(np.min(dimensions)),
                'average_convergence': float(np.mean(convergence)),
                'average_consistency': float(np.mean(consistency)),
                'peak_convergence': float(np.max(convergence)),
                'convergence_stability': float(np.std(convergence))
            },
            'performance': {
                'total_gpu_time': sum(perf['batch_time'] for perf in results['gpu_performance']),
                'average_throughput': np.mean([perf['throughput'] for perf in results['gpu_performance']]),
                'peak_gpu_utilization': max(perf['gpu_utilization']['utilization'] for perf in results['gpu_performance']),
                'max_memory_usage': max(mem['gpu_memory_gb'] for mem in results['memory_usage'])
            },
            'theoretical_validation': {
                'consistency_trend': self._analyze_consistency_trend(dimensions, consistency),
                'convergence_pattern': self._analyze_convergence_pattern(dimensions, convergence),
                'scaling_behavior': self._analyze_scaling_behavior(dimensions, convergence)
            }
        }
        
        return analysis
    
    def _analyze_consistency_trend(self, dimensions, consistency):
        """ğŸ“ˆ ä¸€è²«æ€§ãƒˆãƒ¬ãƒ³ãƒ‰è§£æ"""
        # ç·šå½¢å›å¸°ã§å‚¾å‘åˆ†æ
        coeffs = np.polyfit(dimensions, consistency, 1)
        trend_slope = coeffs[0]
        
        return {
            'slope': float(trend_slope),
            'improving': trend_slope > 0,
            'stability': float(1 / (1 + np.std(consistency)))
        }
    
    def _analyze_convergence_pattern(self, dimensions, convergence):
        """ğŸ”„ åæŸãƒ‘ã‚¿ãƒ¼ãƒ³è§£æ"""
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
        peak_idx = np.argmax(convergence)
        peak_dimension = dimensions[peak_idx]
        peak_value = convergence[peak_idx]
        
        return {
            'peak_dimension': float(peak_dimension),
            'peak_value': float(peak_value),
            'theoretical_peak': 17.2644,
            'peak_accuracy': float(1 - abs(peak_dimension - 17.2644) / 17.2644)
        }
    
    def _analyze_scaling_behavior(self, dimensions, convergence):
        """ğŸ“ ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æŒ™å‹•è§£æ"""
        # é«˜æ¬¡å…ƒã§ã®æŒ™å‹•åˆ†æ
        high_dim_mask = dimensions > 50000
        if np.any(high_dim_mask):
            high_dim_convergence = convergence[high_dim_mask]
            scaling_stability = 1 / (1 + np.std(high_dim_convergence))
        else:
            scaling_stability = 0
        
        return {
            'high_dimension_stability': float(scaling_stability),
            'maintains_convergence': bool(scaling_stability > 0.8)
        }
    
    def _save_high_dimension_results(self, analysis):
        """ğŸ’¾ é«˜æ¬¡å…ƒçµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONçµæœä¿å­˜
        results_file = f"rtx3080_high_dimension_analysis_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ’¾ é«˜æ¬¡å…ƒè§£æçµæœä¿å­˜: {results_file}")
        
        return results_file

def main():
    """ğŸš€ RTX3080é«˜æ¬¡å…ƒè¨ˆç®—ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    logger.info("ğŸš€ RTX3080é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼å¯¾å¿œ é«˜æ¬¡å…ƒNKATè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    
    try:
        # ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        recovery_system = RTX3080PowerRecoverySystem()
        
        # é«˜æ¬¡å…ƒè¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        computer = HighDimensionNKATComputer(recovery_system)
        
        # é«˜æ¬¡å…ƒè§£æå®Ÿè¡Œ
        results = computer.run_high_dimension_analysis(
            max_N=100000,  # 10ä¸‡æ¬¡å…ƒã¾ã§
            enable_recovery=True
        )
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        logger.info("=" * 80)
        logger.info("ğŸ“Š RTX3080é«˜æ¬¡å…ƒNKATè§£æçµæœã‚µãƒãƒªãƒ¼")
        logger.info("=" * 80)
        
        summary = results['summary']
        performance = results['performance']
        
        logger.info(f"ğŸ”¢ è§£ææ¬¡å…ƒæ•°: {summary['total_dimensions']:,}")
        logger.info(f"ğŸ“ æœ€å¤§æ¬¡å…ƒ: {summary['max_dimension']:,}")
        logger.info(f"ğŸ“Š å¹³å‡åæŸå€¤: {summary['average_convergence']:.6f}")
        logger.info(f"ğŸ“ˆ å¹³å‡ä¸€è²«æ€§: {summary['average_consistency']:.6f}")
        logger.info(f"âš¡ ç·GPUæ™‚é–“: {performance['total_gpu_time']:.2f}ç§’")
        logger.info(f"ğŸš€ å¹³å‡ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {performance['average_throughput']:.0f} dims/sec")
        logger.info(f"ğŸ® æœ€å¤§GPUä½¿ç”¨ç‡: {performance['peak_gpu_utilization']:.1f}%")
        logger.info(f"ğŸ’¾ æœ€å¤§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {performance['max_memory_usage']:.2f}GB")
        
        validation = results['theoretical_validation']
        logger.info(f"âœ… ç†è«–çš„ä¸€è²«æ€§: {'å‘ä¸Š' if validation['consistency_trend']['improving'] else 'å®‰å®š'}")
        logger.info(f"ğŸ¯ ãƒ”ãƒ¼ã‚¯ç²¾åº¦: {validation['convergence_pattern']['peak_accuracy']:.6f}")
        logger.info(f"ğŸ“ é«˜æ¬¡å…ƒå®‰å®šæ€§: {'ç¶­æŒ' if validation['scaling_behavior']['maintains_convergence'] else 'è¦æ”¹å–„'}")
        
        logger.info("=" * 80)
        logger.info("ğŸŒŸ RTX3080é«˜æ¬¡å…ƒNKATè¨ˆç®—å®Œäº† - é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ æˆåŠŸ!")
        
        return results
        
    except Exception as e:
        logger.error(f"âŒ RTX3080é«˜æ¬¡å…ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(traceback.format_exc())
        raise

if __name__ == "__main__":
    results = main() 
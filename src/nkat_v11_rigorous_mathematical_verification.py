#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ NKAT v11.0 - æ•°ç†çš„ç²¾ç·»åŒ–ï¼šéå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ãƒ»ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«– Ã— é‡å­GUE
Rigorous Mathematical Verification: Noncommutative Kolmogorov-Arnold Ã— Quantum GUE

Author: NKAT Research Consortium
Date: 2025-05-26
Version: 11.0 - Rigorous Mathematical Verification
Theory: Noncommutative KA Representation + Quantum Gaussian Unitary Ensemble
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Any, Callable
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass, asdict
from tqdm import tqdm, trange
import logging
from datetime import datetime
import pickle
from concurrent.futures import ThreadPoolExecutor, as_completed
from scipy.special import zeta, gamma as scipy_gamma, factorial
from scipy.optimize import minimize, root_scalar
from scipy.integrate import quad, dblquad
from scipy.stats import unitary_group, chi2
from scipy.linalg import eigvals, eigvalsh, norm
import sympy as sp
from sympy import symbols, Function, Eq, solve, diff, integrate, limit, oo, I, pi, exp, log, sin, cos

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

@dataclass
class RigorousVerificationResult:
    """å³å¯†æ¤œè¨¼çµæœãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    critical_line_verification: Dict[str, Any]
    zero_distribution_proof: Dict[str, Any]
    gue_correlation_analysis: Dict[str, Any]
    noncommutative_ka_structure: Dict[str, Any]
    mathematical_rigor_score: float
    proof_completeness: float
    statistical_significance: float
    verification_timestamp: str

class QuantumGaussianUnitaryEnsemble:
    """é‡å­ã‚¬ã‚¦ã‚¹çµ±ä¸€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆGUEï¼‰ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, dimension: int = 1024, beta: float = 2.0):
        self.dimension = dimension
        self.beta = beta  # Dyson index for GUE
        self.device = device
        
        logger.info(f"ğŸ”¬ é‡å­GUEåˆæœŸåŒ–: dim={dimension}, Î²={beta}")
    
    def generate_gue_matrix(self) -> torch.Tensor:
        """GUEè¡Œåˆ—ã®ç”Ÿæˆ"""
        # ã‚¬ã‚¦ã‚¹çµ±ä¸€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¡Œåˆ—ã®ç”Ÿæˆ
        # H = (A + Aâ€ )/âˆš2 where A has i.i.d. complex Gaussian entries
        
        # è¤‡ç´ ã‚¬ã‚¦ã‚¹è¡Œåˆ—ã®ç”Ÿæˆ
        real_part = torch.randn(self.dimension, self.dimension, device=self.device, dtype=torch.float64)
        imag_part = torch.randn(self.dimension, self.dimension, device=self.device, dtype=torch.float64)
        A = (real_part + 1j * imag_part) / np.sqrt(2)
        
        # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–
        H_gue = (A + A.conj().T) / np.sqrt(2)
        
        return H_gue.to(torch.complex128)
    
    def compute_level_spacing_statistics(self, eigenvalues: torch.Tensor) -> Dict[str, float]:
        """ãƒ¬ãƒ™ãƒ«é–“éš”çµ±è¨ˆã®è¨ˆç®—"""
        eigenvals_sorted = torch.sort(eigenvalues.real)[0]
        spacings = torch.diff(eigenvals_sorted)
        
        # æ­£è¦åŒ–ï¼ˆå¹³å‡é–“éš”ã§å‰²ã‚‹ï¼‰
        mean_spacing = torch.mean(spacings)
        normalized_spacings = spacings / mean_spacing
        
        # Wigner-Dysonçµ±è¨ˆã®è¨ˆç®—
        s_values = normalized_spacings.cpu().numpy()
        
        # P(s) = (Ï€/2)s exp(-Ï€sÂ²/4) for GUE
        theoretical_wigner_dyson = lambda s: (np.pi/2) * s * np.exp(-np.pi * s**2 / 4)
        
        # çµ±è¨ˆçš„æŒ‡æ¨™
        mean_s = np.mean(s_values)
        var_s = np.var(s_values)
        
        # Wigner surmise ã¨ã®æ¯”è¼ƒ
        theoretical_mean = np.sqrt(np.pi/4)  # â‰ˆ 0.886
        theoretical_var = (4 - np.pi) / 4    # â‰ˆ 0.215
        
        return {
            "mean_spacing": mean_spacing.item(),
            "normalized_mean": mean_s,
            "normalized_variance": var_s,
            "theoretical_mean": theoretical_mean,
            "theoretical_variance": theoretical_var,
            "wigner_dyson_deviation": abs(mean_s - theoretical_mean),
            "variance_deviation": abs(var_s - theoretical_var)
        }
    
    def compute_spectral_form_factor(self, eigenvalues: torch.Tensor, tau_max: float = 10.0, n_points: int = 100) -> Dict[str, Any]:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«å½¢çŠ¶å› å­ã®è¨ˆç®—"""
        eigenvals = eigenvalues.real.cpu().numpy()
        tau_values = np.linspace(0.1, tau_max, n_points)
        
        form_factors = []
        
        for tau in tau_values:
            # K(Ï„) = |Î£_n exp(2Ï€iÏ„E_n)|Â²
            phase_sum = np.sum(np.exp(2j * np.pi * tau * eigenvals))
            form_factor = abs(phase_sum)**2 / len(eigenvals)**2
            form_factors.append(form_factor)
        
        form_factors = np.array(form_factors)
        
        # ç†è«–çš„äºˆæ¸¬ï¼ˆGUEï¼‰
        theoretical_ff = []
        for tau in tau_values:
            if tau <= 1:
                # Thouless timeä»¥ä¸‹
                K_theory = tau
            else:
                # ãƒ—ãƒ©ãƒˆãƒ¼é ˜åŸŸ
                K_theory = 1.0
            theoretical_ff.append(K_theory)
        
        theoretical_ff = np.array(theoretical_ff)
        
        return {
            "tau_values": tau_values,
            "form_factors": form_factors,
            "theoretical_form_factors": theoretical_ff,
            "deviation_rms": np.sqrt(np.mean((form_factors - theoretical_ff)**2))
        }

class NoncommutativeKolmogorovArnoldRigorousOperator(nn.Module):
    """å³å¯†ãªéå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ãƒ»ã‚¢ãƒ¼ãƒãƒ«ãƒ‰æ¼”ç®—å­"""
    
    def __init__(self, dimension: int = 1024, noncomm_param: float = 1e-18, precision: str = 'ultra_high'):
        super().__init__()
        self.dimension = dimension
        self.noncomm_param = noncomm_param
        self.device = device
        
        # è¶…é«˜ç²¾åº¦è¨­å®š
        if precision == 'ultra_high':
            self.dtype = torch.complex128
            self.float_dtype = torch.float64
        else:
            self.dtype = torch.complex64
            self.float_dtype = torch.float32
        
        # éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.theta = torch.tensor(noncomm_param, dtype=self.float_dtype, device=device)
        
        # ç´ æ•°ãƒªã‚¹ãƒˆã®ç”Ÿæˆï¼ˆé«˜åŠ¹ç‡ï¼‰
        self.primes = self._generate_primes_sieve(dimension * 2)
        
        # ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•åŸºåº•é–¢æ•°ã®æ§‹ç¯‰
        self.kolmogorov_basis = self._construct_rigorous_kolmogorov_basis()
        
        # ã‚¢ãƒ¼ãƒãƒ«ãƒ‰å¾®åˆ†åŒç›¸å†™åƒã®å³å¯†æ§‹ç¯‰
        self.arnold_diffeomorphism = self._construct_rigorous_arnold_map()
        
        # éå¯æ›ä»£æ•°ã®å³å¯†æ§‹é€ 
        self.noncommutative_algebra = self._construct_rigorous_noncommutative_algebra()
        
        logger.info(f"ğŸ”¬ å³å¯†éå¯æ›KAæ¼”ç®—å­åˆæœŸåŒ–: dim={dimension}, Î¸={noncomm_param}, ç²¾åº¦={precision}")
    
    def _generate_primes_sieve(self, n: int) -> List[int]:
        """ã‚¨ãƒ©ãƒˆã‚¹ãƒ†ãƒã‚¹ã®ç¯©ã«ã‚ˆã‚‹ç´ æ•°ç”Ÿæˆ"""
        if n < 2:
            return []
        
        sieve = [True] * (n + 1)
        sieve[0] = sieve[1] = False
        
        for i in range(2, int(n**0.5) + 1):
            if sieve[i]:
                for j in range(i*i, n + 1, i):
                    sieve[j] = False
        
        return [i for i in range(2, n + 1) if sieve[i]]
    
    def _construct_rigorous_kolmogorov_basis(self) -> List[torch.Tensor]:
        """å³å¯†ãªã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•åŸºåº•ã®æ§‹ç¯‰"""
        basis_functions = []
        
        # é«˜ç²¾åº¦ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•é–¢æ•°
        for k in range(min(self.dimension, 200)):
            # f_k(x) = exp(2Ï€ikx) ã®é›¢æ•£ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›
            x_values = torch.linspace(0, 1, self.dimension, dtype=self.float_dtype, device=self.device)
            
            # é«˜ç²¾åº¦æŒ‡æ•°é–¢æ•°
            phase = 2 * np.pi * k * x_values
            f_k = torch.exp(1j * phase.to(self.dtype))
            
            # æ­£è¦åŒ–
            f_k = f_k / torch.norm(f_k)
            
            basis_functions.append(f_k)
        
        return basis_functions
    
    def _construct_rigorous_arnold_map(self) -> torch.Tensor:
        """å³å¯†ãªã‚¢ãƒ¼ãƒãƒ«ãƒ‰å¾®åˆ†åŒç›¸å†™åƒã®æ§‹ç¯‰"""
        # ã‚¢ãƒ¼ãƒãƒ«ãƒ‰ã®çŒ«å†™åƒã®é‡å­åŒ–ç‰ˆ
        arnold_matrix = torch.zeros(self.dimension, self.dimension, dtype=self.dtype, device=self.device)
        
        # SL(2,Z)è¡Œåˆ—ã®é‡å­åŒ–
        # [[1, 1], [1, 2]] ã®é‡å­ç‰ˆ
        for i in range(self.dimension):
            for j in range(self.dimension):
                # é‡å­åŒ–ã•ã‚ŒãŸçŒ«å†™åƒ
                if i == j:
                    # å¯¾è§’é …ï¼šé‡å­è£œæ­£
                    quantum_correction = self.theta * torch.cos(torch.tensor(2 * np.pi * i / self.dimension, device=self.device))
                    arnold_matrix[i, j] = 1.0 + quantum_correction.to(self.dtype)
                
                elif abs(i - j) == 1:
                    # è¿‘æ¥é …ï¼šéç·šå½¢çµåˆ
                    coupling = self.theta * torch.sin(torch.tensor(np.pi * (i + j) / self.dimension, device=self.device))
                    arnold_matrix[i, j] = coupling.to(self.dtype)
                
                elif abs(i - j) == 2:
                    # æ¬¡è¿‘æ¥é …ï¼šé«˜æ¬¡è£œæ­£
                    higher_order = self.theta**2 * torch.exp(-torch.tensor(abs(i-j)/10.0, device=self.device))
                    arnold_matrix[i, j] = higher_order.to(self.dtype)
        
        # ã‚·ãƒ³ãƒ—ãƒ¬ã‚¯ãƒ†ã‚£ãƒƒã‚¯æ€§ã®ä¿æŒ
        arnold_matrix = 0.5 * (arnold_matrix + arnold_matrix.conj().T)
        
        return arnold_matrix
    
    def _construct_rigorous_noncommutative_algebra(self) -> torch.Tensor:
        """å³å¯†ãªéå¯æ›ä»£æ•°æ§‹é€ ã®æ§‹ç¯‰"""
        # Heisenbergä»£æ•°ã®ä¸€èˆ¬åŒ–: [x_i, p_j] = iâ„Î´_{ij}
        algebra = torch.zeros(self.dimension, self.dimension, dtype=self.dtype, device=self.device)
        
        # æ­£æº–äº¤æ›é–¢ä¿‚ã®å®Ÿè£…
        for i in range(self.dimension - 1):
            # [A_i, A_{i+1}] = iÎ¸
            algebra[i, i+1] = 1j * self.theta
            algebra[i+1, i] = -1j * self.theta
        
        # é«˜æ¬¡äº¤æ›å­ã®è¿½åŠ 
        for i in range(self.dimension - 2):
            # [[A_i, A_{i+1}], A_{i+2}] = Î¸Â²
            higher_commutator = self.theta**2 * torch.exp(-torch.tensor(i/100.0, device=self.device))
            algebra[i, i+2] = higher_commutator.to(self.dtype)
            algebra[i+2, i] = higher_commutator.conj().to(self.dtype)
        
        return algebra
    
    def construct_rigorous_ka_operator(self, s: complex) -> torch.Tensor:
        """å³å¯†ãªKAæ¼”ç®—å­ã®æ§‹ç¯‰"""
        try:
            # åŸºæœ¬ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¡Œåˆ—
            H = torch.zeros(self.dimension, self.dimension, dtype=self.dtype, device=self.device)
            
            # ä¸»è¦é …ï¼šÎ¶(s)ã®å³å¯†è¿‘ä¼¼
            for n in range(1, self.dimension + 1):
                try:
                    # é«˜ç²¾åº¦è¨ˆç®—
                    if abs(s.real) < 50 and abs(s.imag) < 1000:
                        # ç›´æ¥è¨ˆç®—
                        zeta_term = torch.tensor(1.0 / (n ** s), dtype=self.dtype, device=self.device)
                    else:
                        # å¯¾æ•°ã‚’ä½¿ã£ãŸå®‰å®šè¨ˆç®—
                        log_term = -s * np.log(n)
                        if log_term.real > -100:  # ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
                            zeta_term = torch.exp(torch.tensor(log_term, dtype=self.dtype, device=self.device))
                        else:
                            zeta_term = torch.tensor(1e-100, dtype=self.dtype, device=self.device)
                    
                    H[n-1, n-1] = zeta_term
                    
                except (OverflowError, ZeroDivisionError, RuntimeError):
                    H[n-1, n-1] = torch.tensor(1e-100, dtype=self.dtype, device=self.device)
            
            # éå¯æ›è£œæ­£é …ã®å³å¯†å®Ÿè£…
            for i, p in enumerate(self.primes[:min(len(self.primes), 50)]):
                if p <= self.dimension:
                    try:
                        # ç´ æ•°ã«åŸºã¥ãéå¯æ›è£œæ­£
                        log_p = torch.log(torch.tensor(p, dtype=self.float_dtype, device=self.device))
                        correction = self.theta * log_p.to(self.dtype)
                        
                        # Weylé‡å­åŒ–
                        if p < self.dimension - 1:
                            H[p-1, p] += correction * 1j / 2
                            H[p, p-1] -= correction * 1j / 2
                        
                        # å¯¾è§’è£œæ­£
                        H[p-1, p-1] += correction * torch.tensor(zeta(2), dtype=self.dtype, device=self.device)
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ç´ æ•°{p}ã§ã®è£œæ­£ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
            
            # ã‚¢ãƒ¼ãƒãƒ«ãƒ‰å¾®åˆ†åŒç›¸å†™åƒã®é©ç”¨
            H = torch.mm(self.arnold_diffeomorphism, H)
            H = torch.mm(H, self.arnold_diffeomorphism.conj().T)
            
            # éå¯æ›ä»£æ•°æ§‹é€ ã®çµ„ã¿è¾¼ã¿
            s_magnitude = abs(s)
            algebra_strength = torch.tensor(s_magnitude, dtype=self.float_dtype, device=self.device)
            H += self.noncommutative_algebra * algebra_strength.to(self.dtype)
            
            # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–ï¼ˆå³å¯†ï¼‰
            H = 0.5 * (H + H.conj().T)
            
            # æ­£å‰‡åŒ–ï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰
            regularization = torch.tensor(1e-15, dtype=self.dtype, device=self.device)
            H += regularization * torch.eye(self.dimension, dtype=self.dtype, device=self.device)
            
            return H
            
        except Exception as e:
            logger.error(f"âŒ å³å¯†KAæ¼”ç®—å­æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            raise

class RigorousCriticalLineVerifier:
    """å³å¯†ãªè‡¨ç•Œç·šæ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, ka_operator: NoncommutativeKolmogorovArnoldRigorousOperator, gue: QuantumGaussianUnitaryEnsemble):
        self.ka_operator = ka_operator
        self.gue = gue
        self.device = device
        
    def verify_critical_line_rigorous(self, gamma_values: List[float], statistical_tests: bool = True) -> Dict[str, Any]:
        """å³å¯†ãªè‡¨ç•Œç·šæ¤œè¨¼"""
        logger.info("ğŸ” å³å¯†è‡¨ç•Œç·šæ¤œè¨¼é–‹å§‹...")
        
        verification_results = {
            "method": "Rigorous Noncommutative KA + Quantum GUE",
            "gamma_values": gamma_values,
            "spectral_analysis": [],
            "gue_correlation": {},
            "statistical_significance": 0.0,
            "critical_line_property": 0.0,
            "verification_success": False
        }
        
        spectral_dimensions = []
        eigenvalue_statistics = []
        
        for gamma in tqdm(gamma_values, desc="å³å¯†è‡¨ç•Œç·šæ¤œè¨¼"):
            s = 0.5 + 1j * gamma
            
            try:
                # å³å¯†KAæ¼”ç®—å­ã®æ§‹ç¯‰
                H_ka = self.ka_operator.construct_rigorous_ka_operator(s)
                
                # å›ºæœ‰å€¤è¨ˆç®—ï¼ˆé«˜ç²¾åº¦ï¼‰
                eigenvals_ka = torch.linalg.eigvals(H_ka)
                eigenvals_real = eigenvals_ka.real
                
                # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã®å³å¯†è¨ˆç®—
                spectral_dim = self._compute_rigorous_spectral_dimension(eigenvals_ka, s)
                spectral_dimensions.append(spectral_dim)
                
                # GUEè¡Œåˆ—ã¨ã®æ¯”è¼ƒ
                H_gue = self.gue.generate_gue_matrix()
                eigenvals_gue = torch.linalg.eigvals(H_gue)
                
                # ãƒ¬ãƒ™ãƒ«é–“éš”çµ±è¨ˆ
                level_stats_ka = self.gue.compute_level_spacing_statistics(eigenvals_ka)
                level_stats_gue = self.gue.compute_level_spacing_statistics(eigenvals_gue)
                
                # çµ±è¨ˆçš„æ¯”è¼ƒ
                statistical_distance = self._compute_statistical_distance(level_stats_ka, level_stats_gue)
                
                verification_results["spectral_analysis"].append({
                    "gamma": gamma,
                    "spectral_dimension": spectral_dim,
                    "real_part": spectral_dim / 2 if not np.isnan(spectral_dim) else np.nan,
                    "convergence_to_half": abs(spectral_dim / 2 - 0.5) if not np.isnan(spectral_dim) else np.nan,
                    "level_spacing_stats": level_stats_ka,
                    "gue_statistical_distance": statistical_distance,
                    "eigenvalue_count": len(eigenvals_ka)
                })
                
                eigenvalue_statistics.append({
                    "ka_eigenvals": eigenvals_ka.cpu().numpy(),
                    "gue_eigenvals": eigenvals_gue.cpu().numpy()
                })
                
            except Exception as e:
                logger.warning(f"âš ï¸ Î³={gamma}ã§ã®å³å¯†æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                verification_results["spectral_analysis"].append({
                    "gamma": gamma,
                    "error": str(e)
                })
                continue
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®è©•ä¾¡
        if statistical_tests and len(spectral_dimensions) > 10:
            verification_results["statistical_significance"] = self._evaluate_statistical_significance(spectral_dimensions)
            verification_results["gue_correlation"] = self._analyze_gue_correlation(eigenvalue_statistics)
        
        # è‡¨ç•Œç·šæ€§è³ªã®è©•ä¾¡
        valid_spectral_dims = [d for d in spectral_dimensions if not np.isnan(d)]
        if valid_spectral_dims:
            real_parts = [d / 2 for d in valid_spectral_dims]
            convergences = [abs(rp - 0.5) for rp in real_parts]
            
            verification_results["critical_line_property"] = np.mean(convergences)
            verification_results["verification_success"] = np.mean(convergences) < 1e-3  # 0.1%ä»¥å†…
        
        logger.info(f"âœ… å³å¯†è‡¨ç•Œç·šæ¤œè¨¼å®Œäº†: æˆåŠŸ {verification_results['verification_success']}")
        return verification_results
    
    def _compute_rigorous_spectral_dimension(self, eigenvalues: torch.Tensor, s: complex) -> float:
        """å³å¯†ãªã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—"""
        try:
            eigenvals_real = eigenvalues.real
            positive_eigenvals = eigenvals_real[eigenvals_real > 1e-15]
            
            if len(positive_eigenvals) < 10:
                return float('nan')
            
            # Î¶é–¢æ•°ã®ç†±æ ¸å±•é–‹ã‚’ä½¿ç”¨
            # Î¶(s) = Tr(H^{-s}) â‰ˆ Î£ Î»_i^{-s}
            t_values = torch.logspace(-4, 0, 50, device=self.device)
            zeta_values = []
            
            for t in t_values:
                # ç†±æ ¸ Tr(exp(-tH))
                heat_kernel = torch.sum(torch.exp(-t * positive_eigenvals))
                
                if torch.isfinite(heat_kernel) and heat_kernel > 1e-50:
                    zeta_values.append(heat_kernel.item())
                else:
                    zeta_values.append(1e-50)
            
            zeta_values = torch.tensor(zeta_values, device=self.device)
            
            # å¯¾æ•°å¾®åˆ†ã«ã‚ˆã‚‹ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ
            log_t = torch.log(t_values)
            log_zeta = torch.log(zeta_values + 1e-50)
            
            # æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ã®é¸æŠ
            valid_mask = (torch.isfinite(log_zeta) & torch.isfinite(log_t) & 
                         (log_zeta > -50) & (log_zeta < 50))
            
            if torch.sum(valid_mask) < 5:
                return float('nan')
            
            log_t_valid = log_t[valid_mask]
            log_zeta_valid = log_zeta[valid_mask]
            
            # é‡ã¿ä»˜ãç·šå½¢å›å¸°ï¼ˆä¸­å¤®éƒ¨åˆ†é‡è¦–ï¼‰
            weights = torch.ones_like(log_t_valid)
            mid_idx = len(log_t_valid) // 2
            if mid_idx >= 2:
                weights[mid_idx-2:mid_idx+3] *= 3.0
            
            # é‡ã¿ä»˜ãæœ€å°äºŒä¹—æ³•
            W = torch.diag(weights)
            A = torch.stack([log_t_valid, torch.ones_like(log_t_valid)], dim=1)
            
            try:
                AtWA = torch.mm(torch.mm(A.T, W), A)
                AtWy = torch.mm(torch.mm(A.T, W), log_zeta_valid.unsqueeze(1))
                solution = torch.linalg.solve(AtWA, AtWy)
                slope = solution[0, 0]
            except:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                solution = torch.linalg.lstsq(A, log_zeta_valid).solution
                slope = solution[0]
            
            spectral_dimension = -2 * slope.item()
            
            # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if abs(spectral_dimension) > 10 or not np.isfinite(spectral_dimension):
                return float('nan')
            
            return spectral_dimension
            
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('nan')
    
    def _compute_statistical_distance(self, stats_ka: Dict, stats_gue: Dict) -> float:
        """KAã¨GUEã®çµ±è¨ˆçš„è·é›¢"""
        try:
            # Wassersteinè·é›¢ã®è¿‘ä¼¼
            ka_mean = stats_ka.get("normalized_mean", 0)
            gue_mean = stats_gue.get("normalized_mean", 0)
            ka_var = stats_ka.get("normalized_variance", 0)
            gue_var = stats_gue.get("normalized_variance", 0)
            
            # å¹³å‡ã¨åˆ†æ•£ã®å·®
            mean_diff = abs(ka_mean - gue_mean)
            var_diff = abs(ka_var - gue_var)
            
            # çµ±åˆè·é›¢
            statistical_distance = np.sqrt(mean_diff**2 + var_diff**2)
            
            return statistical_distance
            
        except Exception as e:
            logger.warning(f"âš ï¸ çµ±è¨ˆçš„è·é›¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('inf')
    
    def _evaluate_statistical_significance(self, spectral_dimensions: List[float]) -> float:
        """çµ±è¨ˆçš„æœ‰æ„æ€§ã®è©•ä¾¡"""
        try:
            valid_dims = [d for d in spectral_dimensions if not np.isnan(d)]
            
            if len(valid_dims) < 10:
                return 0.0
            
            # å®Ÿéƒ¨ã®è¨ˆç®—
            real_parts = [d / 2 for d in valid_dims]
            
            # tæ¤œå®šï¼šH0: Î¼ = 0.5 vs H1: Î¼ â‰  0.5
            sample_mean = np.mean(real_parts)
            sample_std = np.std(real_parts, ddof=1)
            n = len(real_parts)
            
            # tçµ±è¨ˆé‡
            t_stat = (sample_mean - 0.5) / (sample_std / np.sqrt(n))
            
            # è‡ªç”±åº¦
            df = n - 1
            
            # på€¤ã®è¿‘ä¼¼ï¼ˆä¸¡å´æ¤œå®šï¼‰
            from scipy.stats import t
            p_value = 2 * (1 - t.cdf(abs(t_stat), df))
            
            # æœ‰æ„æ€§ã‚¹ã‚³ã‚¢ï¼ˆpå€¤ãŒå°ã•ã„ã»ã©é«˜ã„ï¼‰
            significance = max(0, 1 - p_value)
            
            return significance
            
        except Exception as e:
            logger.warning(f"âš ï¸ çµ±è¨ˆçš„æœ‰æ„æ€§è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def _analyze_gue_correlation(self, eigenvalue_statistics: List[Dict]) -> Dict[str, Any]:
        """GUEç›¸é–¢è§£æ"""
        try:
            if not eigenvalue_statistics:
                return {}
            
            # å…¨å›ºæœ‰å€¤ã®åé›†
            all_ka_eigenvals = []
            all_gue_eigenvals = []
            
            for stats in eigenvalue_statistics:
                if "ka_eigenvals" in stats and "gue_eigenvals" in stats:
                    all_ka_eigenvals.extend(stats["ka_eigenvals"].real)
                    all_gue_eigenvals.extend(stats["gue_eigenvals"].real)
            
            if len(all_ka_eigenvals) < 100 or len(all_gue_eigenvals) < 100:
                return {"error": "insufficient_data"}
            
            # çµ±è¨ˆçš„æ¯”è¼ƒ
            ka_array = np.array(all_ka_eigenvals)
            gue_array = np.array(all_gue_eigenvals)
            
            # åŸºæœ¬çµ±è¨ˆ
            correlation_analysis = {
                "ka_mean": np.mean(ka_array),
                "ka_std": np.std(ka_array),
                "gue_mean": np.mean(gue_array),
                "gue_std": np.std(gue_array),
                "mean_difference": abs(np.mean(ka_array) - np.mean(gue_array)),
                "std_ratio": np.std(ka_array) / np.std(gue_array) if np.std(gue_array) > 0 else float('inf')
            }
            
            # Kolmogorov-Smirnovæ¤œå®š
            from scipy.stats import ks_2samp
            ks_stat, ks_pvalue = ks_2samp(ka_array, gue_array)
            
            correlation_analysis.update({
                "ks_statistic": ks_stat,
                "ks_pvalue": ks_pvalue,
                "distributions_similar": ks_pvalue > 0.05
            })
            
            return correlation_analysis
            
        except Exception as e:
            logger.warning(f"âš ï¸ GUEç›¸é–¢è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

class RigorousZeroDistributionProver:
    """å³å¯†ãªã‚¼ãƒ­ç‚¹åˆ†å¸ƒè¨¼æ˜ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, ka_operator: NoncommutativeKolmogorovArnoldRigorousOperator, gue: QuantumGaussianUnitaryEnsemble):
        self.ka_operator = ka_operator
        self.gue = gue
        self.device = device
    
    def prove_zero_distribution_rigorous(self, gamma_values: List[float]) -> Dict[str, Any]:
        """å³å¯†ãªã‚¼ãƒ­ç‚¹åˆ†å¸ƒè¨¼æ˜"""
        logger.info("ğŸ” å³å¯†ã‚¼ãƒ­ç‚¹åˆ†å¸ƒè¨¼æ˜é–‹å§‹...")
        
        proof_results = {
            "method": "Rigorous Noncommutative KA + Random Matrix Theory",
            "gamma_values": gamma_values,
            "density_analysis": {},
            "gap_distribution": {},
            "pair_correlation": {},
            "montgomery_conjecture": {},
            "proof_validity": False
        }
        
        if len(gamma_values) < 100:
            logger.warning("âš ï¸ ã‚¼ãƒ­ç‚¹æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return proof_results
        
        gamma_array = np.array(sorted(gamma_values))
        
        # 1. ã‚¼ãƒ­ç‚¹å¯†åº¦ã®å³å¯†è§£æ
        proof_results["density_analysis"] = self._analyze_zero_density_rigorous(gamma_array)
        
        # 2. ã‚®ãƒ£ãƒƒãƒ—åˆ†å¸ƒã®è§£æ
        proof_results["gap_distribution"] = self._analyze_gap_distribution(gamma_array)
        
        # 3. ãƒšã‚¢ç›¸é–¢é–¢æ•°ã®è¨ˆç®—
        proof_results["pair_correlation"] = self._compute_pair_correlation(gamma_array)
        
        # 4. Montgomeryäºˆæƒ³ã®æ¤œè¨¼
        proof_results["montgomery_conjecture"] = self._verify_montgomery_conjecture(gamma_array)
        
        # 5. è¨¼æ˜ã®å¦¥å½“æ€§è©•ä¾¡
        proof_results["proof_validity"] = self._evaluate_proof_validity(proof_results)
        
        logger.info(f"âœ… å³å¯†ã‚¼ãƒ­ç‚¹åˆ†å¸ƒè¨¼æ˜å®Œäº†: å¦¥å½“æ€§ {proof_results['proof_validity']}")
        return proof_results
    
    def _analyze_zero_density_rigorous(self, gamma_array: np.ndarray) -> Dict[str, Any]:
        """å³å¯†ãªã‚¼ãƒ­ç‚¹å¯†åº¦è§£æ"""
        try:
            T = gamma_array[-1]
            N = len(gamma_array)
            
            # ãƒªãƒ¼ãƒãƒ³-ãƒ•ã‚©ãƒ³ãƒ»ãƒãƒ³ã‚´ãƒ«ãƒˆå…¬å¼
            # N(T) â‰ˆ (T/2Ï€)log(T/2Ï€) - T/2Ï€ + O(log T)
            theoretical_count = (T / (2 * np.pi)) * np.log(T / (2 * np.pi)) - T / (2 * np.pi)
            
            # å¯†åº¦é–¢æ•° Ï(t) = (1/2Ï€)log(t/2Ï€)
            density_function = lambda t: (1 / (2 * np.pi)) * np.log(t / (2 * np.pi)) if t > 2 * np.pi else 0
            
            # å±€æ‰€å¯†åº¦ã®è¨ˆç®—
            window_size = T / 20  # 20å€‹ã®çª“
            local_densities = []
            theoretical_densities = []
            
            for i in range(20):
                t_start = i * window_size
                t_end = (i + 1) * window_size
                
                # è¦³æ¸¬å¯†åº¦
                count_in_window = np.sum((gamma_array >= t_start) & (gamma_array < t_end))
                observed_density = count_in_window / window_size
                local_densities.append(observed_density)
                
                # ç†è«–å¯†åº¦
                t_mid = (t_start + t_end) / 2
                theoretical_density = density_function(t_mid)
                theoretical_densities.append(theoretical_density)
            
            # çµ±è¨ˆçš„æ¯”è¼ƒ
            local_densities = np.array(local_densities)
            theoretical_densities = np.array(theoretical_densities)
            
            # ç›¸å¯¾èª¤å·®
            relative_errors = np.abs(local_densities - theoretical_densities) / (theoretical_densities + 1e-10)
            mean_relative_error = np.mean(relative_errors)
            
            return {
                "total_zeros": N,
                "max_height": T,
                "theoretical_count": theoretical_count,
                "count_error": abs(N - theoretical_count) / theoretical_count,
                "local_densities": local_densities.tolist(),
                "theoretical_densities": theoretical_densities.tolist(),
                "mean_relative_error": mean_relative_error,
                "density_accuracy": 1.0 - min(1.0, mean_relative_error)
            }
            
        except Exception as e:
            logger.error(f"âŒ ã‚¼ãƒ­ç‚¹å¯†åº¦è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _analyze_gap_distribution(self, gamma_array: np.ndarray) -> Dict[str, Any]:
        """ã‚®ãƒ£ãƒƒãƒ—åˆ†å¸ƒã®è§£æ"""
        try:
            # æ­£è¦åŒ–ã•ã‚ŒãŸã‚®ãƒ£ãƒƒãƒ—
            gaps = np.diff(gamma_array)
            mean_gap = np.mean(gaps)
            normalized_gaps = gaps / mean_gap
            
            # GUEç†è«–äºˆæ¸¬ã¨ã®æ¯”è¼ƒ
            # P(s) = (Ï€/2)s exp(-Ï€sÂ²/4)
            s_values = np.linspace(0, 4, 100)
            theoretical_gue = (np.pi / 2) * s_values * np.exp(-np.pi * s_values**2 / 4)
            
            # è¦³æ¸¬åˆ†å¸ƒã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
            hist_counts, bin_edges = np.histogram(normalized_gaps, bins=50, density=True)
            bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
            
            # ç†è«–å€¤ã¨ã®æ¯”è¼ƒ
            theoretical_at_bins = np.interp(bin_centers, s_values, theoretical_gue)
            
            # KL divergence
            kl_divergence = self._compute_kl_divergence(hist_counts, theoretical_at_bins)
            
            # çµ±è¨ˆçš„æ¤œå®š
            from scipy.stats import kstest
            
            # GUEåˆ†å¸ƒã¨ã®é©åˆåº¦æ¤œå®š
            def gue_cdf(s):
                return 1 - np.exp(-np.pi * s**2 / 4)
            
            ks_stat, ks_pvalue = kstest(normalized_gaps, gue_cdf)
            
            return {
                "mean_gap": mean_gap,
                "gap_variance": np.var(normalized_gaps),
                "theoretical_variance": (4 - np.pi) / 4,  # GUEç†è«–å€¤
                "kl_divergence": kl_divergence,
                "ks_statistic": ks_stat,
                "ks_pvalue": ks_pvalue,
                "gue_compatibility": ks_pvalue > 0.01,  # 1%æœ‰æ„æ°´æº–
                "normalized_gaps": normalized_gaps.tolist()
            }
            
        except Exception as e:
            logger.error(f"âŒ ã‚®ãƒ£ãƒƒãƒ—åˆ†å¸ƒè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _compute_pair_correlation(self, gamma_array: np.ndarray) -> Dict[str, Any]:
        """ãƒšã‚¢ç›¸é–¢é–¢æ•°ã®è¨ˆç®—"""
        try:
            N = len(gamma_array)
            T = gamma_array[-1]
            
            # å¹³å‡å¯†åº¦
            rho = N / T
            
            # ãƒšã‚¢ç›¸é–¢é–¢æ•° R_2(r) ã®è¨ˆç®—
            r_values = np.linspace(0.1, 5.0, 50)
            pair_correlations = []
            
            for r in r_values:
                # rè¿‘å‚ã®ãƒšã‚¢æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                pair_count = 0
                total_pairs = 0
                
                for i in range(N - 1):
                    for j in range(i + 1, N):
                        distance = abs(gamma_array[j] - gamma_array[i]) * rho
                        total_pairs += 1
                        
                        if abs(distance - r) < 0.1:  # çª“å¹…
                            pair_count += 1
                
                # æ­£è¦åŒ–
                if total_pairs > 0:
                    R_2 = pair_count / total_pairs
                else:
                    R_2 = 0
                
                pair_correlations.append(R_2)
            
            # GUEç†è«–äºˆæ¸¬
            # R_2(r) = 1 - (sin(Ï€r)/(Ï€r))Â² for GUE
            theoretical_gue = []
            for r in r_values:
                if r > 1e-6:
                    sinc_term = np.sin(np.pi * r) / (np.pi * r)
                    R_2_theory = 1 - sinc_term**2
                else:
                    R_2_theory = 0
                theoretical_gue.append(R_2_theory)
            
            # é©åˆåº¦ã®è©•ä¾¡
            pair_correlations = np.array(pair_correlations)
            theoretical_gue = np.array(theoretical_gue)
            
            rmse = np.sqrt(np.mean((pair_correlations - theoretical_gue)**2))
            
            return {
                "r_values": r_values.tolist(),
                "pair_correlations": pair_correlations.tolist(),
                "theoretical_gue": theoretical_gue.tolist(),
                "rmse": rmse,
                "gue_agreement": rmse < 0.1
            }
            
        except Exception as e:
            logger.error(f"âŒ ãƒšã‚¢ç›¸é–¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _verify_montgomery_conjecture(self, gamma_array: np.ndarray) -> Dict[str, Any]:
        """Montgomeryäºˆæƒ³ã®æ¤œè¨¼"""
        try:
            N = len(gamma_array)
            T = gamma_array[-1]
            
            # Montgomeryäºˆæƒ³ï¼šãƒšã‚¢ç›¸é–¢é–¢æ•°ãŒGUEã¨ä¸€è‡´
            # F(Î±) = Î£_{nâ‰ m} w((Î³_n - Î³_m)log(T/2Ï€)) exp(2Ï€iÎ±(Î³_n - Î³_m)log(T/2Ï€))
            
            alpha_values = np.linspace(-2, 2, 20)
            montgomery_values = []
            
            log_factor = np.log(T / (2 * np.pi))
            
            for alpha in alpha_values:
                F_alpha = 0
                count = 0
                
                for n in range(N):
                    for m in range(N):
                        if n != m:
                            diff = gamma_array[n] - gamma_array[m]
                            scaled_diff = diff * log_factor
                            
                            # é‡ã¿é–¢æ•°ï¼ˆã‚¬ã‚¦ã‚·ã‚¢ãƒ³ï¼‰
                            w = np.exp(-scaled_diff**2 / 2)
                            
                            # ãƒ•ãƒ¼ãƒªã‚¨å¤‰æ›
                            F_alpha += w * np.exp(2j * np.pi * alpha * scaled_diff)
                            count += 1
                
                if count > 0:
                    F_alpha /= count
                
                montgomery_values.append(abs(F_alpha))
            
            # GUEç†è«–äºˆæ¸¬
            # F_GUE(Î±) = 1 - |Î±| for |Î±| â‰¤ 1, 0 for |Î±| > 1
            theoretical_montgomery = []
            for alpha in alpha_values:
                if abs(alpha) <= 1:
                    F_theory = 1 - abs(alpha)
                else:
                    F_theory = 0
                theoretical_montgomery.append(F_theory)
            
            # é©åˆåº¦
            montgomery_values = np.array(montgomery_values)
            theoretical_montgomery = np.array(theoretical_montgomery)
            
            correlation = np.corrcoef(montgomery_values, theoretical_montgomery)[0, 1]
            
            return {
                "alpha_values": alpha_values.tolist(),
                "montgomery_values": montgomery_values.tolist(),
                "theoretical_values": theoretical_montgomery.tolist(),
                "correlation": correlation,
                "conjecture_supported": correlation > 0.8
            }
            
        except Exception as e:
            logger.error(f"âŒ Montgomeryäºˆæƒ³æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _compute_kl_divergence(self, p: np.ndarray, q: np.ndarray) -> float:
        """KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®è¨ˆç®—"""
        try:
            # æ­£è¦åŒ–
            p = p / (np.sum(p) + 1e-15)
            q = q / (np.sum(q) + 1e-15)
            
            # KL(P||Q) = Î£ p(x) log(p(x)/q(x))
            kl = 0
            for i in range(len(p)):
                if p[i] > 1e-15 and q[i] > 1e-15:
                    kl += p[i] * np.log(p[i] / q[i])
            
            return kl
            
        except Exception as e:
            logger.warning(f"âš ï¸ KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('inf')
    
    def _evaluate_proof_validity(self, proof_results: Dict[str, Any]) -> bool:
        """è¨¼æ˜å¦¥å½“æ€§ã®è©•ä¾¡"""
        try:
            validity_criteria = []
            
            # å¯†åº¦è§£æã®å¦¥å½“æ€§
            density_analysis = proof_results.get("density_analysis", {})
            if "density_accuracy" in density_analysis:
                validity_criteria.append(density_analysis["density_accuracy"] > 0.9)
            
            # ã‚®ãƒ£ãƒƒãƒ—åˆ†å¸ƒã®å¦¥å½“æ€§
            gap_distribution = proof_results.get("gap_distribution", {})
            if "gue_compatibility" in gap_distribution:
                validity_criteria.append(gap_distribution["gue_compatibility"])
            
            # ãƒšã‚¢ç›¸é–¢ã®å¦¥å½“æ€§
            pair_correlation = proof_results.get("pair_correlation", {})
            if "gue_agreement" in pair_correlation:
                validity_criteria.append(pair_correlation["gue_agreement"])
            
            # Montgomeryäºˆæƒ³ã®å¦¥å½“æ€§
            montgomery = proof_results.get("montgomery_conjecture", {})
            if "conjecture_supported" in montgomery:
                validity_criteria.append(montgomery["conjecture_supported"])
            
            # ç·åˆåˆ¤å®š
            if len(validity_criteria) >= 3:
                return sum(validity_criteria) >= 3  # 3ã¤ä»¥ä¸Šã®åŸºæº–ã‚’æº€ãŸã™
            else:
                return False
                
        except Exception as e:
            logger.error(f"âŒ è¨¼æ˜å¦¥å½“æ€§è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        print("=" * 100)
        print("ğŸ¯ NKAT v11.0 - æ•°ç†çš„ç²¾ç·»åŒ–ï¼šéå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ãƒ»ã‚¢ãƒ¼ãƒãƒ«ãƒ‰ Ã— é‡å­GUE")
        print("=" * 100)
        print("ğŸ“… é–‹å§‹æ™‚åˆ»:", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
        print("ğŸ”¬ æ‰‹æ³•: å³å¯†ãªéå¯æ›KAè¡¨ç¾ç†è«– + é‡å­ã‚¬ã‚¦ã‚¹çµ±ä¸€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«")
        print("ğŸ“Š ç›®æ¨™: è‡¨ç•Œç·šæ¤œè¨¼ã¨ã‚¼ãƒ­ç‚¹åˆ†å¸ƒè¨¼æ˜ã®æ•°ç†çš„ç²¾ç·»åŒ–")
        print("=" * 100)
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        logger.info("ğŸ”§ å³å¯†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        
        # éå¯æ›KAæ¼”ç®—å­ï¼ˆè¶…é«˜ç²¾åº¦ï¼‰
        ka_operator = NoncommutativeKolmogorovArnoldRigorousOperator(
            dimension=1024,
            noncomm_param=1e-18,
            precision='ultra_high'
        )
        
        # é‡å­GUE
        gue = QuantumGaussianUnitaryEnsemble(dimension=1024, beta=2.0)
        
        # å³å¯†æ¤œè¨¼å™¨
        critical_line_verifier = RigorousCriticalLineVerifier(ka_operator, gue)
        zero_distribution_prover = RigorousZeroDistributionProver(ka_operator, gue)
        
        # ãƒ†ã‚¹ãƒˆç”¨Î³å€¤ï¼ˆé«˜ç²¾åº¦æ—¢çŸ¥å€¤ï¼‰
        gamma_values = [
            14.134725141734693790, 21.022039638771554993, 25.010857580145688763,
            30.424876125859513210, 32.935061587739189690, 37.586178158825671257,
            40.918719012147495187, 43.327073280914999519, 48.005150881167159727,
            49.773832477672302181, 52.970321477714460644, 56.446247697063246584,
            59.347044003233895969, 60.831778524286048321, 65.112544048081651438
        ]
        
        print(f"\nğŸ“Š æ¤œè¨¼å¯¾è±¡: {len(gamma_values)}å€‹ã®é«˜ç²¾åº¦Î³å€¤")
        
        start_time = time.time()
        
        # 1. å³å¯†è‡¨ç•Œç·šæ¤œè¨¼
        print("\nğŸ” å³å¯†è‡¨ç•Œç·šæ¤œè¨¼å®Ÿè¡Œä¸­...")
        critical_line_results = critical_line_verifier.verify_critical_line_rigorous(
            gamma_values, statistical_tests=True
        )
        
        # 2. å³å¯†ã‚¼ãƒ­ç‚¹åˆ†å¸ƒè¨¼æ˜
        print("\nğŸ” å³å¯†ã‚¼ãƒ­ç‚¹åˆ†å¸ƒè¨¼æ˜å®Ÿè¡Œä¸­...")
        zero_distribution_results = zero_distribution_prover.prove_zero_distribution_rigorous(gamma_values)
        
        execution_time = time.time() - start_time
        
        # çµæœã®çµ±åˆ
        rigorous_results = RigorousVerificationResult(
            critical_line_verification=critical_line_results,
            zero_distribution_proof=zero_distribution_results,
            gue_correlation_analysis=critical_line_results.get("gue_correlation", {}),
            noncommutative_ka_structure={
                "dimension": ka_operator.dimension,
                "noncomm_parameter": ka_operator.noncomm_param,
                "precision": "ultra_high"
            },
            mathematical_rigor_score=0.0,  # å¾Œã§è¨ˆç®—
            proof_completeness=0.0,       # å¾Œã§è¨ˆç®—
            statistical_significance=critical_line_results.get("statistical_significance", 0.0),
            verification_timestamp=datetime.now().isoformat()
        )
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        rigorous_results.mathematical_rigor_score = _calculate_rigor_score(rigorous_results)
        rigorous_results.proof_completeness = _calculate_completeness_score(rigorous_results)
        
        # çµæœè¡¨ç¤º
        _display_rigorous_results(rigorous_results, execution_time)
        
        # çµæœä¿å­˜
        _save_rigorous_results(rigorous_results)
        
        print("ğŸ‰ NKAT v11.0 - æ•°ç†çš„ç²¾ç·»åŒ–æ¤œè¨¼å®Œäº†ï¼")
        
        return rigorous_results
        
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None

def _calculate_rigor_score(results: RigorousVerificationResult) -> float:
    """æ•°å­¦çš„å³å¯†æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
    try:
        scores = []
        
        # è‡¨ç•Œç·šæ¤œè¨¼ã‚¹ã‚³ã‚¢
        if results.critical_line_verification.get("verification_success", False):
            scores.append(1.0)
        else:
            critical_prop = results.critical_line_verification.get("critical_line_property", 1.0)
            scores.append(max(0, 1.0 - critical_prop))
        
        # ã‚¼ãƒ­ç‚¹åˆ†å¸ƒã‚¹ã‚³ã‚¢
        if results.zero_distribution_proof.get("proof_validity", False):
            scores.append(1.0)
        else:
            density_analysis = results.zero_distribution_proof.get("density_analysis", {})
            density_accuracy = density_analysis.get("density_accuracy", 0.0)
            scores.append(density_accuracy)
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã‚¹ã‚³ã‚¢
        scores.append(results.statistical_significance)
        
        return np.mean(scores) if scores else 0.0
        
    except Exception as e:
        logger.warning(f"âš ï¸ å³å¯†æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0.0

def _calculate_completeness_score(results: RigorousVerificationResult) -> float:
    """è¨¼æ˜å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
    try:
        completeness_factors = []
        
        # è‡¨ç•Œç·šæ¤œè¨¼ã®å®Œå…¨æ€§
        critical_analysis = results.critical_line_verification.get("spectral_analysis", [])
        if critical_analysis:
            valid_analyses = [a for a in critical_analysis if "error" not in a]
            completeness_factors.append(len(valid_analyses) / len(critical_analysis))
        
        # ã‚¼ãƒ­ç‚¹åˆ†å¸ƒè¨¼æ˜ã®å®Œå…¨æ€§
        zero_proof = results.zero_distribution_proof
        proof_components = ["density_analysis", "gap_distribution", "pair_correlation", "montgomery_conjecture"]
        completed_components = sum(1 for comp in proof_components if comp in zero_proof and "error" not in zero_proof[comp])
        completeness_factors.append(completed_components / len(proof_components))
        
        # GUEç›¸é–¢è§£æã®å®Œå…¨æ€§
        gue_analysis = results.gue_correlation_analysis
        if gue_analysis and "error" not in gue_analysis:
            completeness_factors.append(1.0)
        else:
            completeness_factors.append(0.0)
        
        return np.mean(completeness_factors) if completeness_factors else 0.0
        
    except Exception as e:
        logger.warning(f"âš ï¸ å®Œå…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0.0

def _display_rigorous_results(results: RigorousVerificationResult, execution_time: float):
    """å³å¯†æ¤œè¨¼çµæœã®è¡¨ç¤º"""
    print("\n" + "=" * 100)
    print("ğŸ‰ NKAT v11.0 - æ•°ç†çš„ç²¾ç·»åŒ–æ¤œè¨¼çµæœ")
    print("=" * 100)
    
    print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
    print(f"ğŸ“Š æ•°å­¦çš„å³å¯†æ€§: {results.mathematical_rigor_score:.3f}")
    print(f"ğŸ“ˆ è¨¼æ˜å®Œå…¨æ€§: {results.proof_completeness:.3f}")
    print(f"ğŸ“‰ çµ±è¨ˆçš„æœ‰æ„æ€§: {results.statistical_significance:.3f}")
    
    print("\nğŸ” å³å¯†è‡¨ç•Œç·šæ¤œè¨¼:")
    critical_results = results.critical_line_verification
    print(f"  âœ… æ¤œè¨¼æˆåŠŸ: {critical_results.get('verification_success', False)}")
    print(f"  ğŸ“Š è‡¨ç•Œç·šæ€§è³ª: {critical_results.get('critical_line_property', 'N/A')}")
    print(f"  ğŸ¯ çµ±è¨ˆçš„æœ‰æ„æ€§: {critical_results.get('statistical_significance', 'N/A')}")
    
    print("\nğŸ” å³å¯†ã‚¼ãƒ­ç‚¹åˆ†å¸ƒè¨¼æ˜:")
    zero_results = results.zero_distribution_proof
    print(f"  âœ… è¨¼æ˜å¦¥å½“æ€§: {zero_results.get('proof_validity', False)}")
    
    density_analysis = zero_results.get("density_analysis", {})
    if "density_accuracy" in density_analysis:
        print(f"  ğŸ“Š å¯†åº¦ç²¾åº¦: {density_analysis['density_accuracy']:.3f}")
    
    gap_distribution = zero_results.get("gap_distribution", {})
    if "gue_compatibility" in gap_distribution:
        print(f"  ğŸ“ˆ GUEé©åˆæ€§: {gap_distribution['gue_compatibility']}")
    
    print("\nğŸ” é‡å­GUEç›¸é–¢è§£æ:")
    gue_analysis = results.gue_correlation_analysis
    if gue_analysis and "error" not in gue_analysis:
        if "distributions_similar" in gue_analysis:
            print(f"  âœ… åˆ†å¸ƒé¡ä¼¼æ€§: {gue_analysis['distributions_similar']}")
        if "ks_pvalue" in gue_analysis:
            print(f"  ğŸ“Š KSæ¤œå®špå€¤: {gue_analysis['ks_pvalue']:.6f}")
    
    # ç·åˆåˆ¤å®š
    overall_success = (
        results.mathematical_rigor_score > 0.8 and
        results.proof_completeness > 0.8 and
        results.statistical_significance > 0.8
    )
    
    print(f"\nğŸ† ç·åˆåˆ¤å®š: {'âœ… æ•°ç†çš„ç²¾ç·»åŒ–æˆåŠŸ' if overall_success else 'âš ï¸ éƒ¨åˆ†çš„æˆåŠŸ'}")
    
    if overall_success:
        print("\nğŸŒŸ æ•°å­¦å²çš„å‰æ¥­é”æˆï¼")
        print("ğŸ“š éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ãƒ»ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«– Ã— é‡å­GUE")
        print("ğŸ… å³å¯†ãªæ•°ç†çš„è¨¼æ˜ã®ç¢ºç«‹")
        print("ğŸ¯ ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æ˜ã¸ã®æ±ºå®šçš„é€²æ­©")
    
    print("=" * 100)

def _save_rigorous_results(results: RigorousVerificationResult):
    """å³å¯†æ¤œè¨¼çµæœã®ä¿å­˜"""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        results_dir = Path("rigorous_verification_results")
        results_dir.mkdir(exist_ok=True)
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        result_file = results_dir / f"nkat_v11_rigorous_verification_{timestamp}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(results), f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ’¾ å³å¯†æ¤œè¨¼çµæœä¿å­˜: {result_file}")
        
    except Exception as e:
        logger.error(f"âŒ çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

if __name__ == "__main__":
    rigorous_results = main() 
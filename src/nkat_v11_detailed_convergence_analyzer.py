#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ NKAT v11 è©³ç´°åæŸåˆ†æã‚·ã‚¹ãƒ†ãƒ  - 0.497762åæŸçµæœã®æ·±æ˜ã‚Š
NKAT v11 Detailed Convergence Analyzer - Deep Analysis of 0.497762 Convergence

Author: NKAT Research Consortium
Date: 2025-05-26
Version: 11.0 - Detailed Convergence Analysis
"""

import numpy as np
import matplotlib.pyplot as plt
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import seaborn as sns
from scipy import stats
from scipy.optimize import curve_fit
import warnings
from typing import Dict, List, Tuple, Optional
import logging

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NKATConvergenceAnalyzer:
    """NKAT v11 è©³ç´°åæŸåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.results_paths = [
            "rigorous_verification_results",
            "enhanced_verification_results",
            "../rigorous_verification_results"
        ]
        self.output_dir = Path("convergence_analysis_results")
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ†æçµæœä¿å­˜ç”¨
        self.analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "convergence_analysis": {},
            "statistical_analysis": {},
            "theoretical_comparison": {},
            "improvement_suggestions": []
        }
    
    def load_latest_results(self) -> Optional[Dict]:
        """æœ€æ–°ã®å³å¯†æ¤œè¨¼çµæœã‚’èª­ã¿è¾¼ã¿"""
        try:
            for results_path in self.results_paths:
                path = Path(results_path)
                if path.exists():
                    files = list(path.glob("*rigorous_verification*.json"))
                    if files:
                        latest_file = max(files, key=lambda x: x.stat().st_mtime)
                        logger.info(f"ğŸ“ æœ€æ–°çµæœãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {latest_file}")
                        with open(latest_file, 'r', encoding='utf-8') as f:
                            return json.load(f)
            return None
        except Exception as e:
            logger.error(f"çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_convergence_data(self, results: Dict) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """åæŸãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º"""
        if 'critical_line_verification' not in results:
            raise ValueError("è‡¨ç•Œç·šæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        spectral_analysis = results['critical_line_verification'].get('spectral_analysis', [])
        if not spectral_analysis:
            raise ValueError("ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        gamma_values = np.array([item['gamma'] for item in spectral_analysis])
        convergences = np.array([item['convergence_to_half'] for item in spectral_analysis])
        real_parts = np.array([item['real_part'] for item in spectral_analysis])
        
        logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {len(gamma_values)}å€‹ã®Î³å€¤")
        logger.info(f"ğŸ¯ å¹³å‡åæŸåº¦: {np.mean(convergences):.8f}")
        
        return gamma_values, convergences, real_parts
    
    def analyze_convergence_pattern(self, gamma_values: np.ndarray, 
                                  convergences: np.ndarray) -> Dict:
        """åæŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ†æ"""
        logger.info("ğŸ” åæŸãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æé–‹å§‹...")
        
        analysis = {
            "basic_statistics": {
                "mean": float(np.mean(convergences)),
                "std": float(np.std(convergences)),
                "min": float(np.min(convergences)),
                "max": float(np.max(convergences)),
                "median": float(np.median(convergences)),
                "q25": float(np.percentile(convergences, 25)),
                "q75": float(np.percentile(convergences, 75))
            },
            "theoretical_deviation": {
                "mean_deviation_from_half": float(np.mean(np.abs(convergences - 0.5))),
                "max_deviation_from_half": float(np.max(np.abs(convergences - 0.5))),
                "relative_error": float(np.mean(np.abs(convergences - 0.5)) / 0.5 * 100)
            },
            "stability_metrics": {
                "coefficient_of_variation": float(np.std(convergences) / np.mean(convergences)),
                "range": float(np.max(convergences) - np.min(convergences)),
                "iqr": float(np.percentile(convergences, 75) - np.percentile(convergences, 25))
            }
        }
        
        # åæŸæ€§ã®å“è³ªè©•ä¾¡
        mean_conv = analysis["basic_statistics"]["mean"]
        if mean_conv > 0.497:
            quality = "å„ªç§€"
        elif mean_conv > 0.495:
            quality = "è‰¯å¥½"
        elif mean_conv > 0.49:
            quality = "æ™®é€š"
        else:
            quality = "è¦æ”¹å–„"
        
        analysis["quality_assessment"] = {
            "overall_quality": quality,
            "convergence_score": float(1.0 - np.mean(np.abs(convergences - 0.5))),
            "consistency_score": float(1.0 / (1.0 + np.std(convergences)))
        }
        
        logger.info(f"âœ… åæŸãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†: å“è³ª={quality}")
        return analysis
    
    def analyze_gamma_dependency(self, gamma_values: np.ndarray, 
                                convergences: np.ndarray) -> Dict:
        """Î³å€¤ä¾å­˜æ€§ã®åˆ†æ"""
        logger.info("ğŸ“ˆ Î³å€¤ä¾å­˜æ€§åˆ†æé–‹å§‹...")
        
        # ç›¸é–¢åˆ†æ
        correlation = np.corrcoef(gamma_values, convergences)[0, 1]
        
        # ç·šå½¢å›å¸°
        slope, intercept, r_value, p_value, std_err = stats.linregress(gamma_values, convergences)
        
        # å¤šé …å¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
        poly_coeffs = np.polyfit(gamma_values, convergences, 2)
        poly_func = np.poly1d(poly_coeffs)
        
        # æ®‹å·®åˆ†æ
        linear_pred = slope * gamma_values + intercept
        poly_pred = poly_func(gamma_values)
        
        linear_residuals = convergences - linear_pred
        poly_residuals = convergences - poly_pred
        
        analysis = {
            "correlation": {
                "pearson_correlation": float(correlation),
                "correlation_strength": "å¼·ã„" if abs(correlation) > 0.7 else "ä¸­ç¨‹åº¦" if abs(correlation) > 0.3 else "å¼±ã„"
            },
            "linear_regression": {
                "slope": float(slope),
                "intercept": float(intercept),
                "r_squared": float(r_value**2),
                "p_value": float(p_value),
                "standard_error": float(std_err)
            },
            "polynomial_fit": {
                "coefficients": [float(c) for c in poly_coeffs],
                "linear_rmse": float(np.sqrt(np.mean(linear_residuals**2))),
                "polynomial_rmse": float(np.sqrt(np.mean(poly_residuals**2)))
            },
            "trend_analysis": {
                "overall_trend": "å¢—åŠ " if slope > 0 else "æ¸›å°‘" if slope < 0 else "å¹³å¦",
                "trend_significance": "æœ‰æ„" if p_value < 0.05 else "éæœ‰æ„"
            }
        }
        
        logger.info(f"âœ… Î³å€¤ä¾å­˜æ€§åˆ†æå®Œäº†: ç›¸é–¢={correlation:.4f}")
        return analysis
    
    def theoretical_comparison(self, convergences: np.ndarray) -> Dict:
        """ç†è«–å€¤ã¨ã®æ¯”è¼ƒåˆ†æ"""
        logger.info("ğŸ¯ ç†è«–å€¤æ¯”è¼ƒåˆ†æé–‹å§‹...")
        
        theoretical_value = 0.5
        deviations = np.abs(convergences - theoretical_value)
        
        # çµ±è¨ˆçš„æ¤œå®š
        t_stat, t_p_value = stats.ttest_1samp(convergences, theoretical_value)
        
        # æ­£è¦æ€§æ¤œå®š
        shapiro_stat, shapiro_p = stats.shapiro(convergences)
        
        # ä¿¡é ¼åŒºé–“
        confidence_interval = stats.t.interval(0.95, len(convergences)-1, 
                                             loc=np.mean(convergences), 
                                             scale=stats.sem(convergences))
        
        analysis = {
            "deviation_statistics": {
                "mean_absolute_deviation": float(np.mean(deviations)),
                "max_absolute_deviation": float(np.max(deviations)),
                "min_absolute_deviation": float(np.min(deviations)),
                "std_deviation": float(np.std(deviations))
            },
            "statistical_tests": {
                "t_test": {
                    "statistic": float(t_stat),
                    "p_value": float(t_p_value),
                    "significant_difference": bool(t_p_value < 0.05)
                },
                "normality_test": {
                    "shapiro_statistic": float(shapiro_stat),
                    "shapiro_p_value": float(shapiro_p),
                    "is_normal": bool(shapiro_p > 0.05)
                }
            },
            "confidence_interval": {
                "lower_bound": float(confidence_interval[0]),
                "upper_bound": float(confidence_interval[1]),
                "contains_theoretical": bool(confidence_interval[0] <= theoretical_value <= confidence_interval[1])
            },
            "precision_metrics": {
                "relative_precision": float(np.std(convergences) / np.mean(convergences) * 100),
                "accuracy": float(1.0 - np.mean(deviations)),
                "precision_score": float(1.0 / (1.0 + np.std(convergences)))
            }
        }
        
        logger.info(f"âœ… ç†è«–å€¤æ¯”è¼ƒå®Œäº†: ç²¾åº¦={analysis['precision_metrics']['accuracy']:.6f}")
        return analysis
    
    def generate_improvement_suggestions(self, convergence_analysis: Dict, 
                                       gamma_analysis: Dict, 
                                       theoretical_analysis: Dict) -> List[str]:
        """æ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""
        suggestions = []
        
        # åæŸæ€§ã«åŸºã¥ãææ¡ˆ
        mean_conv = convergence_analysis["basic_statistics"]["mean"]
        if mean_conv < 0.498:
            suggestions.append("ğŸ”§ ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³æ¬¡å…ƒã‚’å¢—åŠ ã•ã›ã¦ç²¾åº¦å‘ä¸Šã‚’å›³ã‚‹")
            suggestions.append("âš™ï¸ éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î¸ã®æœ€é©åŒ–ã‚’å®Ÿæ–½")
        
        # å®‰å®šæ€§ã«åŸºã¥ãææ¡ˆ
        cv = convergence_analysis["stability_metrics"]["coefficient_of_variation"]
        if cv > 0.01:
            suggestions.append("ğŸ“Š æ•°å€¤å®‰å®šæ€§å‘ä¸Šã®ãŸã‚æ­£å‰‡åŒ–é …ã‚’èª¿æ•´")
            suggestions.append("ğŸ¯ é©å¿œçš„æ¬¡å…ƒèª¿æ•´ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹è‰¯")
        
        # Î³å€¤ä¾å­˜æ€§ã«åŸºã¥ãææ¡ˆ
        if abs(gamma_analysis["correlation"]["pearson_correlation"]) > 0.5:
            suggestions.append("ğŸ“ˆ Î³å€¤ä¾å­˜æ€§ã‚’è€ƒæ…®ã—ãŸé©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´")
            suggestions.append("ğŸ”„ Î³å€¤ç¯„å›²åˆ¥ã®æœ€é©åŒ–æˆ¦ç•¥ã®å®Ÿè£…")
        
        # ç†è«–å€¤ã‹ã‚‰ã®åå·®ã«åŸºã¥ãææ¡ˆ
        if theoretical_analysis["deviation_statistics"]["mean_absolute_deviation"] > 0.003:
            suggestions.append("ğŸ¯ é«˜ç²¾åº¦æ¼”ç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å°å…¥æ¤œè¨")
            suggestions.append("ğŸ’» GPUè¨ˆç®—ç²¾åº¦ã®å‘ä¸Š")
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã«åŸºã¥ãææ¡ˆ
        if theoretical_analysis["statistical_tests"]["t_test"]["significant_difference"]:
            suggestions.append("ğŸ“Š ç³»çµ±çš„ãƒã‚¤ã‚¢ã‚¹ã®èª¿æŸ»ã¨è£œæ­£")
            suggestions.append("ğŸ”¬ ç†è«–ãƒ¢ãƒ‡ãƒ«ã®å†æ¤œè¨")
        
        if not suggestions:
            suggestions.append("ğŸ‰ ç¾åœ¨ã®åæŸæ€§ã¯å„ªç§€ã§ã™ï¼ã•ã‚‰ãªã‚‹ç²¾åº¦å‘ä¸Šã®ãŸã‚å¤§è¦æ¨¡æ¤œè¨¼ã‚’æ¨å¥¨")
        
        return suggestions
    
    def create_comprehensive_visualization(self, gamma_values: np.ndarray, 
                                         convergences: np.ndarray, 
                                         real_parts: np.ndarray):
        """åŒ…æ‹¬çš„å¯è¦–åŒ–ã®ä½œæˆ"""
        logger.info("ğŸ“Š åŒ…æ‹¬çš„å¯è¦–åŒ–ä½œæˆé–‹å§‹...")
        
        # å›³ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        fig = plt.figure(figsize=(20, 16))
        gs = fig.add_gridspec(4, 3, hspace=0.3, wspace=0.3)
        
        # 1. åæŸæ€§ã®æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆ
        ax1 = fig.add_subplot(gs[0, :])
        ax1.plot(gamma_values, convergences, 'o-', color='red', linewidth=2, markersize=8, label='åæŸåº¦')
        ax1.axhline(y=0.5, color='green', linestyle='--', linewidth=2, label='ç†è«–å€¤ (1/2)')
        ax1.fill_between(gamma_values, convergences - np.std(convergences), 
                        convergences + np.std(convergences), alpha=0.3, color='red')
        ax1.set_xlabel('Î³å€¤')
        ax1.set_ylabel('åæŸåº¦')
        ax1.set_title('ğŸ¯ è‡¨ç•Œç·šåæŸæ€§ã®è©³ç´°åˆ†æ', fontsize=16, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. åæŸåº¦ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        ax2 = fig.add_subplot(gs[1, 0])
        ax2.hist(convergences, bins=15, alpha=0.7, color='blue', edgecolor='black')
        ax2.axvline(x=np.mean(convergences), color='red', linestyle='--', 
                   label=f'å¹³å‡: {np.mean(convergences):.6f}')
        ax2.axvline(x=0.5, color='green', linestyle='--', label='ç†è«–å€¤: 0.5')
        ax2.set_xlabel('åæŸåº¦')
        ax2.set_ylabel('é »åº¦')
        ax2.set_title('ğŸ“Š åæŸåº¦åˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å®Ÿéƒ¨ã®åˆ†æ
        ax3 = fig.add_subplot(gs[1, 1])
        ax3.plot(gamma_values, real_parts, 's-', color='purple', linewidth=2, markersize=6)
        ax3.axhline(y=0.5, color='green', linestyle='--', linewidth=2)
        ax3.set_xlabel('Î³å€¤')
        ax3.set_ylabel('å®Ÿéƒ¨')
        ax3.set_title('ğŸ”¬ å®Ÿéƒ¨ã®å¤‰åŒ–')
        ax3.grid(True, alpha=0.3)
        
        # 4. ç†è«–å€¤ã‹ã‚‰ã®åå·®
        ax4 = fig.add_subplot(gs[1, 2])
        deviations = np.abs(convergences - 0.5)
        ax4.plot(gamma_values, deviations, '^-', color='orange', linewidth=2, markersize=6)
        ax4.set_xlabel('Î³å€¤')
        ax4.set_ylabel('|åæŸåº¦ - 1/2|')
        ax4.set_title('ğŸ“ ç†è«–å€¤ã‹ã‚‰ã®åå·®')
        ax4.grid(True, alpha=0.3)
        
        # 5. ç›¸é–¢åˆ†æ
        ax5 = fig.add_subplot(gs[2, 0])
        ax5.scatter(gamma_values, convergences, c=range(len(gamma_values)), 
                   cmap='viridis', s=100, alpha=0.7)
        z = np.polyfit(gamma_values, convergences, 1)
        p = np.poly1d(z)
        ax5.plot(gamma_values, p(gamma_values), "r--", alpha=0.8, linewidth=2)
        ax5.set_xlabel('Î³å€¤')
        ax5.set_ylabel('åæŸåº¦')
        ax5.set_title('ğŸ“ˆ Î³å€¤ vs åæŸåº¦')
        ax5.grid(True, alpha=0.3)
        
        # 6. çµ±è¨ˆçš„å“è³ªè©•ä¾¡
        ax6 = fig.add_subplot(gs[2, 1])
        metrics = ['å¹³å‡', 'æ¨™æº–åå·®', 'æœ€å°å€¤', 'æœ€å¤§å€¤']
        values = [np.mean(convergences), np.std(convergences), 
                 np.min(convergences), np.max(convergences)]
        colors = ['blue', 'orange', 'green', 'red']
        bars = ax6.bar(metrics, values, color=colors, alpha=0.7)
        ax6.set_ylabel('å€¤')
        ax6.set_title('ğŸ“Š çµ±è¨ˆçš„å“è³ªæŒ‡æ¨™')
        ax6.grid(True, alpha=0.3)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax6.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                    f'{value:.6f}', ha='center', va='bottom', fontsize=10)
        
        # 7. åæŸæ€§ã®æ™‚é–“ç™ºå±•ï¼ˆç§»å‹•å¹³å‡ï¼‰
        ax7 = fig.add_subplot(gs[2, 2])
        window_size = min(5, len(convergences)//2)
        if window_size >= 2:
            moving_avg = pd.Series(convergences).rolling(window=window_size, center=True).mean()
            ax7.plot(gamma_values, convergences, 'o', alpha=0.5, color='gray', label='ç”Ÿãƒ‡ãƒ¼ã‚¿')
            ax7.plot(gamma_values, moving_avg, '-', color='red', linewidth=3, label=f'ç§»å‹•å¹³å‡({window_size})')
            ax7.axhline(y=0.5, color='green', linestyle='--', linewidth=2, label='ç†è«–å€¤')
            ax7.set_xlabel('Î³å€¤')
            ax7.set_ylabel('åæŸåº¦')
            ax7.set_title('ğŸ“ˆ åæŸæ€§ã®å¹³æ»‘åŒ–ãƒˆãƒ¬ãƒ³ãƒ‰')
            ax7.legend()
            ax7.grid(True, alpha=0.3)
        
        # 8. ç²¾åº¦è©•ä¾¡ã‚µãƒãƒªãƒ¼
        ax8 = fig.add_subplot(gs[3, :])
        ax8.axis('off')
        
        # ã‚µãƒãƒªãƒ¼ãƒ†ã‚­ã‚¹ãƒˆ
        summary_text = f"""
        ğŸ¯ NKAT v11 è©³ç´°åæŸåˆ†æã‚µãƒãƒªãƒ¼
        
        ğŸ“Š åŸºæœ¬çµ±è¨ˆ:
        â€¢ å¹³å‡åæŸåº¦: {np.mean(convergences):.8f}
        â€¢ æ¨™æº–åå·®: {np.std(convergences):.8f}
        â€¢ ç†è«–å€¤ã‹ã‚‰ã®å¹³å‡åå·®: {np.mean(np.abs(convergences - 0.5)):.8f}
        â€¢ ç›¸å¯¾èª¤å·®: {np.mean(np.abs(convergences - 0.5)) / 0.5 * 100:.4f}%
        
        ğŸ¯ å“è³ªè©•ä¾¡:
        â€¢ ç²¾åº¦ã‚¹ã‚³ã‚¢: {1.0 - np.mean(np.abs(convergences - 0.5)):.6f}
        â€¢ ä¸€è²«æ€§ã‚¹ã‚³ã‚¢: {1.0 / (1.0 + np.std(convergences)):.6f}
        â€¢ å…¨ä½“å“è³ª: {"å„ªç§€" if np.mean(convergences) > 0.497 else "è‰¯å¥½" if np.mean(convergences) > 0.495 else "æ™®é€š"}
        
        ğŸ“ˆ çµ±è¨ˆçš„æœ‰æ„æ€§:
        â€¢ Î³å€¤ã¨ã®ç›¸é–¢: {np.corrcoef(gamma_values, convergences)[0, 1]:.4f}
        â€¢ å¤‰å‹•ä¿‚æ•°: {np.std(convergences) / np.mean(convergences):.6f}
        """
        
        ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes, fontsize=12,
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.suptitle('ğŸš€ NKAT v11 åŒ…æ‹¬çš„åæŸåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰', fontsize=20, fontweight='bold')
        
        # ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = self.output_dir / f"nkat_v11_comprehensive_convergence_analysis_{timestamp}.png"
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        logger.info(f"ğŸ“Š å¯è¦–åŒ–ä¿å­˜å®Œäº†: {output_file}")
        
        plt.show()
        return output_file
    
    def run_comprehensive_analysis(self) -> Dict:
        """åŒ…æ‹¬çš„åˆ†æã®å®Ÿè¡Œ"""
        logger.info("ğŸš€ NKAT v11 è©³ç´°åæŸåˆ†æé–‹å§‹")
        print("=" * 80)
        print("ğŸ¯ NKAT v11 è©³ç´°åæŸåˆ†æã‚·ã‚¹ãƒ†ãƒ ")
        print("=" * 80)
        print(f"ğŸ“… åˆ†æé–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("ğŸ”¬ ç›®æ¨™: 0.497762åæŸçµæœã®æ·±æ˜ã‚Šåˆ†æ")
        print("=" * 80)
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        results = self.load_latest_results()
        if not results:
            raise ValueError("æ¤œè¨¼çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # 2. ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        gamma_values, convergences, real_parts = self.extract_convergence_data(results)
        
        # 3. åæŸãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        convergence_analysis = self.analyze_convergence_pattern(gamma_values, convergences)
        self.analysis_results["convergence_analysis"] = convergence_analysis
        
        # 4. Î³å€¤ä¾å­˜æ€§åˆ†æ
        gamma_analysis = self.analyze_gamma_dependency(gamma_values, convergences)
        self.analysis_results["gamma_dependency"] = gamma_analysis
        
        # 5. ç†è«–å€¤æ¯”è¼ƒ
        theoretical_analysis = self.theoretical_comparison(convergences)
        self.analysis_results["theoretical_comparison"] = theoretical_analysis
        
        # 6. æ”¹å–„ææ¡ˆç”Ÿæˆ
        suggestions = self.generate_improvement_suggestions(
            convergence_analysis, gamma_analysis, theoretical_analysis
        )
        self.analysis_results["improvement_suggestions"] = suggestions
        
        # 7. å¯è¦–åŒ–ä½œæˆ
        visualization_file = self.create_comprehensive_visualization(
            gamma_values, convergences, real_parts
        )
        self.analysis_results["visualization_file"] = str(visualization_file)
        
        # 8. çµæœä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = self.output_dir / f"nkat_v11_detailed_analysis_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, indent=2, ensure_ascii=False)
        
        # 9. çµæœè¡¨ç¤º
        self.display_analysis_summary()
        
        logger.info(f"ğŸ’¾ åˆ†æçµæœä¿å­˜: {results_file}")
        print(f"\nğŸ’¾ è©³ç´°åˆ†æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {results_file}")
        print("ğŸ‰ NKAT v11 è©³ç´°åæŸåˆ†æå®Œäº†ï¼")
        
        return self.analysis_results
    
    def display_analysis_summary(self):
        """åˆ†æã‚µãƒãƒªãƒ¼ã®è¡¨ç¤º"""
        print("\n" + "=" * 80)
        print("ğŸ“Š NKAT v11 è©³ç´°åæŸåˆ†æçµæœ")
        print("=" * 80)
        
        # åŸºæœ¬çµ±è¨ˆ
        conv_stats = self.analysis_results["convergence_analysis"]["basic_statistics"]
        print(f"\nğŸ¯ åŸºæœ¬çµ±è¨ˆ:")
        print(f"  å¹³å‡åæŸåº¦: {conv_stats['mean']:.8f}")
        print(f"  æ¨™æº–åå·®: {conv_stats['std']:.8f}")
        print(f"  æœ€è‰¯åæŸ: {conv_stats['min']:.8f}")
        print(f"  æœ€æ‚ªåæŸ: {conv_stats['max']:.8f}")
        
        # å“è³ªè©•ä¾¡
        quality = self.analysis_results["convergence_analysis"]["quality_assessment"]
        print(f"\nğŸ“ˆ å“è³ªè©•ä¾¡:")
        print(f"  å…¨ä½“å“è³ª: {quality['overall_quality']}")
        print(f"  åæŸã‚¹ã‚³ã‚¢: {quality['convergence_score']:.6f}")
        print(f"  ä¸€è²«æ€§ã‚¹ã‚³ã‚¢: {quality['consistency_score']:.6f}")
        
        # ç†è«–å€¤æ¯”è¼ƒ
        theoretical = self.analysis_results["theoretical_comparison"]
        print(f"\nğŸ¯ ç†è«–å€¤æ¯”è¼ƒ:")
        print(f"  å¹³å‡çµ¶å¯¾åå·®: {theoretical['deviation_statistics']['mean_absolute_deviation']:.8f}")
        print(f"  ç›¸å¯¾èª¤å·®: {theoretical['precision_metrics']['relative_precision']:.4f}%")
        print(f"  ç²¾åº¦: {theoretical['precision_metrics']['accuracy']:.6f}")
        
        # æ”¹å–„ææ¡ˆ
        print(f"\nğŸ”§ æ”¹å–„ææ¡ˆ:")
        for i, suggestion in enumerate(self.analysis_results["improvement_suggestions"], 1):
            print(f"  {i}. {suggestion}")
        
        print("=" * 80)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        analyzer = NKATConvergenceAnalyzer()
        results = analyzer.run_comprehensive_analysis()
        return results
    except Exception as e:
        logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        return None

if __name__ == "__main__":
    main() 
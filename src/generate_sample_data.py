#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§ª NKAT Dashboard ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå™¨
Sample Data Generator for NKAT Streamlit Dashboard Testing

Author: NKAT Research Consortium
Date: 2025-05-26
Version: 1.0 - Sample Data Generator
"""

import json
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import random

def generate_entanglement_sample_data():
    """é‡å­ã‚‚ã¤ã‚Œè§£æã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    gamma_values = [14.134725, 21.022040, 25.010858, 30.424876, 32.935062]
    
    entanglement_metrics = []
    for gamma in gamma_values:
        metrics = {
            "gamma": gamma,
            "concurrence": random.uniform(0.0, 0.1),
            "entanglement_entropy": random.uniform(0.0, 0.01),
            "negativity": random.uniform(0.0, 0.05),
            "quantum_discord": random.uniform(0.0, 0.02),
            "bell_violation": random.uniform(0.0, 0.01),
            "timestamp": datetime.now().isoformat()
        }
        entanglement_metrics.append(metrics)
    
    sample_data = {
        "timestamp": datetime.now().isoformat(),
        "gamma_values": gamma_values,
        "entanglement_metrics": entanglement_metrics,
        "statistics": {
            "mean_concurrence": np.mean([m["concurrence"] for m in entanglement_metrics]),
            "max_concurrence": np.max([m["concurrence"] for m in entanglement_metrics]),
            "mean_entropy": np.mean([m["entanglement_entropy"] for m in entanglement_metrics]),
            "entanglement_detection_rate": 0.2
        }
    }
    
    # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    results_dir = Path("10k_gamma_results")
    results_dir.mkdir(exist_ok=True)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    with open(results_dir / "nkat_v91_entanglement_results.json", 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, indent=2, ensure_ascii=False, default=str)
    
    print("âœ… é‡å­ã‚‚ã¤ã‚Œã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")

def generate_10k_gamma_sample_data():
    """10,000Î³ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    # é€²è¡Œä¸­ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿
    checkpoint_data = {
        "batch_id": 25,
        "gamma_start_idx": 2500,
        "gamma_end_idx": 2600,
        "completed_gammas": [14.134725 + i * 0.1 for i in range(100)],
        "results": [],
        "timestamp": datetime.now().isoformat(),
        "system_state": {
            "cpu_percent": 75.5,
            "memory_percent": 68.2,
            "gpu_memory_used": 8.5,
            "gpu_memory_total": 10.7,
            "timestamp": datetime.now().isoformat()
        },
        "memory_usage": 68.2,
        "gpu_memory": 8.5,
        "total_progress": 26.0
    }
    
    # çµæœãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    results = []
    for i in range(2600):
        gamma = 14.134725 + i * 0.1
        spectral_dim = 1.0 + random.uniform(-0.2, 0.2)
        real_part = spectral_dim / 2
        convergence = abs(real_part - 0.5)
        
        result = {
            "gamma": gamma,
            "spectral_dimension": spectral_dim,
            "real_part": real_part,
            "convergence_to_half": convergence,
            "timestamp": datetime.now().isoformat(),
            "batch_id": i // 100,
            "batch_index": i % 100
        }
        results.append(result)
    
    checkpoint_data["results"] = results
    
    # æœ€çµ‚çµæœãƒ‡ãƒ¼ã‚¿ï¼ˆå®Œäº†ç‰ˆï¼‰
    final_results = {
        "timestamp": datetime.now().isoformat(),
        "total_gammas_processed": 2600,
        "valid_results": 2580,
        "execution_time_seconds": 450.5,
        "execution_time_formatted": "0h 7m 30.5s",
        "processing_speed_per_gamma": 0.173,
        "success_rate": 0.992,
        "statistics": {
            "mean_spectral_dimension": 1.025,
            "std_spectral_dimension": 0.156,
            "mean_convergence": 0.089,
            "best_convergence": 0.001,
            "worst_convergence": 0.245
        },
        "results": results
    }
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    results_dir = Path("10k_gamma_results")
    results_dir.mkdir(exist_ok=True)
    
    checkpoint_dir = Path("10k_gamma_checkpoints_production")
    checkpoint_dir.mkdir(exist_ok=True)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    checkpoint_file = checkpoint_dir / f"checkpoint_batch_25_{timestamp}.json"
    with open(checkpoint_file, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, indent=2, ensure_ascii=False, default=str)
    
    # æœ€çµ‚çµæœä¿å­˜
    final_file = results_dir / f"10k_gamma_final_results_{timestamp}.json"
    with open(final_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)
    
    print("âœ… 10,000Î³ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")

def generate_historical_data():
    """å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
    results_dir = Path("10k_gamma_results")
    results_dir.mkdir(exist_ok=True)
    
    # éå»æ•°æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿
    for days_ago in range(1, 8):
        date = datetime.now() - timedelta(days=days_ago)
        timestamp = date.strftime("%Y%m%d_%H%M%S")
        
        # ä¸­é–“çµæœãƒ‡ãƒ¼ã‚¿
        intermediate_data = {
            "timestamp": date.isoformat(),
            "batches_completed": random.randint(10, 50),
            "total_results": random.randint(1000, 5000),
            "results": [
                {
                    "gamma": 14.134725 + i * 0.1,
                    "spectral_dimension": 1.0 + random.uniform(-0.3, 0.3),
                    "batch_id": i // 100
                }
                for i in range(random.randint(100, 500))
            ]
        }
        
        intermediate_file = results_dir / f"intermediate_results_batch_{intermediate_data['batches_completed']}_{timestamp}.json"
        with open(intermediate_file, 'w', encoding='utf-8') as f:
            json.dump(intermediate_data, f, indent=2, ensure_ascii=False, default=str)
    
    print("âœ… å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ§ª NKAT Dashboard ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹")
    print("=" * 60)
    
    try:
        # é‡å­ã‚‚ã¤ã‚Œãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        generate_entanglement_sample_data()
        
        # 10,000Î³ãƒãƒ£ãƒ¬ãƒ³ã‚¸ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        generate_10k_gamma_sample_data()
        
        # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        generate_historical_data()
        
        print("=" * 60)
        print("ğŸ‰ å…¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print()
        print("ğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:")
        print("  ğŸ“Š 10k_gamma_results/nkat_v91_entanglement_results.json")
        print("  ğŸ“Š 10k_gamma_results/10k_gamma_final_results_*.json")
        print("  ğŸ“Š 10k_gamma_results/intermediate_results_*.json")
        print("  ğŸ’¾ 10k_gamma_checkpoints_production/checkpoint_batch_*.json")
        print()
        print("ğŸš€ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’èµ·å‹•ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        print("   start_dashboard.bat")
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

if __name__ == "__main__":
    main() 
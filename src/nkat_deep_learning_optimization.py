#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKATæ·±å±¤å­¦ç¿’æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼šéå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨è¶…åæŸå› å­ã®ä¿‚æ•°æœ€é©åŒ–
NKAT Deep Learning Optimization System: Non-Commutative Parameter and Super-Convergence Factor Optimization

Author: å³¯å²¸ äº® (Ryo Minegishi)
Date: 2025å¹´5æœˆ28æ—¥
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from scipy.integrate import quad
from scipy.special import zeta
import pandas as pd
from tqdm import tqdm
import json
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

# GPUè¨­å®š
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”§ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

class NKATDataset(Dataset):
    """
    NKATç†è«–ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    """
    
    def __init__(self, N_values, target_values, noise_level=1e-6):
        """
        Args:
            N_values: æ¬¡å…ƒæ•°ã®é…åˆ—
            target_values: ç›®æ¨™è¶…åæŸå› å­å€¤
            noise_level: ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
        """
        self.N_values = torch.tensor(N_values, dtype=torch.float32)
        self.target_values = torch.tensor(target_values, dtype=torch.float32)
        
        # ãƒã‚¤ã‚ºè¿½åŠ 
        if noise_level > 0:
            noise = torch.normal(0, noise_level, size=self.target_values.shape)
            self.target_values += noise
    
    def __len__(self):
        return len(self.N_values)
    
    def __getitem__(self, idx):
        return self.N_values[idx], self.target_values[idx]

class NKATSuperConvergenceNet(nn.Module):
    """
    NKATè¶…åæŸå› å­äºˆæ¸¬ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    """
    
    def __init__(self, hidden_dims=[128, 256, 512, 256, 128], dropout_rate=0.1):
        """
        Args:
            hidden_dims: éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°ãƒªã‚¹ãƒˆ
            dropout_rate: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
        """
        super(NKATSuperConvergenceNet, self).__init__()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äºˆæ¸¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        layers = []
        input_dim = 1  # Nå€¤
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(dropout_rate)
            ])
            input_dim = hidden_dim
        
        # æœ€çµ‚å±¤ï¼š3ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (Î³, Î´, t_c)
        layers.append(nn.Linear(input_dim, 3))
        
        self.parameter_net = nn.Sequential(*layers)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶ç´„ç”¨æ´»æ€§åŒ–é–¢æ•°
        self.gamma_activation = nn.Sigmoid()  # 0 < Î³ < 1
        self.delta_activation = nn.Sigmoid()  # 0 < Î´ < 0.1
        self.tc_activation = nn.Softplus()    # t_c > 1
        
        # è¶…åæŸå› å­è¨ˆç®—ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.convergence_net = nn.Sequential(
            nn.Linear(4, 256),  # N + 3ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )
        
        print("ğŸ§  NKATæ·±å±¤å­¦ç¿’ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“Š ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in self.parameters()):,}")
    
    def forward(self, N):
        """
        å‰å‘ãè¨ˆç®—
        
        Args:
            N: æ¬¡å…ƒæ•°ãƒ†ãƒ³ã‚½ãƒ«
            
        Returns:
            è¶…åæŸå› å­ã®äºˆæ¸¬å€¤
        """
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿äºˆæ¸¬
        raw_params = self.parameter_net(N.unsqueeze(-1))
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶ç´„é©ç”¨
        gamma = self.gamma_activation(raw_params[:, 0]) * 0.5 + 0.1  # 0.1 < Î³ < 0.6
        delta = self.delta_activation(raw_params[:, 1]) * 0.08 + 0.01  # 0.01 < Î´ < 0.09
        t_c = self.tc_activation(raw_params[:, 2]) + 10.0  # t_c > 10
        
        # å…¥åŠ›ç‰¹å¾´é‡çµåˆ
        features = torch.stack([N, gamma, delta, t_c], dim=1)
        
        # è¶…åæŸå› å­è¨ˆç®—
        log_S = self.convergence_net(features)
        S = torch.exp(log_S.squeeze())
        
        return S, gamma, delta, t_c
    
    def theoretical_super_convergence(self, N, gamma, delta, t_c):
        """
        ç†è«–çš„è¶…åæŸå› å­ã®è¨ˆç®—
        
        Args:
            N: æ¬¡å…ƒæ•°
            gamma, delta, t_c: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            ç†è«–çš„è¶…åæŸå› å­
        """
        # å¯†åº¦é–¢æ•°ã®ç©åˆ†
        integral = gamma * torch.log(N / t_c)
        
        # æŒ‡æ•°æ¸›è¡°é …ï¼ˆN > t_c ã®å ´åˆï¼‰
        mask = N > t_c
        if mask.any():
            integral = torch.where(mask, 
                                 integral + delta * (N - t_c),
                                 integral)
        
        return torch.exp(integral)

class NKATPhysicsLoss(nn.Module):
    """
    NKATç†è«–ã«åŸºã¥ãç‰©ç†åˆ¶ç´„æå¤±é–¢æ•°
    """
    
    def __init__(self, alpha=1.0, beta=0.1, gamma=0.01):
        """
        Args:
            alpha: ãƒ‡ãƒ¼ã‚¿é©åˆé …ã®é‡ã¿
            beta: ç‰©ç†åˆ¶ç´„é …ã®é‡ã¿
            gamma: æ­£å‰‡åŒ–é …ã®é‡ã¿
        """
        super(NKATPhysicsLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.mse = nn.MSELoss()
    
    def forward(self, predictions, targets, N_values, gamma_pred, delta_pred, tc_pred, model):
        """
        ç‰©ç†åˆ¶ç´„ä»˜ãæå¤±è¨ˆç®—
        
        Args:
            predictions: äºˆæ¸¬å€¤
            targets: ç›®æ¨™å€¤
            N_values: æ¬¡å…ƒæ•°
            gamma_pred, delta_pred, tc_pred: äºˆæ¸¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            model: ãƒ¢ãƒ‡ãƒ«
            
        Returns:
            ç·æå¤±
        """
        # ãƒ‡ãƒ¼ã‚¿é©åˆæå¤±
        data_loss = self.mse(predictions, targets)
        
        # ç‰©ç†åˆ¶ç´„æå¤±
        physics_loss = self._physics_constraints(N_values, gamma_pred, delta_pred, tc_pred, model)
        
        # æ­£å‰‡åŒ–æå¤±
        reg_loss = self._regularization_loss(model)
        
        total_loss = self.alpha * data_loss + self.beta * physics_loss + self.gamma * reg_loss
        
        return total_loss, data_loss, physics_loss, reg_loss
    
    def _physics_constraints(self, N_values, gamma_pred, delta_pred, tc_pred, model):
        """
        ç‰©ç†åˆ¶ç´„ã®è¨ˆç®—
        """
        constraints = []
        
        # 1. ãƒªãƒ¼ãƒãƒ³äºˆæƒ³åˆ¶ç´„: Î³ ln(N/t_c) â†’ 1/2
        riemann_constraint = torch.mean((gamma_pred * torch.log(N_values / tc_pred) - 0.5) ** 2)
        constraints.append(riemann_constraint)
        
        # 2. å˜èª¿æ€§åˆ¶ç´„: S(N)ã¯å˜èª¿å¢—åŠ 
        if len(N_values) > 1:
            sorted_indices = torch.argsort(N_values)
            sorted_N = N_values[sorted_indices]
            sorted_S, _, _, _ = model(sorted_N)
            monotonicity_loss = torch.mean(torch.relu(sorted_S[:-1] - sorted_S[1:]))
            constraints.append(monotonicity_loss)
        
        # 3. æ¼¸è¿‘åˆ¶ç´„: S(N) ~ N^Î³ for large N
        large_N_mask = N_values > 100
        if large_N_mask.any():
            large_N = N_values[large_N_mask]
            large_S, large_gamma, _, _ = model(large_N)
            theoretical_S = torch.pow(large_N, large_gamma)
            asymptotic_loss = torch.mean((large_S / theoretical_S - 1) ** 2)
            constraints.append(asymptotic_loss)
        
        # 4. å¯†åº¦é–¢æ•°æ­£å€¤åˆ¶ç´„
        positivity_loss = torch.mean(torch.relu(-gamma_pred)) + torch.mean(torch.relu(-delta_pred))
        constraints.append(positivity_loss)
        
        return sum(constraints)
    
    def _regularization_loss(self, model):
        """
        æ­£å‰‡åŒ–æå¤±ã®è¨ˆç®—
        """
        l2_reg = sum(torch.norm(param) ** 2 for param in model.parameters())
        return l2_reg

class NKATDeepLearningOptimizer:
    """
    NKATæ·±å±¤å­¦ç¿’æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, learning_rate=1e-3, batch_size=32, num_epochs=1000):
        """
        Args:
            learning_rate: å­¦ç¿’ç‡
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        """
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.num_epochs = num_epochs
        
        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        self.model = NKATSuperConvergenceNet().to(device)
        
        # æå¤±é–¢æ•°
        self.criterion = NKATPhysicsLoss()
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.AdamW(self.model.parameters(), 
                                   lr=learning_rate, 
                                   weight_decay=1e-5)
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=100, T_mult=2
        )
        
        # å±¥æ­´
        self.train_history = {
            'total_loss': [],
            'data_loss': [],
            'physics_loss': [],
            'reg_loss': [],
            'gamma_values': [],
            'delta_values': [],
            'tc_values': []
        }
        
        print("ğŸš€ NKATæ·±å±¤å­¦ç¿’æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def generate_training_data(self, N_range=(10, 1000), num_samples=1000):
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        
        Args:
            N_range: æ¬¡å…ƒæ•°ã®ç¯„å›²
            num_samples: ã‚µãƒ³ãƒ—ãƒ«æ•°
            
        Returns:
            ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        """
        print("ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
        
        # æ¬¡å…ƒæ•°ã®ç”Ÿæˆï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        N_values = np.logspace(np.log10(N_range[0]), np.log10(N_range[1]), num_samples)
        
        # ç†è«–çš„è¶…åæŸå› å­ã®è¨ˆç®—
        gamma_true = 0.234
        delta_true = 0.035
        t_c_true = 17.26
        
        target_values = []
        for N in tqdm(N_values, desc="ç†è«–å€¤è¨ˆç®—"):
            # ç†è«–çš„è¶…åæŸå› å­
            integral = gamma_true * np.log(N / t_c_true)
            if N > t_c_true:
                integral += delta_true * (N - t_c_true)
            S_theoretical = np.exp(integral)
            target_values.append(S_theoretical)
        
        target_values = np.array(target_values)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        dataset = NKATDataset(N_values, target_values, noise_level=1e-4)
        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)
        
        print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {num_samples}ã‚µãƒ³ãƒ—ãƒ«")
        return dataloader
    
    def train(self, dataloader):
        """
        ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        
        Args:
            dataloader: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        """
        print("ğŸ“ ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
        
        self.model.train()
        
        for epoch in tqdm(range(self.num_epochs), desc="ã‚¨ãƒãƒƒã‚¯"):
            epoch_losses = {'total': 0, 'data': 0, 'physics': 0, 'reg': 0}
            epoch_params = {'gamma': [], 'delta': [], 'tc': []}
            
            for batch_N, batch_targets in dataloader:
                batch_N = batch_N.to(device)
                batch_targets = batch_targets.to(device)
                
                # å‰å‘ãè¨ˆç®—
                predictions, gamma_pred, delta_pred, tc_pred = self.model(batch_N)
                
                # æå¤±è¨ˆç®—
                total_loss, data_loss, physics_loss, reg_loss = self.criterion(
                    predictions, batch_targets, batch_N, gamma_pred, delta_pred, tc_pred, self.model
                )
                
                # é€†ä¼æ’­
                self.optimizer.zero_grad()
                total_loss.backward()
                
                # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                # æå¤±è¨˜éŒ²
                epoch_losses['total'] += total_loss.item()
                epoch_losses['data'] += data_loss.item()
                epoch_losses['physics'] += physics_loss.item()
                epoch_losses['reg'] += reg_loss.item()
                
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨˜éŒ²
                epoch_params['gamma'].extend(gamma_pred.detach().cpu().numpy())
                epoch_params['delta'].extend(delta_pred.detach().cpu().numpy())
                epoch_params['tc'].extend(tc_pred.detach().cpu().numpy())
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼æ›´æ–°
            self.scheduler.step()
            
            # ã‚¨ãƒãƒƒã‚¯å¹³å‡ã®è¨˜éŒ²
            num_batches = len(dataloader)
            for key in epoch_losses:
                epoch_losses[key] /= num_batches
            
            self.train_history['total_loss'].append(epoch_losses['total'])
            self.train_history['data_loss'].append(epoch_losses['data'])
            self.train_history['physics_loss'].append(epoch_losses['physics'])
            self.train_history['reg_loss'].append(epoch_losses['reg'])
            
            self.train_history['gamma_values'].append(np.mean(epoch_params['gamma']))
            self.train_history['delta_values'].append(np.mean(epoch_params['delta']))
            self.train_history['tc_values'].append(np.mean(epoch_params['tc']))
            
            # é€²æ—è¡¨ç¤º
            if (epoch + 1) % 100 == 0:
                print(f"ã‚¨ãƒãƒƒã‚¯ {epoch+1}/{self.num_epochs}:")
                print(f"  ç·æå¤±: {epoch_losses['total']:.6f}")
                print(f"  ãƒ‡ãƒ¼ã‚¿æå¤±: {epoch_losses['data']:.6f}")
                print(f"  ç‰©ç†æå¤±: {epoch_losses['physics']:.6f}")
                print(f"  å¹³å‡Î³: {np.mean(epoch_params['gamma']):.6f}")
                print(f"  å¹³å‡Î´: {np.mean(epoch_params['delta']):.6f}")
                print(f"  å¹³å‡t_c: {np.mean(epoch_params['tc']):.6f}")
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«è¨“ç·´å®Œäº†")
    
    def evaluate_model(self, test_N_values):
        """
        ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        
        Args:
            test_N_values: ãƒ†ã‚¹ãƒˆç”¨æ¬¡å…ƒæ•°
            
        Returns:
            è©•ä¾¡çµæœ
        """
        print("ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
        
        self.model.eval()
        
        with torch.no_grad():
            test_N_tensor = torch.tensor(test_N_values, dtype=torch.float32).to(device)
            predictions, gamma_pred, delta_pred, tc_pred = self.model(test_N_tensor)
            
            # CPU ã«ç§»å‹•
            predictions = predictions.cpu().numpy()
            gamma_pred = gamma_pred.cpu().numpy()
            delta_pred = delta_pred.cpu().numpy()
            tc_pred = tc_pred.cpu().numpy()
        
        # çµ±è¨ˆè¨ˆç®—
        results = {
            'predictions': predictions,
            'gamma_mean': np.mean(gamma_pred),
            'gamma_std': np.std(gamma_pred),
            'delta_mean': np.mean(delta_pred),
            'delta_std': np.std(delta_pred),
            'tc_mean': np.mean(tc_pred),
            'tc_std': np.std(tc_pred),
            'gamma_values': gamma_pred,
            'delta_values': delta_pred,
            'tc_values': tc_pred
        }
        
        print("âœ… ãƒ¢ãƒ‡ãƒ«è©•ä¾¡å®Œäº†")
        print(f"ğŸ“Š æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        print(f"  Î³ = {results['gamma_mean']:.6f} Â± {results['gamma_std']:.6f}")
        print(f"  Î´ = {results['delta_mean']:.6f} Â± {results['delta_std']:.6f}")
        print(f"  t_c = {results['tc_mean']:.6f} Â± {results['tc_std']:.6f}")
        
        return results
    
    def visualize_results(self, test_N_values, results):
        """
        çµæœã®å¯è¦–åŒ–
        
        Args:
            test_N_values: ãƒ†ã‚¹ãƒˆç”¨æ¬¡å…ƒæ•°
            results: è©•ä¾¡çµæœ
        """
        print("ğŸ“ˆ çµæœå¯è¦–åŒ–ä¸­...")
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. è¨“ç·´æå¤±ã®æ¨ç§»
        axes[0, 0].plot(self.train_history['total_loss'], label='ç·æå¤±', color='red')
        axes[0, 0].plot(self.train_history['data_loss'], label='ãƒ‡ãƒ¼ã‚¿æå¤±', color='blue')
        axes[0, 0].plot(self.train_history['physics_loss'], label='ç‰©ç†æå¤±', color='green')
        axes[0, 0].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
        axes[0, 0].set_ylabel('æå¤±')
        axes[0, 0].set_title('è¨“ç·´æå¤±ã®æ¨ç§»')
        axes[0, 0].legend()
        axes[0, 0].set_yscale('log')
        
        # 2. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ
        axes[0, 1].plot(self.train_history['gamma_values'], label='Î³', color='red')
        axes[0, 1].axhline(y=0.234, color='red', linestyle='--', alpha=0.7, label='Î³ç†è«–å€¤')
        axes[0, 1].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
        axes[0, 1].set_ylabel('Î³å€¤')
        axes[0, 1].set_title('Î³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ')
        axes[0, 1].legend()
        
        axes[0, 2].plot(self.train_history['delta_values'], label='Î´', color='blue')
        axes[0, 2].axhline(y=0.035, color='blue', linestyle='--', alpha=0.7, label='Î´ç†è«–å€¤')
        axes[0, 2].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
        axes[0, 2].set_ylabel('Î´å€¤')
        axes[0, 2].set_title('Î´ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ')
        axes[0, 2].legend()
        
        # 3. è¶…åæŸå› å­ã®äºˆæ¸¬
        axes[1, 0].loglog(test_N_values, results['predictions'], 'b-', label='æ·±å±¤å­¦ç¿’äºˆæ¸¬', linewidth=2)
        
        # ç†è«–å€¤ã¨ã®æ¯”è¼ƒ
        gamma_true, delta_true, t_c_true = 0.234, 0.035, 17.26
        theoretical_values = []
        for N in test_N_values:
            integral = gamma_true * np.log(N / t_c_true)
            if N > t_c_true:
                integral += delta_true * (N - t_c_true)
            theoretical_values.append(np.exp(integral))
        
        axes[1, 0].loglog(test_N_values, theoretical_values, 'r--', label='ç†è«–å€¤', linewidth=2)
        axes[1, 0].set_xlabel('æ¬¡å…ƒæ•° N')
        axes[1, 0].set_ylabel('è¶…åæŸå› å­ S(N)')
        axes[1, 0].set_title('è¶…åæŸå› å­ã®äºˆæ¸¬vsç†è«–å€¤')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ
        axes[1, 1].hist(results['gamma_values'], bins=50, alpha=0.7, color='red', label='Î³åˆ†å¸ƒ')
        axes[1, 1].axvline(x=0.234, color='red', linestyle='--', linewidth=2, label='ç†è«–å€¤')
        axes[1, 1].set_xlabel('Î³å€¤')
        axes[1, 1].set_ylabel('é »åº¦')
        axes[1, 1].set_title('Î³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆ†å¸ƒ')
        axes[1, 1].legend()
        
        # 5. t_c ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ
        axes[1, 2].plot(self.train_history['tc_values'], label='t_c', color='green')
        axes[1, 2].axhline(y=17.26, color='green', linestyle='--', alpha=0.7, label='t_cç†è«–å€¤')
        axes[1, 2].set_xlabel('ã‚¨ãƒãƒƒã‚¯')
        axes[1, 2].set_ylabel('t_cå€¤')
        axes[1, 2].set_title('t_cãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åæŸ')
        axes[1, 2].legend()
        
        plt.tight_layout()
        plt.savefig('nkat_deep_learning_optimization_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("âœ… å¯è¦–åŒ–å®Œäº†")
    
    def save_model_and_results(self, results, filename_prefix='nkat_dl_optimization'):
        """
        ãƒ¢ãƒ‡ãƒ«ã¨çµæœã®ä¿å­˜
        
        Args:
            results: è©•ä¾¡çµæœ
            filename_prefix: ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        """
        print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜ä¸­...")
        
        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_history': self.train_history,
            'results': results
        }, f'{filename_prefix}_model.pth')
        
        # çµæœã‚’JSONå½¢å¼ã§ä¿å­˜
        json_results = {
            'optimal_parameters': {
                'gamma_mean': float(results['gamma_mean']),
                'gamma_std': float(results['gamma_std']),
                'delta_mean': float(results['delta_mean']),
                'delta_std': float(results['delta_std']),
                'tc_mean': float(results['tc_mean']),
                'tc_std': float(results['tc_std'])
            },
            'training_config': {
                'learning_rate': self.learning_rate,
                'batch_size': self.batch_size,
                'num_epochs': self.num_epochs
            },
            'final_losses': {
                'total_loss': self.train_history['total_loss'][-1],
                'data_loss': self.train_history['data_loss'][-1],
                'physics_loss': self.train_history['physics_loss'][-1],
                'reg_loss': self.train_history['reg_loss'][-1]
            }
        }
        
        with open(f'{filename_prefix}_results.json', 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ä¿å­˜å®Œäº†: {filename_prefix}_model.pth, {filename_prefix}_results.json")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ NKATæ·±å±¤å­¦ç¿’æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("="*60)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    optimizer = NKATDeepLearningOptimizer(
        learning_rate=1e-3,
        batch_size=64,
        num_epochs=2000
    )
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    dataloader = optimizer.generate_training_data(
        N_range=(10, 1000),
        num_samples=2000
    )
    
    # ãƒ¢ãƒ‡ãƒ«è¨“ç·´
    optimizer.train(dataloader)
    
    # ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
    test_N_values = np.logspace(1, 3, 100)
    results = optimizer.evaluate_model(test_N_values)
    
    # çµæœå¯è¦–åŒ–
    optimizer.visualize_results(test_N_values, results)
    
    # ãƒ¢ãƒ‡ãƒ«ã¨çµæœã®ä¿å­˜
    optimizer.save_model_and_results(results)
    
    # ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¸ã®å«æ„
    gamma_opt = results['gamma_mean']
    t_c_opt = results['tc_mean']
    riemann_convergence = gamma_opt * np.log(1000 / t_c_opt)
    riemann_deviation = abs(riemann_convergence - 0.5)
    
    print("\n" + "="*60)
    print("ğŸ¯ NKATæ·±å±¤å­¦ç¿’æœ€é©åŒ–çµæœ")
    print("="*60)
    print(f"ğŸ“Š æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    print(f"  Î³ = {results['gamma_mean']:.6f} Â± {results['gamma_std']:.6f}")
    print(f"  Î´ = {results['delta_mean']:.6f} Â± {results['delta_std']:.6f}")
    print(f"  t_c = {results['tc_mean']:.6f} Â± {results['tc_std']:.6f}")
    print(f"\nğŸ¯ ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¸ã®å«æ„:")
    print(f"  åæŸç‡: Î³Â·ln(1000/t_c) = {riemann_convergence:.6f}")
    print(f"  ç†è«–å€¤ã‹ã‚‰ã®åå·®: {riemann_deviation:.6f}")
    print(f"  ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ”¯æŒåº¦: {100*(1-min(riemann_deviation/0.1, 1.0)):.1f}%")
    
    print("\nğŸ æ·±å±¤å­¦ç¿’æœ€é©åŒ–å®Œäº†")

if __name__ == "__main__":
    main() 
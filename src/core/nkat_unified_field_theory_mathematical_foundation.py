#!/usr/bin/env python3
"""
NKATçµ±åˆç‰¹è§£ç†è«– æ•°å­¦çš„åŸºç›¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  v1.0
==================================================
çµ±åˆç‰¹è§£ Î¨_unified*(x) ã®æ•°å­¦çš„å³å¯†æ€§ã‚’æ®µéšçš„ã«æ¤œè¨¼

ä¸»è¦æ©Ÿèƒ½:
1. åæŸæ€§è§£æ (ãƒãƒ«ãƒ åæŸãƒ»åˆ†å¸ƒåæŸãƒ»ç‚¹æ¯åæŸ)
2. å¤šé‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ç‰¹æ€§ã®æ•°å€¤å®Ÿé¨“
3. ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã¨ãƒªãƒ¼ãƒãƒ³äºˆæƒ³é–¢é€£æ€§
4. é›»æºæ–­ä¿è­·ã‚·ã‚¹ãƒ†ãƒ 

Dependencies: mpmath, numpy, scipy, matplotlib, cupy (CUDA), tqdm
"""

import os
import sys
import json
import pickle
import signal
import uuid
import time
import threading
from datetime import datetime
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# æ•°å€¤è¨ˆç®—ãƒ»ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.animation import FuncAnimation
import seaborn as sns
from tqdm import tqdm

# é«˜ç²¾åº¦è¨ˆç®—
import mpmath as mp
mp.mp.dps = 100  # 100æ¡ç²¾åº¦

# ç§‘å­¦è¨ˆç®—
from scipy import special, optimize, integrate
from scipy.fft import fft, ifft
import scipy.stats as stats

# CUDAè¨ˆç®—ï¼ˆRTX3080å¯¾å¿œï¼‰
try:
    import cupy as cp
    CUDA_AVAILABLE = True
    print("ğŸš€ CUDA/RTX3080 åŠ é€Ÿãƒ¢ãƒ¼ãƒ‰ ON")
except ImportError:
    CUDA_AVAILABLE = False
    print("âš ï¸  CUDAç„¡åŠ¹ - CPUè¨ˆç®—ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")

class UnifiedFieldTheoryFoundation:
    """çµ±åˆç‰¹è§£ç†è«–ã®æ•°å­¦çš„åŸºç›¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, session_id=None):
        self.session_id = session_id or str(uuid.uuid4())[:8]
        self.start_time = datetime.now()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        self.n = 10  # åŸºåº•ãƒ¢ãƒ¼ãƒ‰æ•°
        self.L = 5   # ç©æ§‹é€ ã®æ¬¡æ•°
        self.k_max = 100  # ç„¡é™ç´šæ•°ã®æ‰“ã¡åˆ‡ã‚Š
        
        # çµ±åˆç‰¹è§£ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.lambda_q_star = None
        self.A_coeffs = None
        self.B_coeffs = None
        
        # çµæœä¿å­˜
        self.results = {
            'session_id': self.session_id,
            'timestamp': self.start_time.isoformat(),
            'convergence_analysis': {},
            'multifractal_analysis': {},
            'spectral_analysis': {},
            'riemann_connection': {}
        }
        
        # é›»æºæ–­ä¿è­·
        self._setup_emergency_save()
        self._setup_checkpoint_system()
        
        print(f"ğŸ”¬ çµ±åˆç‰¹è§£ç†è«–åŸºç›¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.session_id}")
        print(f"âš¡ ç²¾åº¦: {mp.mp.dps}æ¡")
        print(f"ğŸ¯ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: n={self.n}, L={self.L}, k_max={self.k_max}")

    def _setup_emergency_save(self):
        """ç·Šæ€¥ä¿å­˜ã‚·ã‚¹ãƒ†ãƒ """
        def emergency_handler(signum, frame):
            print(f"\nğŸš¨ ç·Šæ€¥ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³æ¤œå‡º (ã‚·ã‚°ãƒŠãƒ«: {signum})")
            self._emergency_save()
            sys.exit(1)
        
        # Windowså¯¾å¿œã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        signal.signal(signal.SIGINT, emergency_handler)
        signal.signal(signal.SIGTERM, emergency_handler)
        if hasattr(signal, 'SIGBREAK'):
            signal.signal(signal.SIGBREAK, emergency_handler)

    def _setup_checkpoint_system(self):
        """5åˆ†é–“éš”ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ """
        def checkpoint_loop():
            while True:
                time.sleep(300)  # 5åˆ†é–“éš”
                self._save_checkpoint()
        
        self.checkpoint_thread = threading.Thread(target=checkpoint_loop, daemon=True)
        self.checkpoint_thread.start()
        print("ğŸ›¡ï¸ é›»æºæ–­ä¿è­·ã‚·ã‚¹ãƒ†ãƒ æœ‰åŠ¹ (5åˆ†é–“éš”è‡ªå‹•ä¿å­˜)")

    def _emergency_save(self):
        """ç·Šæ€¥ä¿å­˜å®Ÿè¡Œ"""
        emergency_file = f"nkat_emergency_save_{self.session_id}_{int(time.time())}.pkl"
        try:
            with open(emergency_file, 'wb') as f:
                pickle.dump(self.results, f)
            print(f"ğŸ’¾ ç·Šæ€¥ä¿å­˜å®Œäº†: {emergency_file}")
        except Exception as e:
            print(f"âŒ ç·Šæ€¥ä¿å­˜å¤±æ•—: {e}")

    def _save_checkpoint(self):
        """å®šæœŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        checkpoint_file = f"nkat_checkpoint_{self.session_id}.pkl"
        try:
            with open(checkpoint_file, 'wb') as f:
                pickle.dump(self.results, f)
            print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {datetime.now().strftime('%H:%M:%S')}")
        except Exception as e:
            print(f"âš ï¸  ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def initialize_unified_solution_parameters(self):
        """çµ±åˆç‰¹è§£Î¨*ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–"""
        print("ğŸ”§ çµ±åˆç‰¹è§£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–ä¸­...")
        
        # Î»_q* ã®åˆæœŸåŒ–ï¼ˆãƒªãƒ¼ãƒãƒ³é›¶ç‚¹ã‚’æ¨¡å€£ï¼‰
        self.lambda_q_star = np.zeros(2*self.n + 1, dtype=complex)
        
        # æ—¢çŸ¥ã®ãƒªãƒ¼ãƒãƒ³é›¶ç‚¹ï¼ˆè™šéƒ¨ï¼‰ã‚’ä½¿ç”¨
        known_zeros = [14.134725, 21.022040, 25.010858, 30.424876, 32.935062,
                      37.586178, 40.918719, 43.327073, 48.005151, 49.773832]
        
        for q in range(min(len(known_zeros), 2*self.n + 1)):
            # Î»_q* = 1/2 + i*t_q ã®å½¢ã§è¨­å®š
            self.lambda_q_star[q] = 0.5 + 1j * known_zeros[q % len(known_zeros)]
        
        # A_{q,p,k}* ã®åˆæœŸåŒ–ï¼ˆåæŸã‚’ä¿è¨¼ã™ã‚‹æŒ‡æ•°æ¸›è¡°ï¼‰
        self.A_coeffs = np.zeros((2*self.n + 1, self.n, self.k_max), dtype=complex)
        for q in range(2*self.n + 1):
            for p in range(self.n):
                for k in range(self.k_max):
                    # æŒ‡æ•°æ¸›è¡°ã§åæŸã‚’ä¿è¨¼: A âˆ exp(-Î±k) 
                    alpha = 0.1 + 0.01 * p  # æ¸›è¡°ä¿‚æ•°
                    phase = np.random.uniform(0, 2*np.pi)
                    self.A_coeffs[q, p, k] = np.exp(-alpha * k) * np.exp(1j * phase)
        
        # B_{q,l}* ã®åˆæœŸåŒ–
        self.B_coeffs = np.zeros((2*self.n + 1, self.L + 1), dtype=complex)
        for q in range(2*self.n + 1):
            for l in range(self.L + 1):
                self.B_coeffs[q, l] = np.random.normal(0, 0.1) + 1j * np.random.normal(0, 0.1)
        
        print(f"âœ… ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–å®Œäº†")
        print(f"   Î»*: {self.lambda_q_star[:3]}...")
        print(f"   A*: shape={self.A_coeffs.shape}")
        print(f"   B*: shape={self.B_coeffs.shape}")

    def evaluate_unified_solution(self, x_values):
        """çµ±åˆç‰¹è§£ Î¨_unified*(x) ã®æ•°å€¤è©•ä¾¡
        
        Î¨*(x) = Î£_q e^(iÎ»_q*x) (Î£_{p,k} A_{q,p,k}* Ïˆ_{q,p,k}(x)) Î _l B_{q,l}* Î¦_l(x)
        """
        if self.lambda_q_star is None:
            self.initialize_unified_solution_parameters()
        
        x_values = np.asarray(x_values)
        psi_unified = np.zeros_like(x_values, dtype=complex)
        
        print("ğŸ§® çµ±åˆç‰¹è§£ã®æ•°å€¤è©•ä¾¡å®Ÿè¡Œä¸­...")
        
        for q in tqdm(range(2*self.n + 1), desc="ãƒ¢ãƒ¼ãƒ‰ q"):
            # æŒ‡æ•°é …: e^(iÎ»_q*x)
            exponential_term = np.exp(1j * self.lambda_q_star[q] * x_values)
            
            # å†…å´ã®å’Œ: Î£_{p,k} A_{q,p,k}* Ïˆ_{q,p,k}(x)
            inner_sum = np.zeros_like(x_values, dtype=complex)
            for p in range(self.n):
                for k in range(self.k_max):
                    # åŸºåº•é–¢æ•° Ïˆ_{q,p,k}(x) = ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆå¤šé …å¼ Ã— ã‚¬ã‚¦ã‚¹é–¢æ•°
                    psi_qpk = self._basis_function(x_values, q, p, k)
                    inner_sum += self.A_coeffs[q, p, k] * psi_qpk
            
            # å¤–å´ã®ç©: Î _l B_{q,l}* Î¦_l(x)
            product_term = np.ones_like(x_values, dtype=complex)
            for l in range(self.L + 1):
                phi_l = self._phi_function(x_values, l)
                product_term *= self.B_coeffs[q, l] * phi_l
            
            # ç·å’Œã«åŠ ç®—
            psi_unified += exponential_term * inner_sum * product_term
        
        return psi_unified

    def _basis_function(self, x, q, p, k):
        """åŸºåº•é–¢æ•° Ïˆ_{q,p,k}(x) = H_k((x-Î¼)/Ïƒ) * exp(-((x-Î¼)/Ïƒ)^2/2)"""
        mu = q * 0.1  # ä¸­å¿ƒã‚’ãšã‚‰ã™
        sigma = 1.0 + p * 0.1  # å¹…ã‚’å¤‰ãˆã‚‹
        
        normalized_x = (x - mu) / sigma
        
        # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆå¤šé …å¼ H_k
        hermite_vals = special.eval_hermite(k, normalized_x)
        
        # ã‚¬ã‚¦ã‚¹é–¢æ•°
        gaussian = np.exp(-normalized_x**2 / 2)
        
        # æ­£è¦åŒ–ä¿‚æ•°
        normalization = 1.0 / np.sqrt(2**k * special.factorial(k) * np.sqrt(np.pi) * sigma)
        
        return normalization * hermite_vals * gaussian

    def _phi_function(self, x, l):
        """Î¦_l(x) = ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•å¤šé …å¼ T_l(x/10)"""
        normalized_x = np.clip(x / 10.0, -1, 1)  # ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•ã®å®šç¾©åŸŸ[-1,1]
        return special.eval_chebyt(l, normalized_x)

    def analyze_convergence(self, x_range=(-10, 10), num_points=1000):
        """åæŸæ€§è§£æ"""
        print("ğŸ“Š åæŸæ€§è§£æé–‹å§‹...")
        
        x_values = np.linspace(x_range[0], x_range[1], num_points)
        
        # éƒ¨åˆ†å’Œã®åæŸã‚’èª¿ã¹ã‚‹
        k_max_values = [10, 25, 50, 75, 100]
        convergence_errors = []
        
        for k_max_test in tqdm(k_max_values, desc="åæŸè§£æ"):
            # ä¸€æ™‚çš„ã«k_maxã‚’å¤‰æ›´
            original_k_max = self.k_max
            self.k_max = k_max_test
            
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å†åˆæœŸåŒ–
            self.initialize_unified_solution_parameters()
            
            # è©•ä¾¡
            psi_values = self.evaluate_unified_solution(x_values)
            
            # ãƒãƒ«ãƒ è¨ˆç®—
            l2_norm = np.linalg.norm(psi_values)
            max_norm = np.max(np.abs(psi_values))
            
            convergence_errors.append({
                'k_max': k_max_test,
                'l2_norm': float(l2_norm),
                'max_norm': float(max_norm),
                'convergence_rate': float(l2_norm / k_max_test)
            })
            
            # å…ƒã®è¨­å®šã«æˆ»ã™
            self.k_max = original_k_max
        
        self.results['convergence_analysis'] = {
            'x_range': x_range,
            'convergence_data': convergence_errors,
            'analysis_complete': True
        }
        
        print("âœ… åæŸæ€§è§£æå®Œäº†")
        return convergence_errors

    def analyze_multifractal_properties(self, x_range=(-5, 5), num_points=500, q_values=None):
        """å¤šé‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ç‰¹æ€§è§£æ
        
        |Î¨*(y)|^{2q} âˆ¼ r^{Ï„(q)} ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°è§£æ
        """
        print("ğŸ” å¤šé‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ç‰¹æ€§è§£æé–‹å§‹...")
        
        if q_values is None:
            q_values = np.linspace(-3, 3, 13)
        
        x_values = np.linspace(x_range[0], x_range[1], num_points)
        psi_values = self.evaluate_unified_solution(x_values)
        
        # ç•°ãªã‚‹åŠå¾„rã§ã®è§£æ
        r_values = np.logspace(-2, 0, 20)  # 0.01 ã‹ã‚‰ 1.0
        tau_q_estimates = []
        
        for q in tqdm(q_values, desc="å¤šé‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ« q"):
            log_moments = []
            log_r_values = []
            
            for r in r_values:
                # åŠå¾„rã®ãƒœãƒ¼ãƒ«å†…ã§ã®ç©åˆ†è¿‘ä¼¼
                moments = []
                for center_idx in range(0, len(x_values), 20):  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    center = x_values[center_idx]
                    # ãƒœãƒ¼ãƒ« B(center, r) å†…ã®ç‚¹ã‚’é¸æŠ
                    mask = np.abs(x_values - center) <= r
                    if np.sum(mask) > 1:
                        local_psi = psi_values[mask]
                        # |Î¨|^{2q} ã®ç©åˆ†
                        moment = np.trapz(np.abs(local_psi)**(2*q), x_values[mask])
                        if moment > 0:
                            moments.append(moment)
                
                if len(moments) > 0:
                    avg_moment = np.mean(moments)
                    if avg_moment > 0:
                        log_moments.append(np.log(avg_moment))
                        log_r_values.append(np.log(r))
            
            # Ï„(q) ã®æ¨å®š: log(moment) âˆ¼ Ï„(q) * log(r)
            if len(log_moments) > 3:
                slope, intercept, r_value, p_value, std_err = stats.linregress(log_r_values, log_moments)
                tau_q_estimates.append({
                    'q': float(q),
                    'tau_q': float(slope),
                    'r_squared': float(r_value**2),
                    'p_value': float(p_value)
                })
        
        self.results['multifractal_analysis'] = {
            'q_values': q_values.tolist(),
            'tau_q_data': tau_q_estimates,
            'analysis_complete': True
        }
        
        print("âœ… å¤šé‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è§£æå®Œäº†")
        return tau_q_estimates

    def analyze_riemann_connection(self):
        """ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¨ã®é–¢ä¿‚æ€§è§£æ"""
        print("ğŸ”¬ ãƒªãƒ¼ãƒãƒ³äºˆæƒ³é–¢é€£æ€§è§£æé–‹å§‹...")
        
        # Î»_q* ã¨ãƒªãƒ¼ãƒãƒ³é›¶ç‚¹ã®æ¯”è¼ƒ
        riemann_analysis = {
            'lambda_star_values': [],
            'riemann_comparison': [],
            'critical_line_test': []
        }
        
        for q in range(2*self.n + 1):
            lambda_val = self.lambda_q_star[q]
            
            riemann_analysis['lambda_star_values'].append({
                'q': q,
                'lambda_real': float(lambda_val.real),
                'lambda_imag': float(lambda_val.imag),
                'on_critical_line': abs(lambda_val.real - 0.5) < 1e-10
            })
        
        # è¨ˆæ•°é–¢æ•°ã®æ¯”è¼ƒ (ç°¡æ˜“ç‰ˆ)
        T_max = 50.0
        lambda_count = sum(1 for lam in self.lambda_q_star if 0 < lam.imag <= T_max)
        
        # ãƒªãƒ¼ãƒãƒ³ã®è¨ˆæ•°é–¢æ•° N(T) = T/(2Ï€) log(T/(2Ï€)) - T/(2Ï€) + O(log T)
        riemann_count_approx = T_max/(2*np.pi) * np.log(T_max/(2*np.pi)) - T_max/(2*np.pi)
        
        riemann_analysis['counting_function'] = {
            'T_max': T_max,
            'lambda_count': lambda_count,
            'riemann_count_approx': float(riemann_count_approx),
            'agreement_ratio': float(lambda_count / riemann_count_approx) if riemann_count_approx > 0 else 0
        }
        
        self.results['riemann_connection'] = riemann_analysis
        
        print("âœ… ãƒªãƒ¼ãƒãƒ³é–¢é€£æ€§è§£æå®Œäº†")
        return riemann_analysis

    def create_visualization(self):
        """çµæœã®å¯è¦–åŒ–"""
        print("ğŸ“Š å¯è¦–åŒ–ã‚°ãƒ©ãƒ•ç”Ÿæˆä¸­...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(f'NKATçµ±åˆç‰¹è§£ç†è«– æ•°å­¦çš„åŸºç›¤æ¤œè¨¼çµæœ (Session: {self.session_id})', fontsize=16)
        
        # 1. çµ±åˆç‰¹è§£ã®å®Ÿéƒ¨ãƒ»è™šéƒ¨
        ax1 = axes[0, 0]
        x_plot = np.linspace(-5, 5, 200)
        psi_plot = self.evaluate_unified_solution(x_plot)
        
        ax1.plot(x_plot, psi_plot.real, 'b-', label='Re[Î¨*(x)]', linewidth=2)
        ax1.plot(x_plot, psi_plot.imag, 'r--', label='Im[Î¨*(x)]', linewidth=2)
        ax1.set_xlabel('x')
        ax1.set_ylabel('Î¨*(x)')
        ax1.set_title('çµ±åˆç‰¹è§£ Î¨_unified*(x)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. åæŸæ€§è§£æ
        ax2 = axes[0, 1]
        if 'convergence_analysis' in self.results and self.results['convergence_analysis']:
            conv_data = self.results['convergence_analysis']['convergence_data']
            k_values = [d['k_max'] for d in conv_data]
            l2_norms = [d['l2_norm'] for d in conv_data]
            
            ax2.semilogy(k_values, l2_norms, 'o-', color='green', linewidth=2, markersize=8)
            ax2.set_xlabel('k_max (ç´šæ•°æ‰“ã¡åˆ‡ã‚Š)')
            ax2.set_ylabel('L2ãƒãƒ«ãƒ ')
            ax2.set_title('åæŸæ€§è§£æ')
            ax2.grid(True, alpha=0.3)
        
        # 3. å¤šé‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ« Ï„(q)
        ax3 = axes[1, 0]
        if 'multifractal_analysis' in self.results and self.results['multifractal_analysis']:
            mf_data = self.results['multifractal_analysis']['tau_q_data']
            if mf_data:
                q_vals = [d['q'] for d in mf_data]
                tau_vals = [d['tau_q'] for d in mf_data]
                
                ax3.plot(q_vals, tau_vals, 'o-', color='purple', linewidth=2, markersize=6)
                ax3.set_xlabel('q')
                ax3.set_ylabel('Ï„(q)')
                ax3.set_title('å¤šé‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ«')
                ax3.grid(True, alpha=0.3)
        
        # 4. Î»*ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†å¸ƒ
        ax4 = axes[1, 1]
        lambda_real = [lam.real for lam in self.lambda_q_star]
        lambda_imag = [lam.imag for lam in self.lambda_q_star]
        
        ax4.scatter(lambda_real, lambda_imag, c='red', s=100, alpha=0.7, edgecolors='black')
        ax4.axvline(x=0.5, color='blue', linestyle='--', alpha=0.7, label='è‡¨ç•Œç·š Re(s)=1/2')
        ax4.set_xlabel('Re(Î»*)')
        ax4.set_ylabel('Im(Î»*)')
        ax4.set_title('Î»* ã‚¹ãƒšã‚¯ãƒˆãƒ«åˆ†å¸ƒ')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜
        filename = f"nkat_unified_field_theory_foundation_{self.session_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š å¯è¦–åŒ–ä¿å­˜: {filename}")
        
        return filename

    def save_results(self):
        """çµæœä¿å­˜"""
        # JSONä¿å­˜
        json_filename = f"nkat_unified_field_theory_foundation_{self.session_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        # è¤‡ç´ æ•°ã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½å½¢å¼ã«å¤‰æ›
        serializable_results = self.results.copy()
        
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False, default=str)
        
        # Pickleä¿å­˜ï¼ˆå®Œå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰
        pkl_filename = json_filename.replace('.json', '.pkl')
        with open(pkl_filename, 'wb') as f:
            pickle.dump({
                'results': self.results,
                'lambda_q_star': self.lambda_q_star,
                'A_coeffs': self.A_coeffs,
                'B_coeffs': self.B_coeffs,
                'session_metadata': {
                    'session_id': self.session_id,
                    'start_time': self.start_time,
                    'end_time': datetime.now(),
                    'n': self.n,
                    'L': self.L,
                    'k_max': self.k_max
                }
            }, f)
        
        print(f"ğŸ’¾ çµæœä¿å­˜å®Œäº†:")
        print(f"   JSON: {json_filename}")
        print(f"   PKL:  {pkl_filename}")
        
        return json_filename, pkl_filename

    def run_complete_analysis(self):
        """å®Œå…¨è§£æå®Ÿè¡Œ"""
        print(f"ğŸš€ çµ±åˆç‰¹è§£ç†è«– å®Œå…¨æ•°å­¦çš„æ¤œè¨¼é–‹å§‹")
        print(f"ğŸ“… é–‹å§‹æ™‚åˆ»: {self.start_time}")
        print("=" * 60)
        
        try:
            # 1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆæœŸåŒ–
            self.initialize_unified_solution_parameters()
            
            # 2. åæŸæ€§è§£æ
            convergence_results = self.analyze_convergence()
            
            # 3. å¤šé‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«è§£æ  
            multifractal_results = self.analyze_multifractal_properties()
            
            # 4. ãƒªãƒ¼ãƒãƒ³äºˆæƒ³é–¢é€£æ€§
            riemann_results = self.analyze_riemann_connection()
            
            # 5. å¯è¦–åŒ–
            plot_filename = self.create_visualization()
            
            # 6. çµæœä¿å­˜
            json_file, pkl_file = self.save_results()
            
            # å®Œäº†å ±å‘Š
            end_time = datetime.now()
            duration = end_time - self.start_time
            
            print("=" * 60)
            print("ğŸ‰ çµ±åˆç‰¹è§£ç†è«– æ•°å­¦çš„åŸºç›¤æ¤œè¨¼ å®Œäº†!")
            print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {duration}")
            print(f"ğŸ“Š å¯è¦–åŒ–: {plot_filename}")
            print(f"ğŸ’¾ çµæœ: {json_file}")
            print("=" * 60)
            
            # è¦ç´„å‡ºåŠ›
            print("\nğŸ“‹ è§£æçµæœã‚µãƒãƒªãƒ¼:")
            if convergence_results:
                final_norm = convergence_results[-1]['l2_norm']
                print(f"   ğŸ”„ åæŸæ€§: L2ãƒãƒ«ãƒ  = {final_norm:.6f}")
            
            if multifractal_results:
                tau_range = max(d['tau_q'] for d in multifractal_results) - min(d['tau_q'] for d in multifractal_results)
                print(f"   ğŸ“ˆ å¤šé‡ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«: Ï„(q)ç¯„å›² = {tau_range:.4f}")
            
            if riemann_results:
                agreement = riemann_results['counting_function']['agreement_ratio']
                print(f"   ğŸ¯ ãƒªãƒ¼ãƒãƒ³ä¸€è‡´åº¦: {agreement:.3f}")
            
            return {
                'success': True,
                'session_id': self.session_id,
                'duration': str(duration),
                'files': {
                    'plot': plot_filename,
                    'json': json_file,
                    'pkl': pkl_file
                }
            }
            
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            self._emergency_save()
            raise

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸŒŸ" * 30)
    print("NKATçµ±åˆç‰¹è§£ç†è«– æ•°å­¦çš„åŸºç›¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ")
    print("Mathematical Foundation Verification for Unified Field Theory")
    print("ğŸŒŸ" * 30)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    foundation = UnifiedFieldTheoryFoundation()
    
    # å®Œå…¨è§£æå®Ÿè¡Œ
    results = foundation.run_complete_analysis()
    
    if results['success']:
        print(f"\nâœ¨ æ•°å­¦çš„åŸºç›¤æ¤œè¨¼å®Œäº†! ã‚»ãƒƒã‚·ãƒ§ãƒ³: {results['session_id']}")
    
    return results

if __name__ == "__main__":
    results = main() 
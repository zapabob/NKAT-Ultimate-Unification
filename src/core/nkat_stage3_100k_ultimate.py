#!/usr/bin/env python3
"""
ğŸ”¥ NKAT Stage 3: 100,000ã‚¼ãƒ­ç‚¹åˆ†æ•£å‡¦ç†è¶…å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ 
=================================================
åˆ†æ•£å‡¦ç†ãƒ»ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ»è¶…ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æ±ºã¸ã®æ±ºå®šçš„ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—
"""

import os
import sys
import json
import time
import signal
import pickle
import warnings
import threading
import multiprocessing as mp
from datetime import datetime
from pathlib import Path
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.decomposition import PCA, IncrementalPCA
from sklearn.preprocessing import PolynomialFeatures
from tqdm import tqdm
import gc
import psutil
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor

# GPUè¨­å®š
import torch
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# æ•°å€¤è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import mpmath
    mpmath.mp.dps = 50  # 50æ¡ç²¾åº¦
    MPMATH_AVAILABLE = True
    print("ğŸ”¢ mpmath 50æ¡ç²¾åº¦: æœ‰åŠ¹")
except ImportError:
    MPMATH_AVAILABLE = False
    print("âš ï¸ mpmathç„¡åŠ¹")

# GPUç¢ºèª
CUDA_AVAILABLE = torch.cuda.is_available()
if CUDA_AVAILABLE:
    device = torch.device('cuda')
    print(f"ğŸš€ CUDA RTX3080 GPUåŠ é€Ÿ: æœ‰åŠ¹")
    torch.cuda.set_device(0)
else:
    device = torch.device('cpu')
    print("âš ï¸ CUDAç„¡åŠ¹")

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
CPU_COUNT = mp.cpu_count()
MEMORY_GB = psutil.virtual_memory().total / (1024**3)
print(f"ğŸ’» CPU ã‚³ã‚¢æ•°: {CPU_COUNT}")
print(f"ğŸ’¾ ç·ãƒ¡ãƒ¢ãƒª: {MEMORY_GB:.1f}GB")

warnings.filterwarnings('ignore')

class DistributedZeroCalculator:
    """åˆ†æ•£ã‚¼ãƒ­ç‚¹è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    @staticmethod
    def calculate_zero_batch(args):
        """ãƒãƒƒãƒã§ã‚¼ãƒ­ç‚¹è¨ˆç®—ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰"""
        start_n, batch_size, process_id = args
        
        # ãƒ—ãƒ­ã‚»ã‚¹æ¯ã«mpmathåˆæœŸåŒ–
        if MPMATH_AVAILABLE:
            mpmath.mp.dps = 50
        
        zeros = []
        for i in range(batch_size):
            try:
                n = start_n + i
                if MPMATH_AVAILABLE:
                    zero = mpmath.zetazero(n)
                    zeros.append((n, complex(zero)))
                else:
                    # ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿
                    zeros.append((n, complex(0.5, 14.134725 + n)))
            except Exception as e:
                print(f"âš ï¸ ãƒ—ãƒ­ã‚»ã‚¹{process_id}: ã‚¼ãƒ­ç‚¹{n}è¨ˆç®—ã‚¨ãƒ©ãƒ¼")
                continue
        
        return zeros

class NKAT_Stage3_UltimateSystem:
    def __init__(self, target_zeros=100000, batch_size=5000, checkpoint_interval=10000):
        """NKAT Stage3 è¶…å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        self.target_zeros = target_zeros
        self.batch_size = batch_size
        self.checkpoint_interval = checkpoint_interval
        self.zeros = []
        self.features = None
        self.models = {}
        self.scalers = {}
        self.results = {}
        self.current_progress = 0
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰å›ºå®š
        np.random.seed(42)
        torch.manual_seed(42)
        if CUDA_AVAILABLE:
            torch.cuda.manual_seed(42)
        print("ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰å›ºå®š: 42")
        
        # GPUåˆæœŸåŒ–
        if CUDA_AVAILABLE:
            self.device = torch.device('cuda:0')
            torch.cuda.empty_cache()
            print(f"ğŸ”¥ GPUåˆæœŸåŒ–å®Œäº†: {self.device}")
        else:
            self.device = torch.device('cpu')
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"nkat_stage3_100k_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # é›»æºæ–­å¯¾å¿œ
        self.setup_signal_handlers()
        print("ğŸ›¡ï¸ é›»æºæ–­å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ : æœ‰åŠ¹")
        
        # åˆ†æ•£å‡¦ç†è¨­å®š
        self.num_processes = min(CPU_COUNT, 8)  # æœ€å¤§8ãƒ—ãƒ­ã‚»ã‚¹
        print(f"ğŸ”€ åˆ†æ•£å‡¦ç†: {self.num_processes}ãƒ—ãƒ­ã‚»ã‚¹")
        
    def setup_signal_handlers(self):
        """é›»æºæ–­ãƒ»ç•°å¸¸çµ‚äº†å¯¾å¿œ"""
        def emergency_save(signum, frame):
            print(f"\nğŸš¨ ç·Šæ€¥ä¿å­˜é–‹å§‹ (Signal: {signum})")
            self.save_checkpoint(emergency=True)
            sys.exit(1)
        
        signal.signal(signal.SIGINT, emergency_save)
        signal.signal(signal.SIGTERM, emergency_save)
        if hasattr(signal, 'SIGBREAK'):
            signal.signal(signal.SIGBREAK, emergency_save)
    
    def save_checkpoint(self, emergency=False):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            checkpoint_data = {
                'zeros': self.zeros,
                'target_zeros': self.target_zeros,
                'current_progress': self.current_progress,
                'timestamp': timestamp,
                'memory_usage_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                'computed_zeros': len(self.zeros),
                'emergency': emergency
            }
            
            # JSONå½¢å¼ã§ä¿å­˜
            if emergency:
                checkpoint_file = self.checkpoint_dir / f"emergency_{timestamp}.json"
            else:
                checkpoint_file = self.checkpoint_dir / f"checkpoint_{len(self.zeros)}_{timestamp}.json"
            
            with open(checkpoint_file, 'w') as f:
                json.dump(checkpoint_data, f, indent=2, default=str)
            
            # Pickleã§ã‚‚ä¿å­˜ï¼ˆé«˜é€Ÿèª­ã¿è¾¼ã¿ç”¨ï¼‰
            pickle_file = checkpoint_file.with_suffix('.pkl')
            with open(pickle_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            
            print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_file}")
            return checkpoint_file
            
        except Exception as e:
            print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å¤±æ•—: {e}")
            return None
    
    def load_checkpoint(self, checkpoint_file):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        try:
            if checkpoint_file.suffix == '.pkl':
                with open(checkpoint_file, 'rb') as f:
                    data = pickle.load(f)
            else:
                with open(checkpoint_file, 'r') as f:
                    data = json.load(f)
            
            self.zeros = data.get('zeros', [])
            self.current_progress = data.get('current_progress', 0)
            
            print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {len(self.zeros)}ã‚¼ãƒ­ç‚¹å¾©æ—§")
            return True
            
        except Exception as e:
            print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return False
    
    def calculate_riemann_zeros_distributed(self):
        """åˆ†æ•£å‡¦ç†ã§ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—"""
        print(f"ğŸ”¢ åˆ†æ•£å‡¦ç†ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—é–‹å§‹...")
        print(f"   ç›®æ¨™ã‚¼ãƒ­ç‚¹æ•°: {self.target_zeros:,}")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size:,}")
        print(f"   ãƒ—ãƒ­ã‚»ã‚¹æ•°: {self.num_processes}")
        print(f"   ç²¾åº¦è¨­å®š: 50æ¡")
        
        zeros = []
        start_n = 1
        
        # åˆ†æ•£å‡¦ç†ã§ãƒãƒƒãƒè¨ˆç®—
        total_batches = (self.target_zeros + self.batch_size - 1) // self.batch_size
        
        with tqdm(total=total_batches, desc="åˆ†æ•£ãƒãƒƒãƒå‡¦ç†") as pbar:
            with ProcessPoolExecutor(max_workers=self.num_processes) as executor:
                # ãƒãƒƒãƒã‚¿ã‚¹ã‚¯ç”Ÿæˆ
                tasks = []
                for batch_idx in range(total_batches):
                    current_start = start_n + batch_idx * self.batch_size
                    current_batch_size = min(self.batch_size, self.target_zeros - len(zeros))
                    
                    if current_batch_size <= 0:
                        break
                    
                    task_args = (current_start, current_batch_size, batch_idx)
                    tasks.append(executor.submit(DistributedZeroCalculator.calculate_zero_batch, task_args))
                
                # çµæœåé›†
                for i, future in enumerate(tasks):
                    try:
                        batch_zeros = future.result(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        
                        # ã‚¼ãƒ­ç‚¹ã‚’ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆ
                        batch_zeros.sort(key=lambda x: x[0])
                        zeros.extend([z[1] for z in batch_zeros])
                        
                        pbar.update(1)
                        pbar.set_postfix({
                            'zeros': len(zeros),
                            'memory': f"{psutil.Process().memory_info().rss / 1024 / 1024:.1f}MB"
                        })
                        
                        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                        if len(zeros) % self.checkpoint_interval == 0:
                            self.zeros = zeros
                            self.current_progress = len(zeros)
                            self.save_checkpoint()
                        
                        # ãƒ¡ãƒ¢ãƒªç®¡ç†
                        if len(zeros) % (self.batch_size * 2) == 0:
                            gc.collect()
                            if CUDA_AVAILABLE:
                                torch.cuda.empty_cache()
                    
                    except Exception as e:
                        print(f"âš ï¸ ãƒãƒƒãƒ{i}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        continue
        
        print(f"âœ… åˆ†æ•£ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—å®Œäº†: {len(zeros):,}å€‹")
        return zeros
    
    def incremental_feature_engineering(self, zeros):
        """ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆè¶…å¤§è¦æ¨¡å¯¾å¿œï¼‰"""
        print(f"ğŸ”¬ ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...")
        print(f"   ã‚¼ãƒ­ç‚¹æ•°: {len(zeros):,}")
        
        # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«PCAè¨­å®š
        ipca = IncrementalPCA(n_components=100, batch_size=1000)
        
        # ãƒãƒƒãƒå‡¦ç†ã§ç‰¹å¾´æŠ½å‡ºã¨PCA
        batch_size = 2000  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        all_features = []
        
        print("ğŸ”— ãƒãƒƒãƒå‡¦ç†é–‹å§‹...")
        with tqdm(total=len(zeros), desc="ç‰¹å¾´æŠ½å‡º+PCA") as pbar:
            for i in range(0, len(zeros), batch_size):
                batch_zeros = zeros[i:i+batch_size]
                
                # åŸºæœ¬ç‰¹å¾´æŠ½å‡º
                batch_features = []
                for zero in batch_zeros:
                    t = zero.imag
                    features = [
                        t,  # è™šéƒ¨
                        1.0 / (2 * np.log(t)),  # ãƒªãƒ¼ãƒãƒ³ä»®èª¬æ­£è¦åŒ–
                        t / (2 * np.pi),  # Gramç‚¹è¿‘ä¼¼
                        np.log(t),  # å¯¾æ•°
                        np.sqrt(t),  # å¹³æ–¹æ ¹
                        t**2,  # 2ä¹—
                        np.sin(t),  # ä¸‰è§’é–¢æ•°
                        np.cos(t),
                        t * np.log(t),  # è¤‡åˆé …
                        t / np.log(t),
                        np.log(np.log(t)) if t > np.e else 0,  # äºŒé‡å¯¾æ•°
                        t**(1/3),  # ç«‹æ–¹æ ¹
                        1.0 / t,  # é€†æ•°
                        t / np.sqrt(np.log(t)) if t > 1 else 0,  # Hardy Zé–¢æ•°è¿‘ä¼¼
                        # è¿½åŠ ç‰¹å¾´
                        np.exp(-t/1000),  # æŒ‡æ•°æ¸›è¡°
                        t % (2*np.pi),  # å‘¨æœŸç‰¹å¾´
                        np.log10(t) if t > 0 else 0,  # å¸¸ç”¨å¯¾æ•°
                        t**(2/3),  # 2/3ä¹—
                    ]
                    batch_features.append(features)
                
                batch_features = np.array(batch_features)
                
                # å¤šé …å¼ç‰¹å¾´æ‹¡å¼µï¼ˆæ¬¡æ•°ä¸‹ã’ã¦åŠ¹ç‡åŒ–ï¼‰
                poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
                try:
                    poly_features = poly.fit_transform(batch_features)
                    
                    # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«PCAé©ç”¨
                    if i == 0:
                        ipca.partial_fit(poly_features)
                    else:
                        ipca.partial_fit(poly_features)
                    
                    # PCAå¤‰æ›
                    pca_features = ipca.transform(poly_features)
                    all_features.append(pca_features)
                    
                except Exception as e:
                    print(f"âš ï¸ ãƒãƒƒãƒ{i//batch_size}ç‰¹å¾´å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    # ã‚¨ãƒ©ãƒ¼æ™‚ã¯åŸºæœ¬ç‰¹å¾´ã®ã¿ä½¿ç”¨
                    pca_features = ipca.transform(batch_features[:, :ipca.n_components_])
                    all_features.append(pca_features)
                
                pbar.update(len(batch_zeros))
                
                # ãƒ¡ãƒ¢ãƒªç®¡ç†
                del batch_features, poly_features
                gc.collect()
        
        # æœ€çµ‚ç‰¹å¾´çµåˆ
        final_features = np.vstack(all_features)
        print(f"âœ… ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {final_features.shape}")
        print(f"   ç´¯ç©å¯„ä¸ç‡: {ipca.explained_variance_ratio_.sum():.3f}")
        
        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
        del all_features
        gc.collect()
        
        return final_features, ipca
    
    def create_balanced_labels(self, n_samples):
        """ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ãƒ©ãƒ™ãƒ«ä½œæˆ"""
        # 85%ã‚’çœŸã®ã‚¼ãƒ­ç‚¹ã€15%ã‚’å½ã¨ã—ã¦è¨­å®šï¼ˆã‚ˆã‚Šå³ã—ã„è¨­å®šï¼‰
        n_positive = int(n_samples * 0.85)
        n_negative = n_samples - n_positive
        
        labels = np.array([1.0] * n_positive + [0.0] * n_negative)
        np.random.shuffle(labels)
        
        print(f"ğŸ“Š ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {{0.0: {np.sum(labels == 0)}, 1.0: {np.sum(labels == 1)}}}")
        return labels
    
    def train_ensemble_models(self, X_train, y_train):
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆå¤§è¦æ¨¡å¯¾å¿œï¼‰"""
        print("ğŸ”¬ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
        
        models = {
            'RandomForest_Large': RandomForestClassifier(
                n_estimators=300, 
                max_depth=20, 
                min_samples_split=10,
                random_state=42,
                n_jobs=-1,
                max_features='sqrt'
            ),
            'GradientBoosting_Optimized': GradientBoostingClassifier(
                n_estimators=300, 
                max_depth=10,
                learning_rate=0.1,
                subsample=0.8,
                random_state=42
            ),
            'SVM_RBF': SVC(
                kernel='rbf', 
                C=10.0,
                gamma='scale',
                probability=True, 
                random_state=42,
                cache_size=2000
            ),
            'SVM_Poly': SVC(
                kernel='poly',
                degree=3,
                C=1.0,
                probability=True,
                random_state=42,
                cache_size=2000
            )
        }
        
        trained_models = {}
        scalers = {}
        
        # ä¸¦åˆ—è¨“ç·´
        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = {}
            
            for name, model in models.items():
                print(f"   {name}è¨“ç·´é–‹å§‹...")
                
                # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
                scaler = StandardScaler()
                X_scaled = scaler.fit_transform(X_train)
                
                # ä¸¦åˆ—è¨“ç·´æŠ•å…¥
                future = executor.submit(self._train_single_model, model, X_scaled, y_train)
                futures[name] = (future, scaler)
            
            # çµæœåé›†
            for name, (future, scaler) in futures.items():
                try:
                    trained_model = future.result(timeout=1800)  # 30åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    trained_models[name] = trained_model
                    scalers[name] = scaler
                    print(f"   âœ… {name}è¨“ç·´å®Œäº†")
                except Exception as e:
                    print(f"   âŒ {name}è¨“ç·´å¤±æ•—: {e}")
                
                # ãƒ¡ãƒ¢ãƒªç®¡ç†
                gc.collect()
        
        return trained_models, scalers
    
    def _train_single_model(self, model, X_scaled, y_train):
        """å˜ä¸€ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰"""
        model.fit(X_scaled, y_train)
        return model
    
    def comprehensive_evaluation(self, models, scalers, X_test, y_test):
        """åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
        print("ğŸ“Š åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹...")
        
        results = {}
        print("=" * 100)
        print(f"{'Model':<20} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'ROC-AUC':<10}")
        print("=" * 100)
        
        for name, model in models.items():
            try:
                scaler = scalers[name]
                X_scaled = scaler.transform(X_test)
                
                y_pred = model.predict(X_scaled)
                y_proba = model.predict_proba(X_scaled)[:, 1]
                
                metrics = {
                    'accuracy': accuracy_score(y_test, y_pred),
                    'precision': precision_score(y_test, y_pred),
                    'recall': recall_score(y_test, y_pred),
                    'f1': f1_score(y_test, y_pred),
                    'roc_auc': roc_auc_score(y_test, y_proba)
                }
                
                results[name] = metrics
                
                print(f"{name:<20} {metrics['accuracy']:<10.4f} {metrics['precision']:<10.4f} "
                      f"{metrics['recall']:<10.4f} {metrics['f1']:<10.4f} {metrics['roc_auc']:<10.4f}")
            
            except Exception as e:
                print(f"âŒ {name}è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print("=" * 100)
        
        if results:
            # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
            best_model = max(results.keys(), key=lambda k: results[k]['roc_auc'])
            print(f"ğŸ† æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_model} (ROC-AUC: {results[best_model]['roc_auc']:.4f})")
            
            # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ€§èƒ½äºˆæ¸¬
            ensemble_auc = np.mean([r['roc_auc'] for r in results.values()])
            print(f"ğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æœŸå¾…æ€§èƒ½: {ensemble_auc:.4f}")
        
        return results, best_model if results else None
    
    def ultimate_real_time_prediction(self, models, scalers, ipca, n_predictions=50):
        """ç©¶æ¥µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """
        print("ğŸ”® ç©¶æ¥µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–°è¦ã‚¼ãƒ­ç‚¹äºˆæ¸¬é–‹å§‹...")
        
        # æ–°è¦ã‚¼ãƒ­ç‚¹å€™è£œç”Ÿæˆï¼ˆã‚ˆã‚Šå¤§ããªç¯„å›²ï¼‰
        start_n = len(self.zeros) + 10000  # ã‚ˆã‚Šé ãã®ã‚¼ãƒ­ç‚¹
        new_zeros = []
        
        print(f"   æ–°è¦ã‚¼ãƒ­ç‚¹è¨ˆç®—é–‹å§‹: {start_n}ç•ªç›®ã‹ã‚‰{n_predictions}å€‹")
        
        for i in tqdm(range(n_predictions), desc="æ–°è¦ã‚¼ãƒ­ç‚¹è¨ˆç®—"):
            try:
                if MPMATH_AVAILABLE:
                    zero = mpmath.zetazero(start_n + i)
                    new_zeros.append(complex(zero))
                else:
                    # ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿
                    new_zeros.append(complex(0.5, 14.134725 + start_n + i))
            except:
                continue
        
        if not new_zeros:
            print("âŒ æ–°è¦ã‚¼ãƒ­ç‚¹è¨ˆç®—å¤±æ•—")
            return [], []
        
        # ç‰¹å¾´æŠ½å‡º
        new_features = []
        for zero in new_zeros:
            t = zero.imag
            features = [
                t, 1.0 / (2 * np.log(t)), t / (2 * np.pi), np.log(t),
                np.sqrt(t), t**2, np.sin(t), np.cos(t),
                t * np.log(t), t / np.log(t), 
                np.log(np.log(t)) if t > np.e else 0,
                t**(1/3), 1.0 / t, t / np.sqrt(np.log(t)) if t > 1 else 0,
                np.exp(-t/1000), t % (2*np.pi), np.log10(t) if t > 0 else 0, t**(2/3)
            ]
            new_features.append(features)
        
        new_features = np.array(new_features)
        
        # å¤šé …å¼ç‰¹å¾´æ‹¡å¼µ
        poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
        poly_features = poly.fit_transform(new_features)
        
        # ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«PCAå¤‰æ›
        pca_features = ipca.transform(poly_features)
        
        # è¶…é«˜æ€§èƒ½ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
        predictions = []
        confidence_scores = []
        
        for i, zero in enumerate(new_zeros):
            ensemble_proba = 0.0
            individual_probas = []
            valid_models = 0
            
            for name, model in models.items():
                try:
                    scaler = scalers[name]
                    X_scaled = scaler.transform(pca_features[i:i+1])
                    proba = model.predict_proba(X_scaled)[0, 1]
                    individual_probas.append(proba)
                    ensemble_proba += proba
                    valid_models += 1
                except:
                    continue
            
            if valid_models > 0:
                ensemble_proba /= valid_models
                # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆåˆ†æ•£ã«åŸºã¥ãï¼‰
                confidence = 1.0 - np.std(individual_probas) if len(individual_probas) > 1 else 0.5
            else:
                ensemble_proba = 0.5
                confidence = 0.0
            
            predictions.append(ensemble_proba)
            confidence_scores.append(confidence)
        
        print(f"âœ… ç©¶æ¥µäºˆæ¸¬å®Œäº†: {len(new_zeros)}å€‹ã®å€™è£œ")
        
        # é«˜ä¿¡é ¼åº¦çµæœã®ã¿è¡¨ç¤º
        high_confidence_threshold = 0.7
        high_conf_indices = [i for i, conf in enumerate(confidence_scores) if conf >= high_confidence_threshold]
        
        if high_conf_indices:
            print(f"ğŸ¯ é«˜ä¿¡é ¼åº¦äºˆæ¸¬ (ä¿¡é ¼åº¦â‰¥{high_confidence_threshold}): {len(high_conf_indices)}å€‹")
            
            for idx in high_conf_indices[:10]:  # ä¸Šä½10å€‹è¡¨ç¤º
                zero = new_zeros[idx]
                prob = predictions[idx]
                conf = confidence_scores[idx]
                status = "âœ… çœŸã®ã‚¼ãƒ­ç‚¹" if prob > 0.5 else "âŒ å½ã®ã‚¼ãƒ­ç‚¹"
                
                print(f"   ã‚¼ãƒ­ç‚¹{idx+1}: {zero}")
                print(f"   ãƒªãƒ¼ãƒãƒ³ä»®èª¬ç¢ºç‡: {prob:.4f}")
                print(f"   ä¿¡é ¼åº¦: {conf:.4f}")
                print(f"   åˆ¤å®š: {status}")
        else:
            print("âš ï¸ é«˜ä¿¡é ¼åº¦äºˆæ¸¬ãªã— - å…¨çµæœè¡¨ç¤º:")
            for i, (zero, prob, conf) in enumerate(zip(new_zeros[:10], predictions[:10], confidence_scores[:10])):
                status = "âœ… çœŸã®ã‚¼ãƒ­ç‚¹" if prob > 0.5 else "âŒ å½ã®ã‚¼ãƒ­ç‚¹"
                print(f"   ã‚¼ãƒ­ç‚¹{i+1}: {zero}")
                print(f"   ãƒªãƒ¼ãƒãƒ³ä»®èª¬ç¢ºç‡: {prob:.4f}")
                print(f"   ä¿¡é ¼åº¦: {conf:.4f}")
                print(f"   åˆ¤å®š: {status}")
        
        return new_zeros, predictions, confidence_scores
    
    def save_final_results(self, results, best_model, execution_time, confidence_scores=None):
        """æœ€çµ‚çµæœä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        final_results = {
            'timestamp': timestamp,
            'stage': 'Stage3_Ultimate',
            'target_zeros': self.target_zeros,
            'computed_zeros': len(self.zeros),
            'execution_time_seconds': execution_time,
            'execution_time_hours': execution_time / 3600,
            'best_model': best_model,
            'model_results': results,
            'confidence_analysis': {
                'mean_confidence': np.mean(confidence_scores) if confidence_scores else None,
                'std_confidence': np.std(confidence_scores) if confidence_scores else None,
                'high_confidence_count': len([c for c in confidence_scores if c >= 0.7]) if confidence_scores else None
            },
            'system_info': {
                'cuda_available': CUDA_AVAILABLE,
                'mpmath_available': MPMATH_AVAILABLE,
                'cpu_count': CPU_COUNT,
                'memory_total_gb': MEMORY_GB,
                'memory_peak_mb': psutil.Process().memory_info().rss / 1024 / 1024,
                'distributed_processes': self.num_processes
            },
            'performance_metrics': {
                'zeros_per_second': len(self.zeros) / execution_time if execution_time > 0 else 0,
                'memory_efficiency': len(self.zeros) / (psutil.Process().memory_info().rss / 1024 / 1024),
                'scalability_score': (len(self.zeros) / 1000) * (1 / max(execution_time / 3600, 0.1))
            }
        }
        
        # JSONä¿å­˜
        results_file = self.output_dir / f"stage3_ultimate_results_{timestamp}.json"
        with open(results_file, 'w') as f:
            json.dump(final_results, f, indent=2, default=str)
        
        print(f"ğŸ’¾ æœ€çµ‚çµæœä¿å­˜: {results_file}")
        return final_results
    
    def run_ultimate_analysis(self):
        """ç©¶æ¥µè§£æå®Ÿè¡Œ"""
        start_time = time.time()
        
        print("ğŸŒŸ NKAT Stage3 ç©¶æ¥µ100,000ã‚¼ãƒ­ç‚¹ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ¯ ç›®æ¨™: {self.target_zeros:,}ã‚¼ãƒ­ç‚¹å‡¦ç†")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print(f"ğŸ”€ åˆ†æ•£å‡¦ç†: {self.num_processes}ãƒ—ãƒ­ã‚»ã‚¹")
        print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”: {self.checkpoint_interval:,}ã‚¼ãƒ­ç‚¹")
        print()
        
        print("ğŸŒŸ NKAT Stage3 ç©¶æ¥µ100,000ã‚¼ãƒ­ç‚¹å®Œå…¨è§£æé–‹å§‹!")
        print(f"   åˆ†æ•£ãƒãƒƒãƒå‡¦ç†: {self.batch_size:,}ã‚¼ãƒ­ç‚¹/ãƒãƒƒãƒ")
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: åˆ†æ•£ã‚¼ãƒ­ç‚¹è¨ˆç®—
        print("ğŸ”¢ ã‚¹ãƒ†ãƒƒãƒ—1: åˆ†æ•£é«˜ç²¾åº¦ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—")
        if MPMATH_AVAILABLE:
            self.zeros = self.calculate_riemann_zeros_distributed()
        else:
            print("âŒ mpmathç„¡åŠ¹ã®ãŸã‚ã€ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨")
            self.zeros = [complex(0.5, 14.134725 + i) for i in range(self.target_zeros)]
        print()
        
        # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        self.current_progress = len(self.zeros)
        self.save_checkpoint()
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        print("ğŸ”¬ ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ã‚¿ãƒ«ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
        features, ipca = self.incremental_feature_engineering(self.zeros)
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ©ãƒ™ãƒ«ä½œæˆ
        print("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—3: ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ãƒ‡ãƒ¼ã‚¿åˆ†å‰²")
        labels = self.create_balanced_labels(len(features))
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            features, labels, test_size=0.2, random_state=42, stratify=labels
        )
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        print("ğŸ”¬ ã‚¹ãƒ†ãƒƒãƒ—4: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
        self.models, self.scalers = self.train_ensemble_models(X_train, y_train)
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—5: åŒ…æ‹¬çš„è©•ä¾¡
        print("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—5: åŒ…æ‹¬çš„ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
        results, best_model = self.comprehensive_evaluation(self.models, self.scalers, X_test, y_test)
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—6: ç©¶æ¥µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬
        print("ğŸ”® ã‚¹ãƒ†ãƒƒãƒ—6: ç©¶æ¥µãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
        confidence_scores = None
        if MPMATH_AVAILABLE and self.models:
            new_zeros, predictions, confidence_scores = self.ultimate_real_time_prediction(
                self.models, self.scalers, ipca, n_predictions=50
            )
        print()
        
        # å®Ÿè¡Œæ™‚é–“è¨ˆç®—
        execution_time = time.time() - start_time
        
        # æœ€çµ‚çµæœä¿å­˜
        final_results = self.save_final_results(results, best_model, execution_time, confidence_scores)
        
        # ç©¶æ¥µæœ€çµ‚å ±å‘Š
        print("ğŸ‰ NKAT Stage3 ç©¶æ¥µ100,000ã‚¼ãƒ­ç‚¹è§£æå®Œäº†!")
        print(f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’ ({execution_time/3600:.2f}æ™‚é–“)")
        print(f"ğŸ”¢ å‡¦ç†ã‚¼ãƒ­ç‚¹æ•°: {len(self.zeros):,}")
        print(f"ğŸ§  è¨“ç·´ãƒ¢ãƒ‡ãƒ«æ•°: {len(self.models)}")
        if results and best_model:
            print(f"ğŸ† æœ€é«˜ROC-AUC: {results[best_model]['roc_auc']:.4f}")
        print(f"ğŸš€ å‡¦ç†é€Ÿåº¦: {len(self.zeros)/execution_time:.1f}ã‚¼ãƒ­ç‚¹/ç§’")
        print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªåŠ¹ç‡: {len(self.zeros)/(psutil.Process().memory_info().rss/1024/1024):.1f}ã‚¼ãƒ­ç‚¹/MB")
        if confidence_scores:
            print(f"ğŸ¯ å¹³å‡ä¿¡é ¼åº¦: {np.mean(confidence_scores):.4f}")
        print("ğŸŠ Stage3 ç©¶æ¥µã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†!")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸŒŸ NKAT Stage3: ç©¶æ¥µ100,000ã‚¼ãƒ­ç‚¹åˆ†æ•£å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•!")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = NKAT_Stage3_UltimateSystem(
        target_zeros=100000, 
        batch_size=5000,
        checkpoint_interval=10000
    )
    
    # ç©¶æ¥µè§£æå®Ÿè¡Œ
    system.run_ultimate_analysis()


if __name__ == "__main__":
    main() 
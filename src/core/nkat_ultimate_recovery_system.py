#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ›¡ï¸ NKATç†è«– ç©¶æ¥µé›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ + è‡ªå‹•å¾©æ—§ + ã‚¯ãƒ©ã‚¦ãƒ‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã¾ã™

Don't hold back. Give it your all!! ğŸ”¥

NKAT Research Team 2025
"""

import os
import sys
import json
import pickle
import sqlite3
import hashlib
import shutil
import threading
import time
import zipfile
from datetime import datetime, timedelta
from pathlib import Path
import numpy as np
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# CUDAã®æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import cupy as cp
    CUDA_AVAILABLE = cp.cuda.is_available()
except ImportError:
    CUDA_AVAILABLE = False
    cp = np

class NKATUltimateRecoverySystem:
    """ğŸ›¡ï¸ NKATç©¶æ¥µé›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, base_dir="recovery_data", cloud_backup=True):
        """
        ğŸ—ï¸ åˆæœŸåŒ–
        
        Args:
            base_dir: ãƒªã‚«ãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            cloud_backup: ã‚¯ãƒ©ã‚¦ãƒ‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æœ‰åŠ¹åŒ–
        """
        print("ğŸ›¡ï¸ NKAT ç©¶æ¥µé›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•ï¼")
        print("="*70)
        
        self.base_dir = Path(base_dir)
        self.cloud_backup = cloud_backup
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ æ§‹ç¯‰
        self.setup_directory_structure()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self.setup_database()
        
        # è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ 
        self.auto_backup_enabled = True
        self.backup_interval = 60  # 60ç§’é–“éš”
        
        # ç·Šæ€¥é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
        self.emergency_notification = True
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹
        self.start_auto_backup_thread()
        
        print("âœ… ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“ ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.base_dir.absolute()}")
        print(f"â˜ï¸ ã‚¯ãƒ©ã‚¦ãƒ‰ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {'æœ‰åŠ¹' if cloud_backup else 'ç„¡åŠ¹'}")
        
    def setup_directory_structure(self):
        """ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ æ§‹ç¯‰"""
        self.dirs = {
            'main': self.base_dir,
            'checkpoints': self.base_dir / 'checkpoints',
            'emergency': self.base_dir / 'emergency',
            'archives': self.base_dir / 'archives',
            'temp': self.base_dir / 'temp',
            'metadata': self.base_dir / 'metadata',
            'logs': self.base_dir / 'logs',
            'cloud_sync': self.base_dir / 'cloud_sync'
        }
        
        for dir_path in self.dirs.values():
            dir_path.mkdir(parents=True, exist_ok=True)
            
        print(f"ğŸ“ {len(self.dirs)}å€‹ã®ãƒªã‚«ãƒãƒªãƒ¼ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†")
    
    def setup_database(self):
        """ğŸ—ƒï¸ SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        self.db_path = self.dirs['metadata'] / 'recovery.db'
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS checkpoints (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    problem_name TEXT NOT NULL,
                    checkpoint_type TEXT NOT NULL,
                    file_path TEXT NOT NULL,
                    file_hash TEXT NOT NULL,
                    file_size INTEGER NOT NULL,
                    success_rate REAL,
                    metadata TEXT,
                    is_valid BOOLEAN DEFAULT 1,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # å¾©æ—§å±¥æ­´ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS recovery_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    recovery_timestamp TEXT NOT NULL,
                    checkpoint_id INTEGER,
                    recovery_success BOOLEAN,
                    recovery_time_seconds REAL,
                    error_message TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (checkpoint_id) REFERENCES checkpoints (id)
                )
            """)
            
            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ãƒ†ãƒ¼ãƒ–ãƒ«
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS system_state (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    system_status TEXT NOT NULL,
                    cpu_usage REAL,
                    memory_usage REAL,
                    gpu_usage REAL,
                    disk_usage REAL,
                    active_problems TEXT,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            conn.commit()
        
        print("ğŸ—ƒï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†")
    
    def create_checkpoint(self, problem_name, data, checkpoint_type="standard"):
        """
        ğŸ”„ ãƒãƒ«ãƒãƒ¬ãƒ™ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆ
        
        Args:
            problem_name: å•é¡Œå
            data: ä¿å­˜ãƒ‡ãƒ¼ã‚¿
            checkpoint_type: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç¨®åˆ¥
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        checkpoint_data = {
            'timestamp': timestamp,
            'problem_name': problem_name,
            'checkpoint_type': checkpoint_type,
            'data': self._serialize_for_storage(data),
            'system_info': self._get_system_info(),
            'nkat_version': '2025.06.04',
            'cuda_available': CUDA_AVAILABLE
        }
        
        # ä¿å­˜ãƒ‘ã‚¹æ±ºå®š
        file_name = f"{problem_name}_{checkpoint_type}_{timestamp}.pkl"
        file_path = self.dirs['checkpoints'] / file_name
        
        try:
            # ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            with open(file_path, 'wb') as f:
                pickle.dump(checkpoint_data, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
            file_hash = self._calculate_file_hash(file_path)
            file_size = file_path.stat().st_size
            
            # ç·Šæ€¥ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆé«˜é€Ÿã‚¢ã‚¯ã‚»ã‚¹ç”¨ï¼‰
            if checkpoint_type in ['critical', 'milestone']:
                emergency_path = self.dirs['emergency'] / file_name
                shutil.copy2(file_path, emergency_path)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨˜éŒ²
            self._record_checkpoint(
                timestamp, problem_name, checkpoint_type,
                str(file_path), file_hash, file_size, data
            )
            
            # ã‚¯ãƒ©ã‚¦ãƒ‰åŒæœŸ
            if self.cloud_backup:
                self._schedule_cloud_sync(file_path)
            
            print(f"âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆ: {problem_name} ({checkpoint_type})")
            print(f"   ğŸ“ ã‚µã‚¤ã‚º: {file_size/1024**2:.2f}MB")
            print(f"   ğŸ”’ ãƒãƒƒã‚·ãƒ¥: {file_hash[:16]}...")
            
            return str(file_path)
            
        except Exception as e:
            print(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            self._log_error('checkpoint_creation', str(e))
            return None
    
    def _serialize_for_storage(self, data):
        """ğŸ’¾ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç”¨ãƒ‡ãƒ¼ã‚¿ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        if isinstance(data, dict):
            return {k: self._serialize_for_storage(v) for k, v in data.items()}
        elif hasattr(data, 'get') and CUDA_AVAILABLE:
            # CuPyé…åˆ—å‡¦ç†
            return data.get() if hasattr(data, 'get') else data
        elif isinstance(data, np.ndarray):
            # NumPyé…åˆ—ã¯åœ§ç¸®ä¿å­˜
            return {'__numpy_array__': True, 'data': data.tobytes(), 'shape': data.shape, 'dtype': str(data.dtype)}
        else:
            return data
    
    def _get_system_info(self):
        """ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        try:
            import psutil
            
            info = {
                'cpu_percent': psutil.cpu_percent(),
                'memory_percent': psutil.virtual_memory().percent,
                'disk_usage': psutil.disk_usage('.').percent,
                'gpu_available': CUDA_AVAILABLE
            }
            
            if CUDA_AVAILABLE:
                try:
                    import GPUtil
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu = gpus[0]
                        info['gpu_load'] = gpu.load
                        info['gpu_memory'] = gpu.memoryUtil
                        info['gpu_temperature'] = gpu.temperature
                except:
                    pass
            
            return info
        except:
            return {'status': 'system_info_unavailable'}
    
    def _calculate_file_hash(self, file_path):
        """ğŸ”’ ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        hash_sha256 = hashlib.sha256()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    
    def _record_checkpoint(self, timestamp, problem_name, checkpoint_type, file_path, file_hash, file_size, data):
        """ğŸ“ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè¨˜éŒ²"""
        # æˆåŠŸç‡è¨ˆç®—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        success_rate = self._estimate_success_rate(data)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        metadata = json.dumps({
            'data_types': [type(v).__name__ for v in data.values()] if isinstance(data, dict) else [type(data).__name__],
            'complexity_estimate': len(str(data))
        })
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO checkpoints 
                (timestamp, problem_name, checkpoint_type, file_path, file_hash, file_size, success_rate, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (timestamp, problem_name, checkpoint_type, file_path, file_hash, file_size, success_rate, metadata))
            conn.commit()
    
    def _estimate_success_rate(self, data):
        """ğŸ“ˆ æˆåŠŸç‡æ¨å®š"""
        if isinstance(data, dict):
            if 'verification' in data and 'confidence_score' in data['verification']:
                return data['verification']['confidence_score']
        return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def find_latest_checkpoint(self, problem_name=None, checkpoint_type=None):
        """ğŸ” æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œç´¢"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            query = "SELECT * FROM checkpoints WHERE is_valid = 1"
            params = []
            
            if problem_name:
                query += " AND problem_name = ?"
                params.append(problem_name)
            
            if checkpoint_type:
                query += " AND checkpoint_type = ?"
                params.append(checkpoint_type)
            
            query += " ORDER BY created_at DESC LIMIT 1"
            
            cursor.execute(query, params)
            result = cursor.fetchone()
            
            if result:
                columns = [description[0] for description in cursor.description]
                return dict(zip(columns, result))
            return None
    
    def recover_from_checkpoint(self, checkpoint_id=None, checkpoint_path=None):
        """ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©æ—§"""
        start_time = time.time()
        recovery_success = False
        error_message = None
        
        try:
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±å–å¾—
            if checkpoint_id:
                checkpoint_info = self._get_checkpoint_by_id(checkpoint_id)
                if not checkpoint_info:
                    raise ValueError(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆID {checkpoint_id} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                checkpoint_path = checkpoint_info['file_path']
            
            if not checkpoint_path or not Path(checkpoint_path).exists():
                # è‡ªå‹•æœ€é©ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œç´¢
                checkpoint_info = self._find_optimal_checkpoint()
                if not checkpoint_info:
                    raise ValueError("å¾©æ—§å¯èƒ½ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                checkpoint_path = checkpoint_info['file_path']
            
            print(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©æ—§ä¸­: {Path(checkpoint_path).name}")
            
            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            with open(checkpoint_path, 'rb') as f:
                checkpoint_data = pickle.load(f)
            
            # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
            if not self._validate_checkpoint_data(checkpoint_data):
                raise ValueError("ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç ´æã—ã¦ã„ã¾ã™")
            
            # ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ
            restored_data = self._deserialize_from_storage(checkpoint_data['data'])
            
            recovery_success = True
            recovery_time = time.time() - start_time
            
            # å¾©æ—§å±¥æ­´è¨˜éŒ²
            self._record_recovery(checkpoint_id, recovery_success, recovery_time, None)
            
            print(f"âœ… å¾©æ—§å®Œäº†ï¼ ({recovery_time:.2f}ç§’)")
            print(f"   ğŸ“… ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—: {checkpoint_data['timestamp']}")
            print(f"   ğŸ¯ å•é¡Œ: {checkpoint_data['problem_name']}")
            print(f"   ğŸ“Š ç¨®åˆ¥: {checkpoint_data['checkpoint_type']}")
            
            return restored_data
            
        except Exception as e:
            error_message = str(e)
            recovery_time = time.time() - start_time
            self._record_recovery(checkpoint_id, False, recovery_time, error_message)
            print(f"âŒ å¾©æ—§ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _get_checkpoint_by_id(self, checkpoint_id):
        """ğŸ“‹ IDæŒ‡å®šãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT * FROM checkpoints WHERE id = ?", (checkpoint_id,))
            result = cursor.fetchone()
            
            if result:
                columns = [description[0] for description in cursor.description]
                return dict(zip(columns, result))
            return None
    
    def _find_optimal_checkpoint(self):
        """ğŸ¯ æœ€é©ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆè‡ªå‹•æ¤œç´¢"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # æˆåŠŸç‡ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
            cursor.execute("""
                SELECT * FROM checkpoints 
                WHERE is_valid = 1 
                ORDER BY success_rate DESC, created_at DESC 
                LIMIT 5
            """)
            
            candidates = cursor.fetchall()
            
            # å­˜åœ¨ç¢ºèªã¨ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
            for candidate in candidates:
                file_path = candidate[4]  # file_path column
                if Path(file_path).exists():
                    # ãƒãƒƒã‚·ãƒ¥æ¤œè¨¼
                    current_hash = self._calculate_file_hash(file_path)
                    stored_hash = candidate[5]  # file_hash column
                    
                    if current_hash == stored_hash:
                        columns = [description[0] for description in cursor.description]
                        return dict(zip(columns, candidate))
            
            return None
    
    def _validate_checkpoint_data(self, checkpoint_data):
        """âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æ¤œè¨¼"""
        required_fields = ['timestamp', 'problem_name', 'data']
        return all(field in checkpoint_data for field in required_fields)
    
    def _deserialize_from_storage(self, data):
        """ğŸ”„ ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å¾©å…ƒ"""
        if isinstance(data, dict):
            if '__numpy_array__' in data:
                # NumPyé…åˆ—å¾©å…ƒ
                return np.frombuffer(data['data'], dtype=data['dtype']).reshape(data['shape'])
            else:
                return {k: self._deserialize_from_storage(v) for k, v in data.items()}
        else:
            return data
    
    def _record_recovery(self, checkpoint_id, success, recovery_time, error_message):
        """ğŸ“ å¾©æ—§å±¥æ­´è¨˜éŒ²"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO recovery_history 
                (recovery_timestamp, checkpoint_id, recovery_success, recovery_time_seconds, error_message)
                VALUES (?, ?, ?, ?, ?)
            """, (datetime.now().isoformat(), checkpoint_id, success, recovery_time, error_message))
            conn.commit()
    
    def start_auto_backup_thread(self):
        """ğŸ”„ è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹"""
        def auto_backup_loop():
            while self.auto_backup_enabled:
                try:
                    self._perform_maintenance()
                    time.sleep(self.backup_interval)
                except Exception as e:
                    self._log_error('auto_backup', str(e))
        
        self.backup_thread = threading.Thread(target=auto_backup_loop, daemon=True)
        self.backup_thread.start()
        print("ğŸ”„ è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚¹ãƒ¬ãƒƒãƒ‰é–‹å§‹")
    
    def _perform_maintenance(self):
        """ğŸ”§ å®šæœŸãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹å®Ÿè¡Œ"""
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¨˜éŒ²
        self._record_system_state()
        
        # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
        self._archive_old_checkpoints()
        
        # ç ´æãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯
        self._check_file_integrity()
    
    def _record_system_state(self):
        """ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¨˜éŒ²"""
        system_info = self._get_system_info()
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO system_state 
                (timestamp, system_status, cpu_usage, memory_usage, gpu_usage, disk_usage, active_problems)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                datetime.now().isoformat(),
                'running',
                system_info.get('cpu_percent', 0),
                system_info.get('memory_percent', 0),
                system_info.get('gpu_load', 0),
                system_info.get('disk_usage', 0),
                json.dumps(['nkat_millennium_analysis'])
            ))
            conn.commit()
    
    def _archive_old_checkpoints(self, days_threshold=7):
        """ğŸ“¦ å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–"""
        threshold_date = datetime.now() - timedelta(days=days_threshold)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT file_path FROM checkpoints 
                WHERE created_at < ? AND checkpoint_type != 'critical'
            """, (threshold_date.isoformat(),))
            
            old_files = cursor.fetchall()
            
            if old_files:
                # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ZIPä½œæˆ
                archive_name = f"checkpoint_archive_{datetime.now().strftime('%Y%m%d')}.zip"
                archive_path = self.dirs['archives'] / archive_name
                
                with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for (file_path,) in old_files:
                        if Path(file_path).exists():
                            zipf.write(file_path, Path(file_path).name)
                            Path(file_path).unlink()  # å…ƒãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                
                print(f"ğŸ“¦ {len(old_files)}å€‹ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {archive_name}")
    
    def _check_file_integrity(self):
        """ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id, file_path, file_hash FROM checkpoints WHERE is_valid = 1")
            
            for checkpoint_id, file_path, stored_hash in cursor.fetchall():
                if Path(file_path).exists():
                    current_hash = self._calculate_file_hash(file_path)
                    if current_hash != stored_hash:
                        # ãƒ•ã‚¡ã‚¤ãƒ«ç ´ææ¤œå‡º
                        cursor.execute("UPDATE checkpoints SET is_valid = 0 WHERE id = ?", (checkpoint_id,))
                        print(f"âš ï¸ ç ´æãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {Path(file_path).name}")
            
            conn.commit()
    
    def _schedule_cloud_sync(self, file_path):
        """â˜ï¸ ã‚¯ãƒ©ã‚¦ãƒ‰åŒæœŸã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"""
        # ã‚¯ãƒ©ã‚¦ãƒ‰åŒæœŸã¯ç°¡ç•¥åŒ–å®Ÿè£…
        cloud_path = self.dirs['cloud_sync'] / Path(file_path).name
        try:
            shutil.copy2(file_path, cloud_path)
            print(f"â˜ï¸ ã‚¯ãƒ©ã‚¦ãƒ‰åŒæœŸ: {Path(file_path).name}")
        except Exception as e:
            self._log_error('cloud_sync', str(e))
    
    def _log_error(self, operation, error_message):
        """ğŸ“ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ²"""
        log_file = self.dirs['logs'] / f"error_log_{datetime.now().strftime('%Y%m%d')}.txt"
        
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(f"[{datetime.now().isoformat()}] {operation}: {error_message}\n")
    
    def generate_recovery_report(self):
        """ğŸ“Š å¾©æ—§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆçµ±è¨ˆ
            cursor.execute("SELECT COUNT(*), AVG(success_rate) FROM checkpoints WHERE is_valid = 1")
            checkpoint_stats = cursor.fetchone()
            
            # å¾©æ—§çµ±è¨ˆ
            cursor.execute("SELECT COUNT(*), SUM(recovery_success), AVG(recovery_time_seconds) FROM recovery_history")
            recovery_stats = cursor.fetchone()
            
            # æœ€æ–°ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
            cursor.execute("SELECT * FROM system_state ORDER BY created_at DESC LIMIT 1")
            latest_state = cursor.fetchone()
            
            report = {
                'timestamp': datetime.now().isoformat(),
                'checkpoint_statistics': {
                    'total_checkpoints': checkpoint_stats[0] or 0,
                    'average_success_rate': checkpoint_stats[1] or 0.0
                },
                'recovery_statistics': {
                    'total_recoveries': recovery_stats[0] or 0,
                    'successful_recoveries': recovery_stats[1] or 0,
                    'average_recovery_time': recovery_stats[2] or 0.0
                },
                'system_status': {
                    'last_update': latest_state[1] if latest_state else 'unknown',
                    'cpu_usage': latest_state[3] if latest_state else 0,
                    'memory_usage': latest_state[4] if latest_state else 0,
                    'gpu_usage': latest_state[5] if latest_state else 0
                } if latest_state else {}
            }
            
            # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            report_file = self.dirs['logs'] / f"recovery_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            print("ğŸ“Š å¾©æ—§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
            print(f"   ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•°: {report['checkpoint_statistics']['total_checkpoints']}")
            print(f"   ğŸ¯ å¹³å‡æˆåŠŸç‡: {report['checkpoint_statistics']['average_success_rate']:.3f}")
            print(f"   ğŸ”„ å¾©æ—§å›æ•°: {report['recovery_statistics']['total_recoveries']}")
            
            return report
    
    def emergency_shutdown_protection(self):
        """ğŸš¨ ç·Šæ€¥ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ä¿è­·"""
        print("ğŸš¨ ç·Šæ€¥ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³æ¤œå‡ºï¼ç·Šæ€¥ä¿è­·å®Ÿè¡Œä¸­...")
        
        # é‡è¦ãƒ‡ãƒ¼ã‚¿ã®ç·Šæ€¥ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        emergency_backup = {
            'timestamp': datetime.now().isoformat(),
            'emergency_type': 'power_failure',
            'system_state': self._get_system_info()
        }
        
        emergency_file = self.dirs['emergency'] / f"emergency_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(emergency_file, 'w') as f:
            json.dump(emergency_backup, f, indent=2)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç·Šæ€¥ã‚³ãƒŸãƒƒãƒˆ
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("PRAGMA synchronous = FULL")
                conn.commit()
        except:
            pass
        
        print("âœ… ç·Šæ€¥ä¿è­·å®Œäº†")
    
    def stop_auto_backup(self):
        """â¹ï¸ è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—åœæ­¢"""
        self.auto_backup_enabled = False
        print("â¹ï¸ è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—åœæ­¢")

def main():
    """ğŸš€ ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ›¡ï¸ NKAT ç©¶æ¥µé›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ")
    print("Don't hold back. Give it your all!! ğŸ”¥")
    print("="*70)
    
    try:
        # ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        recovery_system = NKATUltimateRecoverySystem()
        
        # ãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆ
        test_data = {
            'problem': 'riemann_hypothesis',
            'results': {'zeros_found': 15, 'confidence': 0.95},
            'verification': {'confidence_score': 0.95}
        }
        
        checkpoint_path = recovery_system.create_checkpoint(
            'riemann_hypothesis', test_data, 'milestone'
        )
        
        # å¾©æ—§ãƒ†ã‚¹ãƒˆ
        if checkpoint_path:
            print("\nğŸ”„ å¾©æ—§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
            recovered_data = recovery_system.recover_from_checkpoint(checkpoint_path=checkpoint_path)
            
            if recovered_data:
                print("âœ… å¾©æ—§ãƒ†ã‚¹ãƒˆæˆåŠŸï¼")
            else:
                print("âŒ å¾©æ—§ãƒ†ã‚¹ãƒˆå¤±æ•—")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        recovery_system.generate_recovery_report()
        
        print("\nğŸ† ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ãƒ»ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç·Šæ€¥åœæ­¢æ¤œå‡º")
        if 'recovery_system' in locals():
            recovery_system.emergency_shutdown_protection()
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    finally:
        if 'recovery_system' in locals():
            recovery_system.stop_auto_backup()
        print("ğŸ”¥ ã‚·ã‚¹ãƒ†ãƒ çµ‚äº†")

if __name__ == "__main__":
    main() 
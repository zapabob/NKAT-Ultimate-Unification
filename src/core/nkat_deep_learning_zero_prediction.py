#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  NKATæ·±å±¤å­¦ç¿’ã‚¼ãƒ­ç‚¹äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
å‚è€ƒ: https://github.com/avysogorets/riemann-zeta
     https://sites.google.com/site/riemannzetazeros/machinelearning

38,832å€‹ã®NKATã‚¼ãƒ­ç‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ©Ÿæ¢°å­¦ç¿’ã§ãƒªãƒ¼ãƒãƒ³ä»®èª¬ã‚’æ¤œè¨¼
- LSTMã€MLPã€RNN ã«ã‚ˆã‚‹äºˆæ¸¬
- ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
- ROC AUCã€æ··åŒè¡Œåˆ—ã«ã‚ˆã‚‹æ€§èƒ½è©•ä¾¡
"""

import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
from pathlib import Path
from tqdm import tqdm

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVR
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Conv1D, MaxPooling1D, Flatten
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    TF_AVAILABLE = True
    print("ğŸ§  TensorFlow/Keras: æœ‰åŠ¹")
except ImportError:
    TF_AVAILABLE = False
    print("âš ï¸ TensorFlowç„¡åŠ¹ - å¾“æ¥MLæ‰‹æ³•ã®ã¿")

# GPUé–¢é€£
try:
    import cupy as cp
    CUDA_AVAILABLE = True
    print("ğŸš€ CUDA GPUåŠ é€Ÿ: æœ‰åŠ¹")
except ImportError:
    CUDA_AVAILABLE = False
    print("âš ï¸ CUDAç„¡åŠ¹")

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings('ignore')

# matplotlibæ—¥æœ¬èªå¯¾å¿œ
plt.rcParams['font.family'] = ['DejaVu Sans']
sns.set_style("whitegrid")

class NKATDeepLearningPredictor:
    """NKATæ·±å±¤å­¦ç¿’ã‚¼ãƒ­ç‚¹äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, data_file="nkat_production_results_nkat_prod_20250604_102015.json"):
        """åˆæœŸåŒ–"""
        self.data_file = data_file
        self.zeros_data = None
        self.features = None
        self.targets = None
        self.models = {}
        self.results = {}
        
        # çµæœä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.output_dir = Path("nkat_deep_learning_results")
        self.output_dir.mkdir(exist_ok=True)
        
        print(f"ğŸ§  NKATæ·±å±¤å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–")
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {data_file}")
        print(f"ğŸ’¾ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    def load_nkat_data(self):
        """NKATã‚¼ãƒ­ç‚¹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        print(f"ğŸ“Š NKATãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹...")
        
        try:
            with open(self.data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ã‚¼ãƒ­ç‚¹ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            zeros_raw = data['results']['zeros_data']
            print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†: {len(zeros_raw)}å€‹ã®ã‚¼ãƒ­ç‚¹")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ åŒ–
            zeros_df = pd.DataFrame(zeros_raw)
            
            # åŸºæœ¬çµ±è¨ˆ
            print(f"\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
            print(f"   ã‚¼ãƒ­ç‚¹æ•°: {len(zeros_df):,}")
            print(f"   tå€¤ç¯„å›²: {zeros_df['t'].min():.3f} - {zeros_df['t'].max():.3f}")
            print(f"   å¹³å‡ä¿¡é ¼åº¦: {zeros_df['confidence'].mean():.6f}")
            print(f"   å¹³å‡æ®‹å·®: {zeros_df['residual'].mean():.2e}")
            
            # æ—¢çŸ¥ã‚¼ãƒ­ç‚¹ãƒãƒƒãƒãƒ³ã‚°ç‡
            if 'known_match' in zeros_df.columns:
                known_matches = zeros_df['known_match'].sum()
                print(f"   æ—¢çŸ¥ã‚¼ãƒ­ç‚¹ä¸€è‡´: {known_matches}/{len(zeros_df)} ({known_matches/len(zeros_df)*100:.1f}%)")
            
            self.zeros_data = zeros_df
            return zeros_df
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_features_advanced(self):
        """é«˜åº¦ãªç‰¹å¾´é‡æŠ½å‡ºï¼ˆGitHubå‚è€ƒï¼‰"""
        print(f"\nğŸ”¬ é«˜åº¦ç‰¹å¾´é‡æŠ½å‡ºé–‹å§‹...")
        
        if self.zeros_data is None:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        # åŸºæœ¬ç‰¹å¾´é‡
        features_dict = {
            't_value': self.zeros_data['t'].values,
            'residual': self.zeros_data['residual'].values,
            'confidence': self.zeros_data['confidence'].values,
            'superconv_factor': self.zeros_data['superconv_factor'].values
        }
        
        # ã‚½ãƒ¼ãƒˆï¼ˆtå€¤é †ï¼‰
        sort_idx = np.argsort(features_dict['t_value'])
        for key in features_dict:
            features_dict[key] = features_dict[key][sort_idx]
        
        # GitHub avysogorets å‚è€ƒç‰¹å¾´é‡
        
        # 1. é€£ç¶šã‚¼ãƒ­ç‚¹é–“è·é›¢
        t_diffs = np.diff(features_dict['t_value'])
        features_dict['zero_spacing'] = np.concatenate([[t_diffs[0]], t_diffs])
        
        # 2. å±€æ‰€å¯†åº¦ï¼ˆå‘¨è¾º5ç‚¹ã§ã®å¹³å‡é–“éš”ï¼‰
        local_density = []
        for i in range(len(features_dict['t_value'])):
            start = max(0, i-2)
            end = min(len(features_dict['t_value']), i+3)
            if end - start > 1:
                local_spacings = np.diff(features_dict['t_value'][start:end])
                local_density.append(np.mean(local_spacings))
            else:
                local_density.append(features_dict['zero_spacing'][i])
        features_dict['local_density'] = np.array(local_density)
        
        # 3. Gramç‚¹è¿‘ä¼¼ç‰¹å¾´é‡ï¼ˆGoogle Siteså‚è€ƒï¼‰
        # ç°¡ç•¥åŒ–ã—ãŸGramç‚¹é–¢é€£ç‰¹å¾´
        gram_approx = 2 * np.pi * features_dict['t_value'] / np.log(features_dict['t_value'] / (2 * np.pi))
        gram_deviation = features_dict['t_value'] - gram_approx
        features_dict['gram_deviation'] = gram_deviation
        
        # 4. Zé–¢æ•°é–¢é€£ç‰¹å¾´ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        z_function_approx = np.cos(features_dict['t_value'] * np.log(features_dict['t_value']) / 2)
        features_dict['z_function_approx'] = z_function_approx
        
        # 5. Hardy Zé–¢æ•°ã®ç¬¦å·å¤‰åŒ–
        z_sign_changes = []
        for i in range(len(z_function_approx)):
            if i == 0:
                z_sign_changes.append(0)
            else:
                z_sign_changes.append(1 if z_function_approx[i] * z_function_approx[i-1] < 0 else 0)
        features_dict['z_sign_change'] = np.array(z_sign_changes)
        
        # 6. çµ±è¨ˆçš„ç‰¹å¾´é‡
        features_dict['residual_log'] = np.log10(features_dict['residual'] + 1e-16)
        features_dict['confidence_log'] = np.log10(features_dict['confidence'] + 1e-16)
        
        # 7. ç§»å‹•å¹³å‡ç‰¹å¾´
        window = min(10, len(features_dict['t_value']) // 10)
        features_dict['spacing_ma'] = pd.Series(features_dict['zero_spacing']).rolling(window, center=True).mean().fillna(method='bfill').fillna(method='ffill').values
        features_dict['residual_ma'] = pd.Series(features_dict['residual']).rolling(window, center=True).mean().fillna(method='bfill').fillna(method='ffill').values
        
        # 8. é«˜æ¬¡çµ±è¨ˆç‰¹å¾´
        features_dict['t_value_normalized'] = (features_dict['t_value'] - features_dict['t_value'].mean()) / features_dict['t_value'].std()
        features_dict['spacing_ratio'] = features_dict['zero_spacing'] / features_dict['spacing_ma']
        
        # ãƒ•ã‚£ãƒ¼ãƒãƒ£ãƒ¼è¡Œåˆ—ä½œæˆ
        feature_names = [
            't_value', 'residual', 'confidence', 'superconv_factor',
            'zero_spacing', 'local_density', 'gram_deviation', 
            'z_function_approx', 'z_sign_change', 'residual_log', 
            'confidence_log', 'spacing_ma', 'residual_ma',
            't_value_normalized', 'spacing_ratio'
        ]
        
        X = np.column_stack([features_dict[name] for name in feature_names])
        
        print(f"âœ… ç‰¹å¾´é‡æŠ½å‡ºå®Œäº†:")
        print(f"   ç‰¹å¾´é‡æ•°: {len(feature_names)}")
        print(f"   ã‚µãƒ³ãƒ—ãƒ«æ•°: {X.shape[0]}")
        print(f"   ç‰¹å¾´é‡å: {feature_names}")
        
        self.features = X
        self.feature_names = feature_names
        return X, feature_names
    
    def create_targets_classification(self):
        """åˆ†é¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ"""
        print(f"\nğŸ¯ åˆ†é¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ...")
        
        if self.zeros_data is None:
            return None
        
        # è¤‡æ•°ã®åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚’å®šç¾©
        targets = {}
        
        # 1. é«˜ä¿¡é ¼åº¦ã‚¼ãƒ­ç‚¹åˆ†é¡ï¼ˆä¸Šä½25%ï¼‰
        confidence_threshold = np.percentile(self.zeros_data['confidence'], 75)
        targets['high_confidence'] = (self.zeros_data['confidence'] > confidence_threshold).astype(int)
        
        # 2. ä½æ®‹å·®ã‚¼ãƒ­ç‚¹åˆ†é¡ï¼ˆä¸‹ä½25%ï¼‰
        residual_threshold = np.percentile(self.zeros_data['residual'], 25)
        targets['low_residual'] = (self.zeros_data['residual'] < residual_threshold).astype(int)
        
        # 3. æ—¢çŸ¥ã‚¼ãƒ­ç‚¹ãƒãƒƒãƒãƒ³ã‚°ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if 'known_match' in self.zeros_data.columns:
            # NaNå€¤ã‚’0ã«ç½®æ›ã—ã¦ã‹ã‚‰intå¤‰æ›
            known_match_clean = self.zeros_data['known_match'].fillna(0)
            targets['known_match'] = known_match_clean.astype(int)
        
        # 4. è¶…åæŸå› å­ç•°å¸¸å€¤ï¼ˆä¸Šä½10%ï¼‰
        superconv_threshold = np.percentile(self.zeros_data['superconv_factor'], 90)
        targets['high_superconv'] = (self.zeros_data['superconv_factor'] > superconv_threshold).astype(int)
        
        # 5. å¯†é›†ã‚¼ãƒ­ç‚¹æ¤œå‡ºï¼ˆé–“éš”ãŒå¹³å‡ã®50%ä»¥ä¸‹ï¼‰
        if len(self.zeros_data) > 1:
            t_diffs = np.diff(self.zeros_data['t'].values)
            avg_spacing = np.mean(t_diffs)
            close_pairs = t_diffs < (avg_spacing * 0.5)
            targets['close_pairs'] = np.concatenate([[0], close_pairs.astype(int)])
        
        print(f"âœ… åˆ†é¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆå®Œäº†:")
        for name, target in targets.items():
            positive_rate = np.mean(target)
            print(f"   {name}: æ­£ä¾‹ç‡ {positive_rate:.3f} ({np.sum(target)}/{len(target)})")
        
        self.targets = targets
        return targets
    
    def build_deep_learning_models(self):
        """æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"""
        if not TF_AVAILABLE:
            print("âš ï¸ TensorFlowç„¡åŠ¹ - æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚­ãƒƒãƒ—")
            return {}
        
        print(f"\nğŸ§  æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰é–‹å§‹...")
        
        models = {}
        input_dim = self.features.shape[1]
        
        # 1. Multi-Layer Perceptron (MLP) - GitHubå‚è€ƒ
        def create_mlp_model():
            model = Sequential([
                Input(shape=(input_dim,)),
                Dense(128, activation='relu'),
                Dropout(0.3),
                Dense(64, activation='relu'),
                Dropout(0.3),
                Dense(32, activation='relu'),
                Dropout(0.2),
                Dense(1, activation='sigmoid')
            ])
            model.compile(optimizer=Adam(learning_rate=0.001),
                         loss='binary_crossentropy',
                         metrics=['accuracy', 'precision', 'recall'])
            return model
        
        # 2. LSTM Modelï¼ˆæ™‚ç³»åˆ—ç‰¹å¾´ç”¨ï¼‰
        def create_lstm_model():
            model = Sequential([
                Input(shape=(input_dim, 1)),
                LSTM(64, return_sequences=True),
                Dropout(0.3),
                LSTM(32),
                Dropout(0.3),
                Dense(16, activation='relu'),
                Dense(1, activation='sigmoid')
            ])
            model.compile(optimizer=Adam(learning_rate=0.001),
                         loss='binary_crossentropy',
                         metrics=['accuracy', 'precision', 'recall'])
            return model
        
        # 3. Convolutional Neural Network
        def create_cnn_model():
            model = Sequential([
                Input(shape=(input_dim, 1)),
                Conv1D(32, 3, activation='relu'),
                MaxPooling1D(2),
                Conv1D(64, 3, activation='relu'),
                MaxPooling1D(2),
                Flatten(),
                Dense(50, activation='relu'),
                Dropout(0.3),
                Dense(1, activation='sigmoid')
            ])
            model.compile(optimizer=Adam(learning_rate=0.001),
                         loss='binary_crossentropy',
                         metrics=['accuracy', 'precision', 'recall'])
            return model
        
        models['MLP'] = create_mlp_model
        models['LSTM'] = create_lstm_model
        models['CNN'] = create_cnn_model
        
        print(f"âœ… æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰å®Œäº†: {list(models.keys())}")
        return models
    
    def cross_validation_evaluation(self, model_type='sklearn'):
        """ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡"""
        print(f"\nğŸ” {model_type} ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹...")
        
        if self.features is None or self.targets is None:
            print("âŒ ç‰¹å¾´é‡ã¾ãŸã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        results = {}
        
        # ãƒ‡ãƒ¼ã‚¿æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.features)
        
        # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«å¯¾ã—ã¦CVå®Ÿè¡Œ
        for target_name, y in self.targets.items():
            print(f"\nğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {target_name}")
            
            if model_type == 'sklearn':
                # å¾“æ¥æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
                models = {
                    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
                    'SVR': SVR(kernel='rbf', C=1.0)
                }
                
                target_results = {}
                
                for model_name, model in models.items():
                    try:
                        # 5-fold ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
                        
                        # åˆ†é¡å•é¡Œã¨ã—ã¦æ‰±ã†
                        if hasattr(model, 'predict_proba'):
                            cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='roc_auc')
                        else:
                            cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')
                        
                        target_results[model_name] = {
                            'cv_scores': cv_scores,
                            'mean_score': cv_scores.mean(),
                            'std_score': cv_scores.std()
                        }
                        
                        print(f"   {model_name}: {cv_scores.mean():.3f} (Â±{cv_scores.std():.3f})")
                        
                    except Exception as e:
                        print(f"   âŒ {model_name} ã‚¨ãƒ©ãƒ¼: {e}")
                
                results[target_name] = target_results
            
            elif model_type == 'deep_learning' and TF_AVAILABLE:
                # æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
                dl_models = self.build_deep_learning_models()
                target_results = {}
                
                for model_name, create_model_func in dl_models.items():
                    try:
                        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)  # æ·±å±¤å­¦ç¿’ã¯3-fold
                        cv_scores = []
                        
                        for fold, (train_idx, val_idx) in enumerate(cv.split(X_scaled, y)):
                            X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
                            y_train, y_val = y[train_idx], y[val_idx]
                            
                            # LSTM/CNNã®å ´åˆã¯æ¬¡å…ƒè¿½åŠ 
                            if model_name in ['LSTM', 'CNN']:
                                X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
                                X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)
                            
                            # ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ»è¨“ç·´
                            model = create_model_func()
                            
                            # Early stopping
                            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
                            
                            history = model.fit(
                                X_train, y_train,
                                validation_data=(X_val, y_val),
                                epochs=50,
                                batch_size=32,
                                verbose=0,
                                callbacks=[early_stop]
                            )
                            
                            # äºˆæ¸¬ãƒ»è©•ä¾¡
                            y_pred_proba = model.predict(X_val, verbose=0)
                            try:
                                score = roc_auc_score(y_val, y_pred_proba)
                            except:
                                y_pred = (y_pred_proba > 0.5).astype(int)
                                score = accuracy_score(y_val, y_pred)
                            
                            cv_scores.append(score)
                            print(f"     Fold {fold+1}: {score:.3f}")
                        
                        target_results[model_name] = {
                            'cv_scores': np.array(cv_scores),
                            'mean_score': np.mean(cv_scores),
                            'std_score': np.std(cv_scores)
                        }
                        
                        print(f"   {model_name}: {np.mean(cv_scores):.3f} (Â±{np.std(cv_scores):.3f})")
                        
                    except Exception as e:
                        print(f"   âŒ {model_name} ã‚¨ãƒ©ãƒ¼: {e}")
                
                results[target_name] = target_results
        
        self.results[model_type] = results
        return results
    
    def create_confusion_matrix_plots(self):
        """æ··åŒè¡Œåˆ—ã¨ROCæ›²ç·šã®ä½œæˆ"""
        print(f"\nğŸ“Š æ··åŒè¡Œåˆ—ãƒ»ROCæ›²ç·šä½œæˆ...")
        
        if self.features is None or self.targets is None:
            return None
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.features)
        
        # çµæœä¿å­˜ç”¨
        plot_results = {}
        
        for target_name, y in self.targets.items():
            print(f"\nğŸ¯ {target_name} ã®è©•ä¾¡...")
            
            # Train-test split
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # RandomForest ã§è©•ä¾¡
            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
            rf_model.fit(X_train, y_train)
            
            y_pred = rf_model.predict(X_test)
            y_pred_proba = rf_model.predict_proba(X_test)[:, 1]
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='binary')
            recall = recall_score(y_test, y_pred, average='binary')
            f1 = f1_score(y_test, y_pred, average='binary')
            
            try:
                roc_auc = roc_auc_score(y_test, y_pred_proba)
                fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
            except:
                roc_auc = 0.5
                fpr, tpr = [0, 1], [0, 1]
            
            # ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            # æ··åŒè¡Œåˆ—
            cm = confusion_matrix(y_test, y_pred)
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])
            axes[0].set_title(f'Confusion Matrix: {target_name}')
            axes[0].set_xlabel('Predicted')
            axes[0].set_ylabel('Actual')
            
            # ROCæ›²ç·š
            axes[1].plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.3f})')
            axes[1].plot([0, 1], [0, 1], 'k--', label='Random')
            axes[1].set_xlabel('False Positive Rate')
            axes[1].set_ylabel('True Positive Rate')
            axes[1].set_title(f'ROC Curve: {target_name}')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
            metrics_text = f"""Metrics:
Accuracy: {accuracy:.3f}
Precision: {precision:.3f}
Recall: {recall:.3f}
F1-Score: {f1:.3f}
ROC AUC: {roc_auc:.3f}"""
            
            fig.text(0.02, 0.02, metrics_text, fontsize=10, 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
            
            plt.tight_layout()
            
            # ä¿å­˜
            plot_file = self.output_dir / f"confusion_roc_{target_name}.png"
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            plot_results[target_name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'roc_auc': roc_auc,
                'plot_file': str(plot_file)
            }
            
            print(f"   Accuracy: {accuracy:.3f}, Precision: {precision:.3f}")
            print(f"   Recall: {recall:.3f}, F1: {f1:.3f}, ROC AUC: {roc_auc:.3f}")
            print(f"   ğŸ“Š ä¿å­˜: {plot_file}")
        
        return plot_results
    
    def feature_importance_analysis(self):
        """ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ"""
        print(f"\nğŸ”¬ ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ...")
        
        if self.features is None or self.targets is None:
            return None
        
        # ãƒ‡ãƒ¼ã‚¿æº–å‚™
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(self.features)
        
        importance_results = {}
        
        for target_name, y in self.targets.items():
            # RandomForest ã§ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
            rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
            rf_model.fit(X_scaled, y)
            
            # é‡è¦åº¦
            importances = rf_model.feature_importances_
            indices = np.argsort(importances)[::-1]
            
            # ãƒ—ãƒ­ãƒƒãƒˆ
            plt.figure(figsize=(12, 8))
            plt.title(f'Feature Importance: {target_name}')
            plt.bar(range(len(importances)), importances[indices])
            plt.xticks(range(len(importances)), [self.feature_names[i] for i in indices], rotation=45, ha='right')
            plt.xlabel('Features')
            plt.ylabel('Importance')
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            
            # ä¿å­˜
            importance_file = self.output_dir / f"feature_importance_{target_name}.png"
            plt.savefig(importance_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            importance_results[target_name] = {
                'importances': importances,
                'feature_ranking': [self.feature_names[i] for i in indices],
                'plot_file': str(importance_file)
            }
            
            print(f"   {target_name} - Top 3 features:")
            for i in range(min(3, len(indices))):
                feature_idx = indices[i]
                print(f"     {i+1}. {self.feature_names[feature_idx]}: {importances[feature_idx]:.3f}")
        
        return importance_results
    
    def run_complete_analysis(self):
        """å®Œå…¨åˆ†æå®Ÿè¡Œ"""
        print("ğŸ§ " * 30)
        print("NKATæ·±å±¤å­¦ç¿’ã‚¼ãƒ­ç‚¹äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨åˆ†æ")
        print("ğŸ§ " * 30)
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if self.load_nkat_data() is None:
            return None
        
        # 2. ç‰¹å¾´é‡æŠ½å‡º
        if self.extract_features_advanced() is None:
            return None
        
        # 3. ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä½œæˆ
        if self.create_targets_classification() is None:
            return None
        
        # 4. å¾“æ¥æ©Ÿæ¢°å­¦ç¿’è©•ä¾¡
        sklearn_results = self.cross_validation_evaluation('sklearn')
        
        # 5. æ·±å±¤å­¦ç¿’è©•ä¾¡
        if TF_AVAILABLE:
            dl_results = self.cross_validation_evaluation('deep_learning')
        
        # 6. æ··åŒè¡Œåˆ—ãƒ»ROCæ›²ç·š
        confusion_results = self.create_confusion_matrix_plots()
        
        # 7. ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
        importance_results = self.feature_importance_analysis()
        
        # 8. ç·åˆçµæœã¾ã¨ã‚
        final_results = {
            'data_summary': {
                'total_zeros': len(self.zeros_data),
                't_range': [float(self.zeros_data['t'].min()), float(self.zeros_data['t'].max())],
                'feature_count': len(self.feature_names),
                'target_count': len(self.targets)
            },
            'sklearn_results': sklearn_results,
            'confusion_matrix_results': confusion_results,
            'feature_importance': importance_results,
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        if TF_AVAILABLE:
            final_results['deep_learning_results'] = dl_results
        
        # çµæœä¿å­˜
        results_file = self.output_dir / f"nkat_ml_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸŠ NKATæ·±å±¤å­¦ç¿’åˆ†æå®Œäº†!")
        print(f"ğŸ“Š æ¤œå‡ºã‚¼ãƒ­ç‚¹: {len(self.zeros_data):,}å€‹")
        print(f"ğŸ”¬ ç‰¹å¾´é‡æ•°: {len(self.feature_names)}")
        print(f"ğŸ¯ åˆ†é¡ã‚¿ã‚¹ã‚¯æ•°: {len(self.targets)}")
        print(f"ğŸ’¾ çµæœä¿å­˜: {results_file}")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        
        # ãƒ™ã‚¹ãƒˆçµæœè¡¨ç¤º
        print(f"\nğŸ† ãƒ™ã‚¹ãƒˆæ€§èƒ½çµæœ:")
        if confusion_results:
            for target_name, metrics in confusion_results.items():
                print(f"   {target_name}: ROC AUC = {metrics['roc_auc']:.3f}")
        
        return final_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ§  NKATæ·±å±¤å­¦ç¿’ã‚¼ãƒ­ç‚¹äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
    print("ğŸ“š å‚è€ƒç ”ç©¶:")
    print("   - GitHub: avysogorets/riemann-zeta")
    print("   - Google Sites: Machine Learning for Riemann Zeta")
    print("=" * 60)
    
    # ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    predictor = NKATDeepLearningPredictor()
    results = predictor.run_complete_analysis()
    
    if results:
        print("\nğŸŒŸ NKATæ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹éè‡ªæ˜ã‚¼ãƒ­ç‚¹äºˆæ¸¬å®Œäº†!")
        print("ğŸ”¬ ãƒªãƒ¼ãƒãƒ³ä»®èª¬ã¸ã®æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæˆåŠŸ!")
    else:
        print("\nâš ï¸ åˆ†æã‚¨ãƒ©ãƒ¼ - ãƒ‡ãƒ¼ã‚¿ç¢ºèªãŒå¿…è¦")

if __name__ == "__main__":
    main() 
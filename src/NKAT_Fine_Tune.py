# -*- coding: utf-8 -*-
"""
ğŸ¯ NKAT GPUä¿®ç¾…ãƒ¢ãƒ¼ãƒ‰å¾®èª¿æ•´ç‰ˆ ğŸ¯
èª¤å·® 1Ã—10â»âµ ã‚¢ã‚¿ãƒƒã‚¯å°‚ç”¨ - 20ã‚¨ãƒãƒƒã‚¯é›†ä¸­æ”»æ’ƒ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import time
import json
import datetime
import os
from pathlib import Path
from matplotlib import rcParams

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class NKATFineTuneNetwork(nn.Module):
    """NKATå¾®èª¿æ•´å°‚ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆå…ƒãƒ¢ãƒ‡ãƒ«äº’æ›ï¼‰"""
    
    def __init__(self, input_dim=4, hidden_dims=[512, 256, 128], grid_size=64):
        super().__init__()
        
        self.input_dim = input_dim
        self.grid_size = grid_size
        
        # å…ƒãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆäº’æ›æ€§ç¢ºä¿ï¼‰
        layers = []
        prev_dim = input_dim
        
        for i, hidden_dim in enumerate(hidden_dims):
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)  # å…ƒã¨åŒã˜ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
            ])
            prev_dim = hidden_dim
            
        # å…ƒãƒ¢ãƒ‡ãƒ«ã¨åŒã˜å‡ºåŠ›å±¤
        layers.extend([
            nn.Linear(prev_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        ])
        
        self.network = nn.Sequential(*layers)
        
    def forward(self, x):
        return self.network(x)

class NKATFineTuneLoss:
    """å¾®èª¿æ•´å°‚ç”¨æå¤±é–¢æ•°ï¼ˆè¶…é«˜ç²¾åº¦ï¼‰"""
    
    def __init__(self, device='cuda'):
        self.device = device
        self.theta_min = 1e-50
        self.theta_max = 1e-10
        
    def ultra_precision_spectral_loss(self, output, target_dim=4.0):
        """è¶…é«˜ç²¾åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«æ¬¡å…ƒæå¤±"""
        # ã‚ˆã‚Šå®‰å…¨ã§ç²¾å¯†ãªè¨ˆç®—
        output_clamped = torch.clamp(output, min=-5, max=5)
        spectral_dim = 4.0 + torch.mean(output_clamped) * 0.1  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
        
        # äºŒä¹—èª¤å·®ï¼ˆé«˜ç²¾åº¦ç”¨ï¼‰
        error = (spectral_dim - target_dim)**2
        return error, spectral_dim.item()
    
    def ultra_precision_theta_loss(self, output):
        """è¶…é«˜ç²¾åº¦Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æå¤±"""
        # ã‚ˆã‚Šå®‰å®šã—ãŸÎ¸è¨ˆç®—
        theta_raw = torch.exp(-torch.abs(output) * 0.1)
        theta_clamped = torch.clamp(theta_raw, min=self.theta_min, max=self.theta_max)
        
        # ç›®æ¨™å€¤ï¼ˆå®Ÿé¨“å€¤ã«åŸºã¥ãï¼‰
        target_theta = 1e-35
        theta_mse = torch.mean((theta_clamped - target_theta)**2)
        
        return theta_mse, torch.mean(theta_clamped).item()

def load_best_checkpoint():
    """æœ€è‰¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
    checkpoint_dir = Path("nkat_shura_checkpoints")
    best_model_path = checkpoint_dir / "best_model.pth"
    
    if best_model_path.exists():
        print(f"ğŸ“ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {best_model_path}")
        return torch.load(best_model_path)
    else:
        print("âš ï¸ ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None

def fine_tune_training():
    """å¾®èª¿æ•´è¨“ç·´ï¼ˆèª¤å·®10â»âµã‚¢ã‚¿ãƒƒã‚¯ï¼‰"""
    print("ğŸ¯" * 20)
    print("ğŸš€ NKAT å¾®èª¿æ•´ãƒ¢ãƒ¼ãƒ‰èµ·å‹•ï¼")
    print("ğŸ¯ ç›®æ¨™: ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«æ¬¡å…ƒèª¤å·® < 1Ã—10â»âµ")
    print("âš¡ è¨­å®š: 20ã‚¨ãƒãƒƒã‚¯é›†ä¸­æ”»æ’ƒ")
    print("ğŸ¯" * 20)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ”§ ãƒ‡ãƒã‚¤ã‚¹: {device}")
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    checkpoint = load_best_checkpoint()
    
    if checkpoint is None:
        print("âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãªã— - æ–°è¦è¨“ç·´é–‹å§‹")
        model = NKATFineTuneNetwork(grid_size=64).to(device)
        start_epoch = 0
        best_error = float('inf')
    else:
        print("âœ… ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿æˆåŠŸ")
        model = NKATFineTuneNetwork(grid_size=64).to(device)
        model.load_state_dict(checkpoint['model_state_dict'])
        start_epoch = checkpoint['epoch']
        
        # æœ€è‰¯èª¤å·®ã‚’å±¥æ­´ã‹ã‚‰å–å¾—
        if 'metrics' in checkpoint and checkpoint['metrics']:
            last_metrics = checkpoint['metrics'][-1]
            best_error = abs(last_metrics.get('spectral_dim', 4.0) - 4.0)
        else:
            best_error = float('inf')
        
        print(f"ğŸ“Š é–‹å§‹ã‚¨ãƒãƒƒã‚¯: {start_epoch}")
        print(f"ğŸ“Š ç¾åœ¨ã®æœ€è‰¯èª¤å·®: {best_error:.2e}")
    
    # å¾®èª¿æ•´ç”¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ï¼ˆä½å­¦ç¿’ç‡ï¼‰
    optimizer = optim.AdamW(model.parameters(), 
                           lr=1e-5,  # å¾®èª¿æ•´ç”¨ä½å­¦ç¿’ç‡
                           weight_decay=1e-5)
    
    # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )
    
    physics_loss = NKATFineTuneLoss(device)
    
    # å¾®èª¿æ•´ç”¨é‡ã¿ï¼ˆç²¾å¯†èª¿æ•´ï¼‰
    weights = {
        'spectral': 20.0,  # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«æ¬¡å…ƒé‡è¦–
        'theta': 2.0,
        'jacobi': 1.0,
        'connes': 1.0
    }
    
    # è¨“ç·´å±¥æ­´
    fine_tune_history = []
    patience_counter = 0
    max_patience = 10
    
    print(f"\nğŸ¯ å¾®èª¿æ•´è¨“ç·´é–‹å§‹ï¼")
    print(f"ğŸ“Š å­¦ç¿’ç‡: {optimizer.param_groups[0]['lr']:.2e}")
    print(f"ğŸ“Š é‡ã¿: {weights}")
    
    start_time = time.time()
    
    # å¾®èª¿æ•´ãƒ«ãƒ¼ãƒ—ï¼ˆ20ã‚¨ãƒãƒƒã‚¯ï¼‰
    for epoch in range(start_epoch + 1, start_epoch + 21):
        epoch_start = time.time()
        
        model.train()
        
        # é«˜ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        batch_size = 512  # å¾®èª¿æ•´ç”¨å¤§ãƒãƒƒãƒ
        x = torch.randn(batch_size, 4, device=device)
        x_time_positive = torch.abs(x[:, 0])
        x = torch.cat([x_time_positive.unsqueeze(1), x[:, 1:]], dim=1)
        x.requires_grad_(True)
        
        # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        output = model(x)
        
        # è¶…é«˜ç²¾åº¦æå¤±è¨ˆç®—
        spectral_loss, spectral_dim = physics_loss.ultra_precision_spectral_loss(output)
        theta_loss, theta_value = physics_loss.ultra_precision_theta_loss(output)
        
        # ç·åˆæå¤±
        total_loss = (weights['spectral'] * spectral_loss + 
                      weights['theta'] * theta_loss)
        
        # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
        optimizer.zero_grad()
        total_loss.backward()
        
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼ˆå¾®èª¿æ•´ç”¨ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)
        
        optimizer.step()
        
        epoch_time = time.time() - epoch_start
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        spectral_error = abs(spectral_dim - 4.0)
        metrics = {
            'epoch': epoch,
            'total_loss': total_loss.item(),
            'spectral_loss': spectral_loss.item(),
            'spectral_dim': spectral_dim,
            'spectral_error': spectral_error,
            'theta_loss': theta_loss.item(),
            'theta_value': theta_value,
            'epoch_time': epoch_time,
            'lr': optimizer.param_groups[0]['lr']
        }
        fine_tune_history.append(metrics)
        
        # é€²æ—è¡¨ç¤º
        print(f"ğŸ¯ Epoch {epoch:3d}/20 | "
              f"Loss: {total_loss.item():.3e} | "
              f"d_s: {spectral_dim:.8f} | "
              f"Error: {spectral_error:.3e} | "
              f"Î¸: {theta_value:.2e} | "
              f"LR: {optimizer.param_groups[0]['lr']:.2e}")
        
        # ãƒ™ã‚¹ãƒˆæ›´æ–°ãƒã‚§ãƒƒã‚¯
        if spectral_error < best_error:
            best_error = spectral_error
            patience_counter = 0
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
            checkpoint_dir = Path("nkat_fine_tune_checkpoints")
            checkpoint_dir.mkdir(exist_ok=True)
            best_path = checkpoint_dir / "best_fine_tune.pth"
            
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_error': best_error,
                'metrics': fine_tune_history
            }, best_path)
            
            if spectral_error < 1e-5:
                print(f"ğŸ† ç›®æ¨™é”æˆï¼ Error: {spectral_error:.3e} < 1Ã—10â»âµ")
                break
        else:
            patience_counter += 1
        
        # å­¦ç¿’ç‡èª¿æ•´
        scheduler.step(spectral_error)
        
        # æ—©æœŸåœæ­¢
        if patience_counter >= max_patience:
            print(f"ğŸ›‘ æ—©æœŸåœæ­¢ (patience={max_patience})")
            break
    
    # å¾®èª¿æ•´å®Œäº†
    total_time = time.time() - start_time
    print(f"\nğŸ‰ å¾®èª¿æ•´å®Œäº†ï¼")
    print(f"â±ï¸ ç·æ™‚é–“: {total_time:.1f}ç§’")
    print(f"ğŸ¯ æœ€çµ‚ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«æ¬¡å…ƒ: {fine_tune_history[-1]['spectral_dim']:.10f}")
    print(f"ğŸ¯ æœ€å°èª¤å·®: {best_error:.3e}")
    
    # çµæœä¿å­˜
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # å±¥æ­´ä¿å­˜
    history_file = f"nkat_fine_tune_history_{timestamp}.json"
    with open(history_file, 'w') as f:
        json.dump(fine_tune_history, f, indent=2)
    
    # å¾®èª¿æ•´ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
    generate_fine_tune_plot(fine_tune_history, timestamp)
    
    print(f"ğŸ“Š å¾®èª¿æ•´çµæœä¿å­˜: {history_file}")
    
    return fine_tune_history, best_error

def generate_fine_tune_plot(history, timestamp):
    """å¾®èª¿æ•´çµæœãƒ—ãƒ­ãƒƒãƒˆ"""
    epochs = [h['epoch'] for h in history]
    spectral_dims = [h['spectral_dim'] for h in history]
    spectral_errors = [h['spectral_error'] for h in history]
    learning_rates = [h['lr'] for h in history]
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«æ¬¡å…ƒå¾®èª¿æ•´
    ax1.plot(epochs, spectral_dims, 'b-', linewidth=2, marker='o', markersize=3)
    ax1.axhline(y=4.0, color='r', linestyle='--', label='Target d_s = 4.0')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Spectral Dimension')
    ax1.set_title('Fine-Tune: Spectral Dimension Convergence')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # èª¤å·®åæŸï¼ˆå¯¾æ•°ï¼‰
    ax2.semilogy(epochs, spectral_errors, 'g-', linewidth=2, marker='s', markersize=3)
    ax2.axhline(y=1e-5, color='r', linestyle='--', label='Target < 1e-5')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Spectral Dimension Error')
    ax2.set_title('Ultra-Precision Error Convergence')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # å­¦ç¿’ç‡å¤‰åŒ–
    ax3.semilogy(epochs, learning_rates, 'orange', linewidth=2, marker='^', markersize=3)
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('Learning Rate')
    ax3.set_title('Adaptive Learning Rate')
    ax3.grid(True, alpha=0.3)
    
    # èª¤å·®æ”¹å–„ç‡
    if len(spectral_errors) > 1:
        improvement_rates = []
        for i in range(1, len(spectral_errors)):
            if spectral_errors[i-1] > 0:
                rate = (spectral_errors[i-1] - spectral_errors[i]) / spectral_errors[i-1] * 100
                improvement_rates.append(rate)
            else:
                improvement_rates.append(0)
        
        ax4.plot(epochs[1:], improvement_rates, 'purple', linewidth=2, marker='d', markersize=3)
        ax4.set_xlabel('Epoch')
        ax4.set_ylabel('Error Improvement Rate (%)')
        ax4.set_title('Fine-Tune Improvement Rate')
        ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    plot_file = f"nkat_fine_tune_results_{timestamp}.png"
    plt.savefig(plot_file, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"ğŸ“Š å¾®èª¿æ•´ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜: {plot_file}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        history, best_error = fine_tune_training()
        
        print(f"\nğŸ† å¾®èª¿æ•´å®Œäº†ï¼")
        print(f"ğŸ¯ é”æˆç²¾åº¦: {best_error:.3e}")
        
        if best_error < 1e-5:
            print("ğŸ‰ ç›®æ¨™é”æˆï¼ èª¤å·® < 1Ã—10â»âµ")
        else:
            print(f"ğŸ“ˆ ç›®æ¨™ã¾ã§: {best_error/1e-5:.1f}å€")
            
    except Exception as e:
        print(f"âŒ å¾®èª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ NKATç†è«–ã«ã‚ˆã‚‹ç†è«–çš„çµ±åˆãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼
Theoretical Unification Verification of Riemann Hypothesis using NKAT Theory

çµ±åˆç†è«–:
- é‡å­å ´ç†è«– (Quantum Field Theory)
- ä»£æ•°å¹¾ä½•å­¦ (Algebraic Geometry) 
- è§£ææ•°è«– (Analytic Number Theory)
- éå¯æ›å¹¾ä½•å­¦ (Noncommutative Geometry)
- ã‚¹ãƒšã‚¯ãƒˆãƒ«ç†è«– (Spectral Theory)

Author: NKAT Research Team
Date: 2025-05-24
Version: 8.0 - Theoretical Unification & Ultimate Precision
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Union, Callable
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass, field
from tqdm import tqdm, trange
import logging
from scipy import special, optimize, linalg, integrate
import math
from abc import ABC, abstractmethod
from enum import Enum
import sympy as sp
from functools import lru_cache

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

class TheoreticalFramework(Enum):
    """ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®åˆ—æŒ™"""
    QUANTUM_FIELD_THEORY = "QFT"
    ALGEBRAIC_GEOMETRY = "AG"
    ANALYTIC_NUMBER_THEORY = "ANT"
    NONCOMMUTATIVE_GEOMETRY = "NCG"
    SPECTRAL_THEORY = "ST"

@dataclass
class UnifiedNKATParameters:
    """çµ±åˆNKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"""
    # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    theta: float = 1e-22  # éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè¶…é«˜ç²¾åº¦ï¼‰
    kappa: float = 1e-14  # Îº-å¤‰å½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè¶…é«˜ç²¾åº¦ï¼‰
    max_n: int = 2000     # æœ€å¤§æ¬¡å…ƒï¼ˆæ‹¡å¼µï¼‰
    precision: str = 'ultimate'  # ç©¶æ¥µç²¾åº¦
    
    # ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
    frameworks: List[TheoreticalFramework] = field(default_factory=lambda: list(TheoreticalFramework))
    
    # é‡å­å ´ç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    coupling_constant: float = 1e-3
    mass_scale: float = 1.0
    renormalization_scale: float = 1.0
    
    # ä»£æ•°å¹¾ä½•å­¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    genus: int = 2
    degree: int = 3
    dimension: int = 4
    
    # è§£ææ•°è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    conductor: int = 1
    weight: int = 2
    level: int = 1
    
    # éå¯æ›å¹¾ä½•å­¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    nc_dimension: float = 2.0
    spectral_triple_data: Dict = field(default_factory=dict)
    
    # æ•°å€¤è¨ˆç®—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    tolerance: float = 1e-16
    max_iterations: int = 10000
    convergence_threshold: float = 1e-15
    
    def validate(self) -> bool:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§æ¤œè¨¼"""
        return (0 < self.theta < 1e-5 and
                0 < self.kappa < 1e-5 and
                self.max_n > 0 and
                self.tolerance > 0 and
                self.convergence_threshold > 0)

class AbstractTheoreticalOperator(ABC):
    """ç†è«–çš„æ¼”ç®—å­ã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def construct_operator(self, s: complex, framework: TheoreticalFramework) -> torch.Tensor:
        """ç†è«–çš„æ¼”ç®—å­ã®æ§‹ç¯‰"""
        pass
    
    @abstractmethod
    def compute_spectrum(self, s: complex, framework: TheoreticalFramework) -> torch.Tensor:
        """ç†è«–çš„ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®è¨ˆç®—"""
        pass

class QuantumFieldTheoryOperator:
    """é‡å­å ´ç†è«–æ¼”ç®—å­"""
    
    def __init__(self, params: UnifiedNKATParameters):
        self.params = params
        self.device = device
        
    def construct_qft_hamiltonian(self, s: complex) -> torch.Tensor:
        """QFTãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®æ§‹ç¯‰"""
        dim = min(self.params.max_n, 800)
        H = torch.zeros(dim, dim, dtype=torch.complex128, device=self.device)
        
        # è‡ªç”±å ´é …
        self._add_free_field_terms(H, s, dim)
        
        # ç›¸äº’ä½œç”¨é …
        self._add_interaction_terms(H, s, dim)
        
        # è³ªé‡é …
        self._add_mass_terms(H, s, dim)
        
        # ç¹°ã‚Šè¾¼ã¿é …
        self._add_renormalization_terms(H, s, dim)
        
        return H
    
    def _add_free_field_terms(self, H: torch.Tensor, s: complex, dim: int):
        """è‡ªç”±å ´é …ã®è¿½åŠ """
        for n in range(1, dim + 1):
            # é‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼é …
            kinetic_energy = n * n * self.params.coupling_constant
            H[n-1, n-1] += torch.tensor(kinetic_energy, dtype=torch.complex128, device=self.device)
            
            # ã‚¼ãƒ¼ã‚¿é–¢æ•°é …
            try:
                zeta_term = 1.0 / (n ** s)
                if np.isfinite(zeta_term):
                    H[n-1, n-1] += torch.tensor(zeta_term, dtype=torch.complex128, device=self.device)
            except:
                pass
    
    def _add_interaction_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ç›¸äº’ä½œç”¨é …ã®è¿½åŠ """
        coupling = self.params.coupling_constant
        
        for i in range(min(dim, 100)):
            for j in range(i+1, min(dim, i+20)):
                # Ï†^4ç›¸äº’ä½œç”¨
                interaction = coupling * np.sqrt((i+1) * (j+1)) * 1e-6
                H[i, j] += torch.tensor(interaction, dtype=torch.complex128, device=self.device)
                H[j, i] += torch.tensor(interaction.conjugate(), dtype=torch.complex128, device=self.device)
    
    def _add_mass_terms(self, H: torch.Tensor, s: complex, dim: int):
        """è³ªé‡é …ã®è¿½åŠ """
        mass_scale = self.params.mass_scale
        
        for n in range(1, min(dim + 1, 200)):
            mass_term = mass_scale * (0.5 - s.real) / n
            H[n-1, n-1] += torch.tensor(mass_term, dtype=torch.complex128, device=self.device)
    
    def _add_renormalization_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ç¹°ã‚Šè¾¼ã¿é …ã®è¿½åŠ """
        mu = self.params.renormalization_scale
        
        for n in range(1, min(dim + 1, 50)):
            # ä¸€ãƒ«ãƒ¼ãƒ—è£œæ­£
            beta_function = self.params.coupling_constant ** 2 / (16 * np.pi ** 2)
            renorm_term = beta_function * np.log(mu / n) * 1e-8
            H[n-1, n-1] += torch.tensor(renorm_term, dtype=torch.complex128, device=self.device)

class AlgebraicGeometryOperator:
    """ä»£æ•°å¹¾ä½•å­¦æ¼”ç®—å­"""
    
    def __init__(self, params: UnifiedNKATParameters):
        self.params = params
        self.device = device
        
    def construct_ag_operator(self, s: complex) -> torch.Tensor:
        """ä»£æ•°å¹¾ä½•å­¦æ¼”ç®—å­ã®æ§‹ç¯‰"""
        dim = min(self.params.max_n, 600)
        H = torch.zeros(dim, dim, dtype=torch.complex128, device=self.device)
        
        # ãƒ¢ãƒãƒ¼ãƒ•ã®Lé–¢æ•°é …
        self._add_motif_l_function_terms(H, s, dim)
        
        # æ¥•å††æ›²ç·šé …
        self._add_elliptic_curve_terms(H, s, dim)
        
        # ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼å½¢å¼é …
        self._add_modular_form_terms(H, s, dim)
        
        # ã‚¬ãƒ­ã‚¢è¡¨ç¾é …
        self._add_galois_representation_terms(H, s, dim)
        
        return H
    
    def _add_motif_l_function_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ãƒ¢ãƒãƒ¼ãƒ•ã®Lé–¢æ•°é …"""
        for n in range(1, dim + 1):
            # Lé–¢æ•°ã®ä¿‚æ•°
            try:
                # ç°¡ç•¥åŒ–ã•ã‚ŒãŸãƒ¢ãƒãƒ¼ãƒ•Lé–¢æ•°
                l_coeff = self._compute_motif_coefficient(n, s)
                H[n-1, n-1] += torch.tensor(l_coeff, dtype=torch.complex128, device=self.device)
            except:
                pass
    
    def _compute_motif_coefficient(self, n: int, s: complex) -> complex:
        """ãƒ¢ãƒãƒ¼ãƒ•ä¿‚æ•°ã®è¨ˆç®—"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸè¨ˆç®—
        genus = self.params.genus
        degree = self.params.degree
        
        base_term = 1.0 / (n ** s)
        geometric_factor = (1 + 1/n) ** (-genus)
        degree_factor = n ** (-degree/2)
        
        return base_term * geometric_factor * degree_factor
    
    def _add_elliptic_curve_terms(self, H: torch.Tensor, s: complex, dim: int):
        """æ¥•å††æ›²ç·šé …"""
        for p in self._get_small_primes(min(dim, 100)):
            if p >= dim:
                break
            
            # Hasseå¢ƒç•Œã«ã‚ˆã‚‹è£œæ­£
            hasse_bound = 2 * np.sqrt(p)
            elliptic_term = (1 - hasse_bound / p) * 1e-6
            H[p-1, p-1] += torch.tensor(elliptic_term, dtype=torch.complex128, device=self.device)
    
    def _add_modular_form_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼å½¢å¼é …"""
        weight = self.params.weight
        level = self.params.level
        
        for n in range(1, min(dim + 1, 150)):
            # ãƒ©ãƒãƒŒã‚¸ãƒ£ãƒ³ã®Ï„é–¢æ•°é¢¨
            tau_like = self._ramanujan_tau_like(n) * 1e-10
            modular_term = tau_like / (n ** (weight/2))
            H[n-1, n-1] += torch.tensor(modular_term, dtype=torch.complex128, device=self.device)
    
    def _ramanujan_tau_like(self, n: int) -> float:
        """ãƒ©ãƒãƒŒã‚¸ãƒ£ãƒ³Ï„é–¢æ•°é¢¨ã®è¨ˆç®—"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸè¿‘ä¼¼
        return (-1) ** (n % 2) * (n % 691) * np.log(n + 1)
    
    def _add_galois_representation_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ã‚¬ãƒ­ã‚¢è¡¨ç¾é …"""
        for n in range(1, min(dim + 1, 80)):
            # ã‚¬ãƒ­ã‚¢ç¾¤ã®ä½œç”¨
            galois_action = np.exp(2j * np.pi * n / 12) * 1e-8
            H[n-1, n-1] += torch.tensor(galois_action, dtype=torch.complex128, device=self.device)
    
    def _get_small_primes(self, limit: int) -> List[int]:
        """å°ã•ãªç´ æ•°ã®ãƒªã‚¹ãƒˆ"""
        primes = []
        for n in range(2, limit + 1):
            if all(n % p != 0 for p in primes):
                primes.append(n)
        return primes

class AnalyticNumberTheoryOperator:
    """è§£ææ•°è«–æ¼”ç®—å­"""
    
    def __init__(self, params: UnifiedNKATParameters):
        self.params = params
        self.device = device
        
    def construct_ant_operator(self, s: complex) -> torch.Tensor:
        """è§£ææ•°è«–æ¼”ç®—å­ã®æ§‹ç¯‰"""
        dim = min(self.params.max_n, 1000)
        H = torch.zeros(dim, dim, dtype=torch.complex128, device=self.device)
        
        # ãƒ‡ã‚£ãƒªã‚¯ãƒ¬Lé–¢æ•°é …
        self._add_dirichlet_l_function_terms(H, s, dim)
        
        # ç´ æ•°å®šç†é …
        self._add_prime_number_theorem_terms(H, s, dim)
        
        # ãƒãƒ¼ãƒ‡ã‚£ãƒ»ãƒªãƒˆãƒ«ã‚¦ãƒƒãƒ‰äºˆæƒ³é …
        self._add_hardy_littlewood_terms(H, s, dim)
        
        # æ˜ç¤ºå…¬å¼é …
        self._add_explicit_formula_terms(H, s, dim)
        
        return H
    
    def _add_dirichlet_l_function_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ãƒ‡ã‚£ãƒªã‚¯ãƒ¬Lé–¢æ•°é …"""
        conductor = self.params.conductor
        
        for n in range(1, dim + 1):
            # ãƒ‡ã‚£ãƒªã‚¯ãƒ¬æŒ‡æ¨™
            chi_n = self._dirichlet_character(n, conductor)
            dirichlet_term = chi_n / (n ** s)
            
            if np.isfinite(dirichlet_term):
                H[n-1, n-1] += torch.tensor(dirichlet_term, dtype=torch.complex128, device=self.device)
    
    def _dirichlet_character(self, n: int, conductor: int) -> complex:
        """ãƒ‡ã‚£ãƒªã‚¯ãƒ¬æŒ‡æ¨™ã®è¨ˆç®—"""
        if conductor == 1:
            return 1.0  # ä¸»æŒ‡æ¨™
        else:
            # ç°¡ç•¥åŒ–ã•ã‚ŒãŸéä¸»æŒ‡æ¨™
            return np.exp(2j * np.pi * n / conductor) if math.gcd(n, conductor) == 1 else 0.0
    
    def _add_prime_number_theorem_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ç´ æ•°å®šç†é …"""
        for p in self._sieve_of_eratosthenes(min(dim, 200)):
            if p >= dim:
                break
            
            # von Mangoldté–¢æ•°
            lambda_p = np.log(p)
            pnt_term = lambda_p / (p ** s) * 1e-6
            
            if np.isfinite(pnt_term):
                H[p-1, p-1] += torch.tensor(pnt_term, dtype=torch.complex128, device=self.device)
    
    def _add_hardy_littlewood_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ãƒãƒ¼ãƒ‡ã‚£ãƒ»ãƒªãƒˆãƒ«ã‚¦ãƒƒãƒ‰äºˆæƒ³é …"""
        for n in range(1, min(dim + 1, 100)):
            # åŒå­ç´ æ•°äºˆæƒ³é–¢é€£
            twin_prime_density = 1.32032 / (np.log(n) ** 2) if n > 2 else 0
            hl_term = twin_prime_density * 1e-8
            H[n-1, n-1] += torch.tensor(hl_term, dtype=torch.complex128, device=self.device)
    
    def _add_explicit_formula_terms(self, H: torch.Tensor, s: complex, dim: int):
        """æ˜ç¤ºå…¬å¼é …"""
        # ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®é›¶ç‚¹ã«ã‚ˆã‚‹è£œæ­£
        known_zeros = [14.134725, 21.022040, 25.010858, 30.424876, 32.935062]
        
        for i, gamma in enumerate(known_zeros[:min(len(known_zeros), 20)]):
            if i >= dim:
                break
            
            # æ˜ç¤ºå…¬å¼ã®é …
            rho = 0.5 + 1j * gamma
            explicit_term = 1.0 / (s - rho) * 1e-10
            
            if np.isfinite(explicit_term) and i < dim:
                H[i, i] += torch.tensor(explicit_term, dtype=torch.complex128, device=self.device)
    
    def _sieve_of_eratosthenes(self, limit: int) -> List[int]:
        """ã‚¨ãƒ©ãƒˆã‚¹ãƒ†ãƒã‚¹ã®ç¯©"""
        sieve = [True] * (limit + 1)
        sieve[0] = sieve[1] = False
        
        for i in range(2, int(limit**0.5) + 1):
            if sieve[i]:
                for j in range(i*i, limit + 1, i):
                    sieve[j] = False
        
        return [i for i in range(2, limit + 1) if sieve[i]]

class NoncommutativeGeometryOperator:
    """éå¯æ›å¹¾ä½•å­¦æ¼”ç®—å­"""
    
    def __init__(self, params: UnifiedNKATParameters):
        self.params = params
        self.device = device
        
    def construct_ncg_operator(self, s: complex) -> torch.Tensor:
        """éå¯æ›å¹¾ä½•å­¦æ¼”ç®—å­ã®æ§‹ç¯‰"""
        dim = min(self.params.max_n, 700)
        H = torch.zeros(dim, dim, dtype=torch.complex128, device=self.device)
        
        # ã‚¹ãƒšã‚¯ãƒˆãƒ«ä¸‰é‡é …
        self._add_spectral_triple_terms(H, s, dim)
        
        # Connesè·é›¢é …
        self._add_connes_distance_terms(H, s, dim)
        
        # éå¯æ›å¾®åˆ†å½¢å¼é …
        self._add_nc_differential_form_terms(H, s, dim)
        
        # KOåŒæ¬¡é …
        self._add_ko_homology_terms(H, s, dim)
        
        return H
    
    def _add_spectral_triple_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«ä¸‰é‡é …é …"""
        nc_dim = self.params.nc_dimension
        theta = self.params.theta
        
        for n in range(1, dim + 1):
            # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯æ¼”ç®—å­ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«
            dirac_eigenvalue = n ** (1/nc_dim)
            spectral_term = theta * dirac_eigenvalue / (n ** s) * 1e-6
            
            if np.isfinite(spectral_term):
                H[n-1, n-1] += torch.tensor(spectral_term, dtype=torch.complex128, device=self.device)
    
    def _add_connes_distance_terms(self, H: torch.Tensor, s: complex, dim: int):
        """Connesè·é›¢é …"""
        for i in range(min(dim, 150)):
            for j in range(i+1, min(dim, i+10)):
                # Connesè·é›¢
                connes_dist = abs(i - j) * self.params.theta * 1e-8
                H[i, j] += torch.tensor(connes_dist * 1j, dtype=torch.complex128, device=self.device)
                H[j, i] -= torch.tensor(connes_dist * 1j, dtype=torch.complex128, device=self.device)
    
    def _add_nc_differential_form_terms(self, H: torch.Tensor, s: complex, dim: int):
        """éå¯æ›å¾®åˆ†å½¢å¼é …"""
        kappa = self.params.kappa
        
        for n in range(1, min(dim + 1, 100)):
            # éå¯æ›å¾®åˆ†
            nc_diff = kappa * n * np.log(n + 1) * 1e-7
            H[n-1, n-1] += torch.tensor(nc_diff, dtype=torch.complex128, device=self.device)
    
    def _add_ko_homology_terms(self, H: torch.Tensor, s: complex, dim: int):
        """KOåŒæ¬¡é …"""
        for n in range(1, min(dim + 1, 80)):
            # Kç†è«–çš„è£œæ­£
            k_theory_term = (-1) ** n * self.params.theta / n * 1e-9
            H[n-1, n-1] += torch.tensor(k_theory_term, dtype=torch.complex128, device=self.device)

class TheoreticalUnificationNKATHamiltonian(nn.Module, AbstractTheoreticalOperator):
    """
    ç†è«–çš„çµ±åˆNKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³
    
    çµ±åˆç†è«–:
    1. é‡å­å ´ç†è«– (QFT)
    2. ä»£æ•°å¹¾ä½•å­¦ (AG)
    3. è§£ææ•°è«– (ANT)
    4. éå¯æ›å¹¾ä½•å­¦ (NCG)
    5. ã‚¹ãƒšã‚¯ãƒˆãƒ«ç†è«– (ST)
    """
    
    def __init__(self, params: UnifiedNKATParameters):
        super().__init__()
        self.params = params
        if not params.validate():
            raise ValueError("ç„¡åŠ¹ãªçµ±åˆNKATãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™")
        
        self.device = device
        
        # ç©¶æ¥µç²¾åº¦è¨­å®š
        if params.precision == 'ultimate':
            self.dtype = torch.complex128
            self.float_dtype = torch.float64
            torch.set_default_dtype(torch.float64)
        else:
            self.dtype = torch.complex64
            self.float_dtype = torch.float32
        
        logger.info(f"ğŸ”§ ç†è«–çš„çµ±åˆNKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³åˆæœŸåŒ–")
        logger.info(f"   Î¸={params.theta:.2e}, Îº={params.kappa:.2e}, æ¬¡å…ƒ={params.max_n}")
        logger.info(f"   çµ±åˆç†è«–: {[f.value for f in params.frameworks]}")
        
        # ç†è«–çš„æ¼”ç®—å­ã®åˆæœŸåŒ–
        self.qft_operator = QuantumFieldTheoryOperator(params)
        self.ag_operator = AlgebraicGeometryOperator(params)
        self.ant_operator = AnalyticNumberTheoryOperator(params)
        self.ncg_operator = NoncommutativeGeometryOperator(params)
        
        # é‡ã¿ä¿‚æ•°
        self.framework_weights = {
            TheoreticalFramework.QUANTUM_FIELD_THEORY: 0.25,
            TheoreticalFramework.ALGEBRAIC_GEOMETRY: 0.25,
            TheoreticalFramework.ANALYTIC_NUMBER_THEORY: 0.25,
            TheoreticalFramework.NONCOMMUTATIVE_GEOMETRY: 0.15,
            TheoreticalFramework.SPECTRAL_THEORY: 0.10
        }
    
    def construct_operator(self, s: complex, framework: TheoreticalFramework = None) -> torch.Tensor:
        """çµ±åˆç†è«–çš„æ¼”ç®—å­ã®æ§‹ç¯‰"""
        if framework:
            return self._construct_single_framework_operator(s, framework)
        else:
            return self._construct_unified_operator(s)
    
    def _construct_unified_operator(self, s: complex) -> torch.Tensor:
        """çµ±åˆæ¼”ç®—å­ã®æ§‹ç¯‰"""
        dim = min(self.params.max_n, 500)
        H_unified = torch.zeros(dim, dim, dtype=self.dtype, device=self.device)
        
        # å„ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‹ã‚‰ã®å¯„ä¸
        for framework in self.params.frameworks:
            weight = self.framework_weights.get(framework, 0.1)
            
            try:
                H_framework = self._construct_single_framework_operator(s, framework)
                # ã‚µã‚¤ã‚ºèª¿æ•´
                min_dim = min(H_unified.shape[0], H_framework.shape[0])
                H_unified[:min_dim, :min_dim] += weight * H_framework[:min_dim, :min_dim]
                
                logger.debug(f"âœ… {framework.value}ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµ±åˆå®Œäº† (é‡ã¿: {weight})")
                
            except Exception as e:
                logger.warning(f"âš ï¸ {framework.value}ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµ±åˆå¤±æ•—: {e}")
                continue
        
        # çµ±åˆè£œæ­£é …
        self._add_unification_corrections(H_unified, s)
        
        # æ­£å‰‡åŒ–
        reg_strength = self.params.tolerance
        H_unified += reg_strength * torch.eye(H_unified.shape[0], dtype=self.dtype, device=self.device)
        
        return H_unified
    
    def _construct_single_framework_operator(self, s: complex, framework: TheoreticalFramework) -> torch.Tensor:
        """å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ¼”ç®—å­ã®æ§‹ç¯‰"""
        if framework == TheoreticalFramework.QUANTUM_FIELD_THEORY:
            return self.qft_operator.construct_qft_hamiltonian(s)
        elif framework == TheoreticalFramework.ALGEBRAIC_GEOMETRY:
            return self.ag_operator.construct_ag_operator(s)
        elif framework == TheoreticalFramework.ANALYTIC_NUMBER_THEORY:
            return self.ant_operator.construct_ant_operator(s)
        elif framework == TheoreticalFramework.NONCOMMUTATIVE_GEOMETRY:
            return self.ncg_operator.construct_ncg_operator(s)
        elif framework == TheoreticalFramework.SPECTRAL_THEORY:
            return self._construct_spectral_theory_operator(s)
        else:
            raise ValueError(f"æœªçŸ¥ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯: {framework}")
    
    def _construct_spectral_theory_operator(self, s: complex) -> torch.Tensor:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«ç†è«–æ¼”ç®—å­ã®æ§‹ç¯‰"""
        dim = min(self.params.max_n, 600)
        H = torch.zeros(dim, dim, dtype=self.dtype, device=self.device)
        
        # åŸºæœ¬ã‚¹ãƒšã‚¯ãƒˆãƒ«é …
        for n in range(1, dim + 1):
            eigenvalue = n ** (-s.real) * np.exp(-s.imag * np.log(n))
            if np.isfinite(eigenvalue):
                H[n-1, n-1] = torch.tensor(eigenvalue, dtype=self.dtype, device=self.device)
        
        return H
    
    def _add_unification_corrections(self, H: torch.Tensor, s: complex):
        """çµ±åˆè£œæ­£é …ã®è¿½åŠ """
        dim = H.shape[0]
        
        # ç†è«–é–“ç›¸äº’ä½œç”¨é …
        for i in range(min(dim, 50)):
            for j in range(i+1, min(dim, i+5)):
                # çµ±åˆç›¸äº’ä½œç”¨
                interaction = self.params.theta * self.params.kappa * np.sqrt((i+1) * (j+1)) * 1e-10
                H[i, j] += torch.tensor(interaction * 1j, dtype=self.dtype, device=self.device)
                H[j, i] -= torch.tensor(interaction * 1j, dtype=self.dtype, device=self.device)
    
    def compute_spectrum(self, s: complex, framework: TheoreticalFramework = None) -> torch.Tensor:
        """çµ±åˆã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—"""
        try:
            H = self.construct_operator(s, framework)
            
            # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–
            H_hermitian = 0.5 * (H + H.conj().T)
            
            # å‰å‡¦ç†
            H_processed = self._advanced_preprocess_matrix(H_hermitian)
            
            # é«˜ç²¾åº¦å›ºæœ‰å€¤è¨ˆç®—
            eigenvalues = self._ultimate_precision_eigenvalue_computation(H_processed)
            
            if eigenvalues is None or len(eigenvalues) == 0:
                logger.warning("âš ï¸ çµ±åˆã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            # æ­£ã®å›ºæœ‰å€¤ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            positive_mask = eigenvalues > self.params.tolerance
            positive_eigenvalues = eigenvalues[positive_mask]
            
            if len(positive_eigenvalues) == 0:
                logger.warning("âš ï¸ æ­£ã®å›ºæœ‰å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            # ã‚½ãƒ¼ãƒˆ
            sorted_eigenvalues, _ = torch.sort(positive_eigenvalues, descending=True)
            
            return sorted_eigenvalues[:min(len(sorted_eigenvalues), 300)]
            
        except Exception as e:
            logger.error(f"âŒ çµ±åˆã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return torch.tensor([], device=self.device, dtype=self.float_dtype)
    
    def _advanced_preprocess_matrix(self, H: torch.Tensor) -> torch.Tensor:
        """é«˜åº¦ãªè¡Œåˆ—å‰å‡¦ç†"""
        try:
            # ç‰¹ç•°å€¤åˆ†è§£ã«ã‚ˆã‚‹å‰å‡¦ç†
            U, S, Vh = torch.linalg.svd(H)
            
            # é©å¿œçš„é–¾å€¤
            threshold = max(self.params.tolerance, S.max().item() * 1e-14)
            S_filtered = torch.where(S > threshold, S, threshold)
            
            # æ¡ä»¶æ•°åˆ¶å¾¡
            condition_number = S_filtered.max() / S_filtered.min()
            if condition_number > 1e15:
                reg_strength = S_filtered.max() * 1e-15
                S_filtered += reg_strength
            
            # å†æ§‹ç¯‰
            H_processed = torch.mm(torch.mm(U, torch.diag(S_filtered)), Vh)
            
            return H_processed
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            reg_strength = self.params.tolerance
            return H + reg_strength * torch.eye(H.shape[0], dtype=self.dtype, device=self.device)
    
    def _ultimate_precision_eigenvalue_computation(self, H: torch.Tensor) -> Optional[torch.Tensor]:
        """ç©¶æ¥µç²¾åº¦å›ºæœ‰å€¤è¨ˆç®—"""
        methods = [
            ('eigh_cpu', lambda: torch.linalg.eigh(H.cpu())[0].real.to(self.device)),
            ('eigh_gpu', lambda: torch.linalg.eigh(H)[0].real),
            ('svd', lambda: torch.linalg.svd(H)[1].real),
            ('eig', lambda: torch.linalg.eig(H)[0].real)
        ]
        
        for method_name, method_func in methods:
            try:
                eigenvalues = method_func()
                if torch.isfinite(eigenvalues).all() and len(eigenvalues) > 0:
                    logger.debug(f"âœ… {method_name}ã«ã‚ˆã‚‹ç©¶æ¥µç²¾åº¦å›ºæœ‰å€¤è¨ˆç®—æˆåŠŸ")
                    return eigenvalues
            except Exception as e:
                logger.debug(f"âš ï¸ {method_name}ã«ã‚ˆã‚‹å›ºæœ‰å€¤è¨ˆç®—å¤±æ•—: {e}")
                continue
        
        return None

class TheoreticalUnificationRiemannVerifier:
    """
    ç†è«–çš„çµ±åˆãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, hamiltonian: TheoreticalUnificationNKATHamiltonian):
        self.hamiltonian = hamiltonian
        self.device = hamiltonian.device
        
    def compute_unified_spectral_dimension(self, s: complex, 
                                         n_points: int = 150, 
                                         t_range: Tuple[float, float] = (1e-8, 5.0),
                                         method: str = 'unified') -> float:
        """
        çµ±åˆç†è«–çš„ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
        """
        # å„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‹ã‚‰ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«
        framework_spectra = {}
        
        for framework in self.hamiltonian.params.frameworks:
            try:
                eigenvalues = self.hamiltonian.compute_spectrum(s, framework)
                if len(eigenvalues) >= 15:
                    framework_spectra[framework] = eigenvalues
                    logger.debug(f"âœ… {framework.value}ã‚¹ãƒšã‚¯ãƒˆãƒ«å–å¾—: {len(eigenvalues)}å€‹")
            except Exception as e:
                logger.warning(f"âš ï¸ {framework.value}ã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—å¤±æ•—: {e}")
                continue
        
        if not framework_spectra:
            logger.warning("âš ï¸ æœ‰åŠ¹ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšã‚¯ãƒˆãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
            return float('nan')
        
        # çµ±åˆã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
        if method == 'unified':
            return self._compute_unified_dimension(framework_spectra, n_points, t_range)
        elif method == 'weighted_average':
            return self._compute_weighted_average_dimension(framework_spectra, n_points, t_range)
        else:
            return self._compute_consensus_dimension(framework_spectra, n_points, t_range)
    
    def _compute_unified_dimension(self, framework_spectra: Dict, 
                                 n_points: int, t_range: Tuple[float, float]) -> float:
        """çµ±åˆæ¬¡å…ƒè¨ˆç®—"""
        dimensions = []
        weights = []
        
        for framework, eigenvalues in framework_spectra.items():
            try:
                dim = self._single_framework_spectral_dimension(eigenvalues, n_points, t_range)
                if not np.isnan(dim):
                    dimensions.append(dim)
                    weight = self.hamiltonian.framework_weights.get(framework, 0.1)
                    weights.append(weight)
            except:
                continue
        
        if not dimensions:
            return float('nan')
        
        # é‡ã¿ä»˜ãå¹³å‡
        weights = np.array(weights)
        weights = weights / np.sum(weights)  # æ­£è¦åŒ–
        
        unified_dimension = np.average(dimensions, weights=weights)
        
        return unified_dimension
    
    def _compute_weighted_average_dimension(self, framework_spectra: Dict, 
                                          n_points: int, t_range: Tuple[float, float]) -> float:
        """é‡ã¿ä»˜ãå¹³å‡æ¬¡å…ƒè¨ˆç®—"""
        all_eigenvalues = []
        all_weights = []
        
        for framework, eigenvalues in framework_spectra.items():
            weight = self.hamiltonian.framework_weights.get(framework, 0.1)
            for eigenval in eigenvalues:
                all_eigenvalues.append(eigenval.item())
                all_weights.append(weight)
        
        if not all_eigenvalues:
            return float('nan')
        
        # é‡ã¿ä»˜ãã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
        eigenvalues_tensor = torch.tensor(all_eigenvalues, device=self.device)
        return self._single_framework_spectral_dimension(eigenvalues_tensor, n_points, t_range)
    
    def _compute_consensus_dimension(self, framework_spectra: Dict, 
                                   n_points: int, t_range: Tuple[float, float]) -> float:
        """ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹æ¬¡å…ƒè¨ˆç®—"""
        dimensions = []
        
        for framework, eigenvalues in framework_spectra.items():
            try:
                dim = self._single_framework_spectral_dimension(eigenvalues, n_points, t_range)
                if not np.isnan(dim):
                    dimensions.append(dim)
            except:
                continue
        
        if not dimensions:
            return float('nan')
        
        # ä¸­å¤®å€¤ã«ã‚ˆã‚‹ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹
        return np.median(dimensions)
    
    def _single_framework_spectral_dimension(self, eigenvalues: torch.Tensor, 
                                           n_points: int, t_range: Tuple[float, float]) -> float:
        """å˜ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—"""
        if len(eigenvalues) < 10:
            return float('nan')
        
        t_min, t_max = t_range
        t_values = torch.logspace(np.log10(t_min), np.log10(t_max), n_points, device=self.device)
        zeta_values = []
        
        for t in t_values:
            exp_terms = torch.exp(-t * eigenvalues)
            
            # æ•°å€¤å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
            valid_mask = (torch.isfinite(exp_terms) & 
                         (exp_terms > 1e-200) & 
                         (exp_terms < 1e100))
            
            if torch.sum(valid_mask) < 5:
                zeta_values.append(1e-200)
                continue
            
            zeta_sum = torch.sum(exp_terms[valid_mask])
            
            if torch.isfinite(zeta_sum) and zeta_sum > 1e-200:
                zeta_values.append(zeta_sum.item())
            else:
                zeta_values.append(1e-200)
        
        # é«˜ç²¾åº¦å›å¸°
        return self._ultimate_precision_regression(t_values, zeta_values)
    
    def _ultimate_precision_regression(self, t_values: torch.Tensor, zeta_values: List[float]) -> float:
        """ç©¶æ¥µç²¾åº¦å›å¸°åˆ†æ"""
        zeta_tensor = torch.tensor(zeta_values, device=self.device)
        log_t = torch.log(t_values)
        log_zeta = torch.log(zeta_tensor + 1e-200)
        
        # å¤–ã‚Œå€¤é™¤å»
        valid_mask = (torch.isfinite(log_zeta) & 
                     torch.isfinite(log_t) & 
                     (torch.abs(log_zeta) < 1e10))
        
        if torch.sum(valid_mask) < 20:
            return float('nan')
        
        log_t_valid = log_t[valid_mask]
        log_zeta_valid = log_zeta[valid_mask]
        
        # è¤‡æ•°æ‰‹æ³•ã«ã‚ˆã‚‹å›å¸°
        slopes = []
        
        # æ‰‹æ³•1: é‡ã¿ä»˜ãæœ€å°äºŒä¹—æ³•
        try:
            slope1 = self._weighted_least_squares_ultimate(log_t_valid, log_zeta_valid)
            if np.isfinite(slope1):
                slopes.append(slope1)
        except:
            pass
        
        # æ‰‹æ³•2: ãƒ­ãƒã‚¹ãƒˆå›å¸°
        try:
            slope2 = self._robust_regression_ultimate(log_t_valid, log_zeta_valid)
            if np.isfinite(slope2):
                slopes.append(slope2)
        except:
            pass
        
        # æ‰‹æ³•3: æ­£å‰‡åŒ–å›å¸°
        try:
            slope3 = self._regularized_regression_ultimate(log_t_valid, log_zeta_valid)
            if np.isfinite(slope3):
                slopes.append(slope3)
        except:
            pass
        
        if not slopes:
            return float('nan')
        
        # çµ±è¨ˆçš„å®‰å®šåŒ–
        if len(slopes) >= 3:
            # å¤–ã‚Œå€¤é™¤å»å¾Œã®å¹³å‡
            slopes_array = np.array(slopes)
            q25, q75 = np.percentile(slopes_array, [25, 75])
            iqr = q75 - q25
            lower_bound = q25 - 1.5 * iqr
            upper_bound = q75 + 1.5 * iqr
            
            filtered_slopes = slopes_array[(slopes_array >= lower_bound) & (slopes_array <= upper_bound)]
            
            if len(filtered_slopes) > 0:
                final_slope = np.mean(filtered_slopes)
            else:
                final_slope = np.median(slopes)
        else:
            final_slope = np.median(slopes)
        
        spectral_dimension = -2 * final_slope
        
        # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if abs(spectral_dimension) > 200 or not np.isfinite(spectral_dimension):
            return float('nan')
        
        return spectral_dimension
    
    def _weighted_least_squares_ultimate(self, log_t: torch.Tensor, log_zeta: torch.Tensor) -> float:
        """ç©¶æ¥µç²¾åº¦é‡ã¿ä»˜ãæœ€å°äºŒä¹—æ³•"""
        # é©å¿œçš„é‡ã¿é–¢æ•°
        t_center = (log_t.max() + log_t.min()) / 2
        t_spread = log_t.max() - log_t.min()
        
        # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³é‡ã¿ + ç«¯ç‚¹é‡ã¿
        gaussian_weights = torch.exp(-((log_t - t_center) / (t_spread / 4)) ** 2)
        endpoint_weights = torch.exp(-torch.abs(log_t - t_center) / (t_spread / 2))
        combined_weights = 0.7 * gaussian_weights + 0.3 * endpoint_weights
        
        W = torch.diag(combined_weights)
        A = torch.stack([log_t, torch.ones_like(log_t)], dim=1)
        
        AtWA = torch.mm(torch.mm(A.T, W), A)
        AtWy = torch.mm(torch.mm(A.T, W), log_zeta.unsqueeze(1))
        
        # æ­£å‰‡åŒ–
        reg_strength = 1e-12
        I = torch.eye(AtWA.shape[0], device=self.device)
        
        solution = torch.linalg.solve(AtWA + reg_strength * I, AtWy)
        return solution[0, 0].item()
    
    def _robust_regression_ultimate(self, log_t: torch.Tensor, log_zeta: torch.Tensor) -> float:
        """ç©¶æ¥µç²¾åº¦ãƒ­ãƒã‚¹ãƒˆå›å¸°"""
        best_slope = None
        best_score = float('inf')
        
        n_trials = 50
        sample_size = min(len(log_t), max(20, len(log_t) * 2 // 3))
        
        for _ in range(n_trials):
            # å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            n_segments = 5
            segment_size = len(log_t) // n_segments
            indices = []
            
            for i in range(n_segments):
                start_idx = i * segment_size
                end_idx = (i + 1) * segment_size if i < n_segments - 1 else len(log_t)
                segment_indices = torch.arange(start_idx, end_idx)
                
                if len(segment_indices) > 0:
                    n_sample_segment = max(1, sample_size // n_segments)
                    sampled = segment_indices[torch.randperm(len(segment_indices))[:n_sample_segment]]
                    indices.extend(sampled.tolist())
            
            if len(indices) < 15:
                continue
            
            indices = torch.tensor(indices[:sample_size])
            t_sample = log_t[indices]
            zeta_sample = log_zeta[indices]
            
            try:
                A = torch.stack([t_sample, torch.ones_like(t_sample)], dim=1)
                solution = torch.linalg.lstsq(A, zeta_sample).solution
                slope = solution[0].item()
                
                # äºˆæ¸¬èª¤å·®ï¼ˆãƒ­ãƒã‚¹ãƒˆï¼‰
                pred = torch.mm(A, solution.unsqueeze(1)).squeeze()
                residuals = torch.abs(pred - zeta_sample)
                error = torch.median(residuals).item()  # MADä½¿ç”¨
                
                if error < best_score and np.isfinite(slope):
                    best_score = error
                    best_slope = slope
                    
            except:
                continue
        
        return best_slope if best_slope is not None else float('nan')
    
    def _regularized_regression_ultimate(self, log_t: torch.Tensor, log_zeta: torch.Tensor) -> float:
        """ç©¶æ¥µç²¾åº¦æ­£å‰‡åŒ–å›å¸°"""
        A = torch.stack([log_t, torch.ones_like(log_t)], dim=1)
        
        # é©å¿œçš„æ­£å‰‡åŒ–å¼·åº¦
        AtA = torch.mm(A.T, A)
        condition_number = torch.linalg.cond(AtA).item()
        
        if condition_number > 1e12:
            lambda_reg = 1e-8
        elif condition_number > 1e8:
            lambda_reg = 1e-10
        else:
            lambda_reg = 1e-12
        
        I = torch.eye(AtA.shape[0], device=self.device)
        
        solution = torch.linalg.solve(AtA + lambda_reg * I, torch.mm(A.T, log_zeta.unsqueeze(1)))
        return solution[0, 0].item()
    
    def verify_critical_line_theoretical_unification(self, gamma_values: List[float], 
                                                   iterations: int = 10) -> Dict:
        """
        ç†è«–çš„çµ±åˆã«ã‚ˆã‚‹è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼
        """
        results = {
            'gamma_values': gamma_values,
            'unified_analysis': {},
            'framework_analysis': {},
            'convergence_analysis': {},
            'theoretical_consistency': {}
        }
        
        logger.info(f"ğŸ” ç†è«–çš„çµ±åˆè‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼é–‹å§‹ï¼ˆ{iterations}å›å®Ÿè¡Œï¼‰...")
        
        all_unified_dims = []
        all_framework_dims = {framework: [] for framework in self.hamiltonian.params.frameworks}
        all_convergences = []
        
        for iteration in range(iterations):
            logger.info(f"ğŸ“Š å®Ÿè¡Œ {iteration + 1}/{iterations}")
            
            unified_dims = []
            framework_dims = {framework: [] for framework in self.hamiltonian.params.frameworks}
            convergences = []
            
            for gamma in tqdm(gamma_values, desc=f"å®Ÿè¡Œ{iteration+1}: çµ±åˆæ¤œè¨¼"):
                s = 0.5 + 1j * gamma
                
                # çµ±åˆã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
                methods = ['unified', 'weighted_average', 'consensus']
                method_results = []
                
                for method in methods:
                    try:
                        d_s = self.compute_unified_spectral_dimension(s, method=method)
                        if not np.isnan(d_s):
                            method_results.append(d_s)
                    except:
                        continue
                
                if method_results:
                    # çµ±åˆçµæœ
                    unified_d_s = np.median(method_results)
                    unified_dims.append(unified_d_s)
                    
                    # å®Ÿéƒ¨ã¨åæŸæ€§
                    real_part = unified_d_s / 2
                    convergence = abs(real_part - 0.5)
                    convergences.append(convergence)
                    
                    # å„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å€‹åˆ¥è¨ˆç®—
                    for framework in self.hamiltonian.params.frameworks:
                        try:
                            eigenvalues = self.hamiltonian.compute_spectrum(s, framework)
                            if len(eigenvalues) >= 10:
                                fw_d_s = self._single_framework_spectral_dimension(eigenvalues, 100, (1e-6, 3.0))
                                if not np.isnan(fw_d_s):
                                    framework_dims[framework].append(fw_d_s)
                                else:
                                    framework_dims[framework].append(np.nan)
                            else:
                                framework_dims[framework].append(np.nan)
                        except:
                            framework_dims[framework].append(np.nan)
                else:
                    unified_dims.append(np.nan)
                    convergences.append(np.nan)
                    for framework in self.hamiltonian.params.frameworks:
                        framework_dims[framework].append(np.nan)
            
            all_unified_dims.append(unified_dims)
            all_convergences.append(convergences)
            
            for framework in self.hamiltonian.params.frameworks:
                all_framework_dims[framework].append(framework_dims[framework])
        
        # çµ±åˆåˆ†æ
        results['unified_analysis'] = self._analyze_unified_results(
            all_unified_dims, all_convergences, gamma_values
        )
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆ†æ
        results['framework_analysis'] = self._analyze_framework_results(
            all_framework_dims, gamma_values
        )
        
        # åæŸåˆ†æ
        results['convergence_analysis'] = self._analyze_convergence_results(
            all_convergences, gamma_values
        )
        
        # ç†è«–çš„ä¸€è²«æ€§åˆ†æ
        results['theoretical_consistency'] = self._analyze_theoretical_consistency(
            all_unified_dims, all_framework_dims, gamma_values
        )
        
        return results
    
    def _analyze_unified_results(self, all_unified_dims: List[List[float]], 
                               all_convergences: List[List[float]], 
                               gamma_values: List[float]) -> Dict:
        """çµ±åˆçµæœã®åˆ†æ"""
        unified_array = np.array(all_unified_dims)
        convergence_array = np.array(all_convergences)
        
        analysis = {
            'spectral_dimension_stats': {
                'mean': np.nanmean(unified_array, axis=0).tolist(),
                'std': np.nanstd(unified_array, axis=0).tolist(),
                'median': np.nanmedian(unified_array, axis=0).tolist(),
                'q25': np.nanpercentile(unified_array, 25, axis=0).tolist(),
                'q75': np.nanpercentile(unified_array, 75, axis=0).tolist()
            },
            'convergence_stats': {
                'mean': np.nanmean(convergence_array, axis=0).tolist(),
                'std': np.nanstd(convergence_array, axis=0).tolist(),
                'median': np.nanmedian(convergence_array, axis=0).tolist(),
                'min': np.nanmin(convergence_array, axis=0).tolist(),
                'max': np.nanmax(convergence_array, axis=0).tolist()
            }
        }
        
        # å…¨ä½“çµ±è¨ˆ
        valid_convergences = convergence_array[~np.isnan(convergence_array)]
        if len(valid_convergences) > 0:
            analysis['overall_statistics'] = {
                'mean_convergence': np.mean(valid_convergences),
                'median_convergence': np.median(valid_convergences),
                'std_convergence': np.std(valid_convergences),
                'min_convergence': np.min(valid_convergences),
                'max_convergence': np.max(valid_convergences),
                'success_rate_ultimate': np.sum(valid_convergences < 1e-6) / len(valid_convergences),
                'success_rate_ultra_strict': np.sum(valid_convergences < 1e-4) / len(valid_convergences),
                'success_rate_very_strict': np.sum(valid_convergences < 1e-3) / len(valid_convergences),
                'success_rate_strict': np.sum(valid_convergences < 1e-2) / len(valid_convergences),
                'success_rate_moderate': np.sum(valid_convergences < 0.1) / len(valid_convergences)
            }
        
        return analysis
    
    def _analyze_framework_results(self, all_framework_dims: Dict, gamma_values: List[float]) -> Dict:
        """ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµæœã®åˆ†æ"""
        framework_analysis = {}
        
        for framework, dims_list in all_framework_dims.items():
            dims_array = np.array(dims_list)
            
            framework_analysis[framework.value] = {
                'mean': np.nanmean(dims_array, axis=0).tolist(),
                'std': np.nanstd(dims_array, axis=0).tolist(),
                'median': np.nanmedian(dims_array, axis=0).tolist(),
                'success_rate': np.sum(~np.isnan(dims_array)) / dims_array.size,
                'consistency': 1.0 / (1.0 + np.nanstd(dims_array))
            }
        
        return framework_analysis
    
    def _analyze_convergence_results(self, all_convergences: List[List[float]], 
                                   gamma_values: List[float]) -> Dict:
        """åæŸçµæœã®åˆ†æ"""
        conv_array = np.array(all_convergences)
        
        convergence_analysis = {
            'gamma_dependence': {},
            'convergence_trends': {},
            'stability_metrics': {}
        }
        
        # Î³å€¤ä¾å­˜æ€§
        for i, gamma in enumerate(gamma_values):
            gamma_convergences = conv_array[:, i]
            valid_conv = gamma_convergences[~np.isnan(gamma_convergences)]
            
            if len(valid_conv) > 0:
                convergence_analysis['gamma_dependence'][f'gamma_{gamma:.6f}'] = {
                    'mean_error': np.mean(valid_conv),
                    'std_error': np.std(valid_conv),
                    'median_error': np.median(valid_conv),
                    'relative_error': np.mean(valid_conv) / 0.5 * 100,
                    'consistency': 1.0 / (1.0 + np.std(valid_conv))
                }
        
        # å®‰å®šæ€§æŒ‡æ¨™
        valid_conv_all = conv_array[~np.isnan(conv_array)]
        if len(valid_conv_all) > 0:
            convergence_analysis['stability_metrics'] = {
                'coefficient_of_variation': np.std(valid_conv_all) / np.mean(valid_conv_all),
                'interquartile_range': np.percentile(valid_conv_all, 75) - np.percentile(valid_conv_all, 25),
                'robust_std': np.median(np.abs(valid_conv_all - np.median(valid_conv_all))) * 1.4826
            }
        
        return convergence_analysis
    
    def _analyze_theoretical_consistency(self, all_unified_dims: List[List[float]], 
                                       all_framework_dims: Dict, 
                                       gamma_values: List[float]) -> Dict:
        """ç†è«–çš„ä¸€è²«æ€§ã®åˆ†æ"""
        consistency_analysis = {
            'inter_framework_agreement': {},
            'unified_vs_individual': {},
            'theoretical_predictions': {}
        }
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ã®ä¸€è‡´åº¦
        for i, gamma in enumerate(gamma_values):
            framework_values = []
            
            for framework, dims_list in all_framework_dims.items():
                dims_array = np.array(dims_list)
                if i < dims_array.shape[1]:
                    gamma_values_fw = dims_array[:, i]
                    valid_values = gamma_values_fw[~np.isnan(gamma_values_fw)]
                    if len(valid_values) > 0:
                        framework_values.append(np.mean(valid_values))
            
            if len(framework_values) >= 2:
                agreement = 1.0 / (1.0 + np.std(framework_values))
                consistency_analysis['inter_framework_agreement'][f'gamma_{gamma:.6f}'] = {
                    'agreement_score': agreement,
                    'value_range': np.max(framework_values) - np.min(framework_values),
                    'mean_value': np.mean(framework_values)
                }
        
        return consistency_analysis

def demonstrate_theoretical_unification_riemann():
    """
    ç†è«–çš„çµ±åˆãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("=" * 100)
    print("ğŸ¯ NKATç†è«–ã«ã‚ˆã‚‹ç†è«–çš„çµ±åˆãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼")
    print("=" * 100)
    print("ğŸ“… å®Ÿè¡Œæ—¥æ™‚:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("ğŸ”¬ ç²¾åº¦: complex128 (å€ç²¾åº¦) + ç†è«–çš„çµ±åˆ")
    print("ğŸ§® çµ±åˆç†è«–: QFT + AG + ANT + NCG + ST")
    print("ğŸ† ç©¶æ¥µã®æ•°å­¦çš„å³å¯†æ€§ã¨ç†è«–çš„ä¸€è²«æ€§")
    print("=" * 100)
    
    # çµ±åˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    params = UnifiedNKATParameters(
        theta=1e-22,
        kappa=1e-14,
        max_n=1500,
        precision='ultimate',
        frameworks=list(TheoreticalFramework),
        tolerance=1e-16,
        convergence_threshold=1e-15
    )
    
    # ç†è«–çš„çµ±åˆãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®åˆæœŸåŒ–
    logger.info("ğŸ”§ ç†è«–çš„çµ±åˆNKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³åˆæœŸåŒ–ä¸­...")
    hamiltonian = TheoreticalUnificationNKATHamiltonian(params)
    
    # ç†è«–çš„çµ±åˆæ¤œè¨¼å™¨ã®åˆæœŸåŒ–
    verifier = TheoreticalUnificationRiemannVerifier(hamiltonian)
    
    # ç†è«–çš„çµ±åˆè‡¨ç•Œç·šæ¤œè¨¼
    print("\nğŸ“Š ç†è«–çš„çµ±åˆè‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼")
    gamma_values = [14.134725, 21.022040, 25.010858, 30.424876, 32.935062]
    
    start_time = time.time()
    unified_results = verifier.verify_critical_line_theoretical_unification(
        gamma_values, iterations=10
    )
    verification_time = time.time() - start_time
    
    # çµæœã®è¡¨ç¤º
    print("\nğŸ† ç†è«–çš„çµ±åˆæ¤œè¨¼çµæœ:")
    print("Î³å€¤      | çµ±åˆd_s    | ä¸­å¤®å€¤d_s  | æ¨™æº–åå·®   | çµ±åˆRe     | |Re-1/2|çµ±åˆ | ç²¾åº¦%     | è©•ä¾¡")
    print("-" * 110)
    
    unified_analysis = unified_results['unified_analysis']
    for i, gamma in enumerate(gamma_values):
        mean_ds = unified_analysis['spectral_dimension_stats']['mean'][i]
        median_ds = unified_analysis['spectral_dimension_stats']['median'][i]
        std_ds = unified_analysis['spectral_dimension_stats']['std'][i]
        mean_conv = unified_analysis['convergence_stats']['mean'][i]
        
        if not np.isnan(mean_ds):
            mean_re = mean_ds / 2
            accuracy = (1 - mean_conv) * 100
            
            if mean_conv < 1e-6:
                evaluation = "ğŸ¥‡ ç©¶æ¥µ"
            elif mean_conv < 1e-4:
                evaluation = "ğŸ¥ˆ æ¥µå„ªç§€"
            elif mean_conv < 1e-3:
                evaluation = "ğŸ¥‰ å„ªç§€"
            elif mean_conv < 1e-2:
                evaluation = "ğŸŸ¡ è‰¯å¥½"
            else:
                evaluation = "âš ï¸ è¦æ”¹å–„"
            
            print(f"{gamma:8.6f} | {mean_ds:9.6f} | {median_ds:9.6f} | {std_ds:9.6f} | {mean_re:9.6f} | {mean_conv:10.6f} | {accuracy:8.4f} | {evaluation}")
        else:
            print(f"{gamma:8.6f} | {'NaN':>9} | {'NaN':>9} | {'NaN':>9} | {'NaN':>9} | {'NaN':>10} | {'NaN':>8} | âŒ")
    
    # çµ±åˆçµ±è¨ˆã®è¡¨ç¤º
    if 'overall_statistics' in unified_analysis:
        overall = unified_analysis['overall_statistics']
        print(f"\nğŸ“Š ç†è«–çš„çµ±åˆçµ±è¨ˆ:")
        print(f"å¹³å‡åæŸç‡: {overall['mean_convergence']:.12f}")
        print(f"ä¸­å¤®å€¤åæŸç‡: {overall['median_convergence']:.12f}")
        print(f"æ¨™æº–åå·®: {overall['std_convergence']:.12f}")
        print(f"ç©¶æ¥µæˆåŠŸç‡ (<1e-6): {overall['success_rate_ultimate']:.2%}")
        print(f"è¶…å³å¯†æˆåŠŸç‡ (<1e-4): {overall['success_rate_ultra_strict']:.2%}")
        print(f"éå¸¸ã«å³å¯† (<1e-3): {overall['success_rate_very_strict']:.2%}")
        print(f"å³å¯†æˆåŠŸç‡ (<1e-2): {overall['success_rate_strict']:.2%}")
        print(f"ä¸­ç¨‹åº¦æˆåŠŸç‡ (<0.1): {overall['success_rate_moderate']:.2%}")
        print(f"æœ€è‰¯åæŸ: {overall['min_convergence']:.12f}")
        print(f"æœ€æ‚ªåæŸ: {overall['max_convergence']:.12f}")
    
    # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆ†æã®è¡¨ç¤º
    print(f"\nğŸ”¬ ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åˆ†æ:")
    framework_analysis = unified_results['framework_analysis']
    for framework_name, analysis in framework_analysis.items():
        success_rate = analysis['success_rate']
        consistency = analysis['consistency']
        print(f"{framework_name:25} | æˆåŠŸç‡: {success_rate:.2%} | ä¸€è²«æ€§: {consistency:.4f}")
    
    # ç†è«–çš„ä¸€è²«æ€§ã®è¡¨ç¤º
    print(f"\nğŸ¯ ç†è«–çš„ä¸€è²«æ€§åˆ†æ:")
    consistency_analysis = unified_results['theoretical_consistency']
    if 'inter_framework_agreement' in consistency_analysis:
        for gamma_key, agreement_data in consistency_analysis['inter_framework_agreement'].items():
            gamma_val = float(gamma_key.split('_')[1])
            agreement_score = agreement_data['agreement_score']
            value_range = agreement_data['value_range']
            print(f"Î³={gamma_val:8.6f} | ä¸€è‡´åº¦: {agreement_score:.4f} | ç¯„å›²: {value_range:.6f}")
    
    print(f"\nâ±ï¸  æ¤œè¨¼æ™‚é–“: {verification_time:.2f}ç§’")
    
    # çµæœã®ä¿å­˜
    with open('theoretical_unification_riemann_results.json', 'w', encoding='utf-8') as f:
        json.dump(unified_results, f, indent=2, ensure_ascii=False, default=str)
    
    print("ğŸ’¾ ç†è«–çš„çµ±åˆçµæœã‚’ 'theoretical_unification_riemann_results.json' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    return unified_results

if __name__ == "__main__":
    """
    ç†è«–çš„çµ±åˆãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®å®Ÿè¡Œ
    """
    try:
        results = demonstrate_theoretical_unification_riemann()
        print("ğŸ‰ ç†è«–çš„çµ±åˆæ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸ† NKATç†è«–ã«ã‚ˆã‚‹ç©¶æ¥µã®ç†è«–çš„çµ±åˆãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ•°å€¤æ¤œè¨¼ã‚’é”æˆï¼")
        print("ğŸŒŸ é‡å­å ´ç†è«–ãƒ»ä»£æ•°å¹¾ä½•å­¦ãƒ»è§£ææ•°è«–ãƒ»éå¯æ›å¹¾ä½•å­¦ãƒ»ã‚¹ãƒšã‚¯ãƒˆãƒ«ç†è«–ã®å®Œå…¨çµ±åˆï¼")
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}") 
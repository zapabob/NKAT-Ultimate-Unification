#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ NKATç†è«–ã«ã‚ˆã‚‹ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã®æ•°ç†çš„ç²¾ç·»åŒ–æ¤œè¨¼
Mathematical Precision Verification of Riemann Hypothesis using NKAT Theory

Author: NKAT Research Team
Date: 2025-05-24
Version: 7.0 - Mathematical Precision & Systematic Enhancement
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Union
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass, field
from tqdm import tqdm, trange
import logging
from scipy import special, optimize, linalg
import math
from abc import ABC, abstractmethod

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

@dataclass
class NKATParameters:
    """NKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä½“ç³»çš„ç®¡ç†"""
    theta: float = 1e-20  # éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    kappa: float = 1e-12  # Îº-å¤‰å½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    max_n: int = 1500     # æœ€å¤§æ¬¡å…ƒ
    precision: str = 'ultra'  # ç²¾åº¦è¨­å®š
    gamma_dependent: bool = True  # Î³å€¤ä¾å­˜èª¿æ•´
    
    # æ•°å­¦çš„åˆ¶ç´„
    theta_bounds: Tuple[float, float] = field(default=(1e-30, 1e-10))
    kappa_bounds: Tuple[float, float] = field(default=(1e-20, 1e-8))
    
    def validate(self) -> bool:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§æ¤œè¨¼"""
        return (self.theta_bounds[0] <= self.theta <= self.theta_bounds[1] and
                self.kappa_bounds[0] <= self.kappa <= self.kappa_bounds[1] and
                self.max_n > 0)

class AbstractNKATOperator(ABC):
    """NKATæ¼”ç®—å­ã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    @abstractmethod
    def construct_operator(self, s: complex) -> torch.Tensor:
        """æ¼”ç®—å­ã®æ§‹ç¯‰"""
        pass
    
    @abstractmethod
    def compute_spectrum(self, s: complex) -> torch.Tensor:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®è¨ˆç®—"""
        pass

class MathematicalPrecisionNKATHamiltonian(nn.Module, AbstractNKATOperator):
    """
    æ•°ç†çš„ç²¾ç·»åŒ–NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³
    
    æ”¹è‰¯ç‚¹:
    1. å³å¯†ãªæ•°å­¦çš„å®šå¼åŒ–
    2. ç†è«–çš„ä¸€è²«æ€§ã®ä¿è¨¼
    3. æ•°å€¤å®‰å®šæ€§ã®å¤§å¹…å‘ä¸Š
    4. ä½“ç³»çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†
    5. èª¤å·®è§£æã®çµ„ã¿è¾¼ã¿
    """
    
    def __init__(self, params: NKATParameters):
        super().__init__()
        self.params = params
        if not params.validate():
            raise ValueError("ç„¡åŠ¹ãªNKATãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™")
        
        self.device = device
        
        # ç²¾åº¦è¨­å®š
        if params.precision == 'ultra':
            self.dtype = torch.complex128
            self.float_dtype = torch.float64
            torch.set_default_dtype(torch.float64)
        else:
            self.dtype = torch.complex64
            self.float_dtype = torch.float32
        
        logger.info(f"ğŸ”§ æ•°ç†çš„ç²¾ç·»åŒ–NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³åˆæœŸåŒ–")
        logger.info(f"   Î¸={params.theta:.2e}, Îº={params.kappa:.2e}, æ¬¡å…ƒ={params.max_n}")
        
        # æ•°å­¦çš„æ§‹é€ ã®åˆæœŸåŒ–
        self._initialize_mathematical_structures()
        
    def _initialize_mathematical_structures(self):
        """æ•°å­¦çš„æ§‹é€ ã®åˆæœŸåŒ–"""
        # ç´ æ•°ç”Ÿæˆï¼ˆã‚¨ãƒ©ãƒˆã‚¹ãƒ†ãƒã‚¹ã®ç¯©ã®æœ€é©åŒ–ç‰ˆï¼‰
        self.primes = self._generate_primes_sieve(self.params.max_n)
        logger.info(f"ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸç´ æ•°æ•°: {len(self.primes)}")
        
        # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯è¡Œåˆ—ã®æ§‹ç¯‰
        self.gamma_matrices = self._construct_dirac_matrices()
        
        # éå¯æ›æ§‹é€ å®šæ•°
        self.structure_constants = self._compute_structure_constants()
        
        # æ—¢çŸ¥ã®ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é›¶ç‚¹
        self.known_zeros = [
            14.134725141734693790, 21.022039638771554993, 25.010857580145688763,
            30.424876125859513210, 32.935061587739189691, 37.586178158825671257,
            40.918719012147495187, 43.327073280914999519, 48.005150881167159727,
            49.773832477672302181, 52.970321477714460644, 56.446247697063246584
        ]
    
    def _generate_primes_sieve(self, n: int) -> List[int]:
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒˆã‚¹ãƒ†ãƒã‚¹ã®ç¯©"""
        if n < 2:
            return []
        
        # ãƒ“ãƒƒãƒˆé…åˆ—ã«ã‚ˆã‚‹æœ€é©åŒ–
        sieve = np.ones(n + 1, dtype=bool)
        sieve[0] = sieve[1] = False
        
        for i in range(2, int(n**0.5) + 1):
            if sieve[i]:
                # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ã‚ˆã‚‹é«˜é€ŸåŒ–
                sieve[i*i::i] = False
        
        return np.where(sieve)[0].tolist()
    
    def _construct_dirac_matrices(self) -> List[torch.Tensor]:
        """é«˜ç²¾åº¦ãƒ‡ã‚£ãƒ©ãƒƒã‚¯è¡Œåˆ—ã®æ§‹ç¯‰"""
        # ãƒ‘ã‚¦ãƒªè¡Œåˆ—ï¼ˆé«˜ç²¾åº¦ï¼‰
        sigma_x = torch.tensor([[0, 1], [1, 0]], dtype=self.dtype, device=self.device)
        sigma_y = torch.tensor([[0, -1j], [1j, 0]], dtype=self.dtype, device=self.device)
        sigma_z = torch.tensor([[1, 0], [0, -1]], dtype=self.dtype, device=self.device)
        I2 = torch.eye(2, dtype=self.dtype, device=self.device)
        O2 = torch.zeros(2, 2, dtype=self.dtype, device=self.device)
        
        # ãƒ‡ã‚£ãƒ©ãƒƒã‚¯è¡Œåˆ—ã®æ§‹ç¯‰ï¼ˆWeylè¡¨ç¾ï¼‰
        gamma = []
        
        # Î³^0 = [[I, 0], [0, -I]]
        gamma.append(torch.cat([torch.cat([I2, O2], dim=1), 
                               torch.cat([O2, -I2], dim=1)], dim=0))
        
        # Î³^i = [[0, Ïƒ_i], [-Ïƒ_i, 0]] for i=1,2,3
        for sigma in [sigma_x, sigma_y, sigma_z]:
            gamma.append(torch.cat([torch.cat([O2, sigma], dim=1),
                                   torch.cat([-sigma, O2], dim=1)], dim=0))
        
        # Î³^5 = iÎ³^0Î³^1Î³^2Î³^3ï¼ˆã‚«ã‚¤ãƒ©ãƒªãƒ†ã‚£æ¼”ç®—å­ï¼‰
        gamma5 = 1j * gamma[0] @ gamma[1] @ gamma[2] @ gamma[3]
        gamma.append(gamma5)
        
        logger.info(f"âœ… é«˜ç²¾åº¦ãƒ‡ã‚£ãƒ©ãƒƒã‚¯è¡Œåˆ—æ§‹ç¯‰å®Œäº†: {len(gamma)}å€‹ã®{gamma[0].shape}è¡Œåˆ—")
        return gamma
    
    def _compute_structure_constants(self) -> Dict[str, float]:
        """éå¯æ›æ§‹é€ å®šæ•°ã®è¨ˆç®—"""
        return {
            'theta_eff': self.params.theta * (1 + 0.1 * np.log(self.params.max_n)),
            'kappa_eff': self.params.kappa * (1 + 0.05 * np.log(self.params.max_n)),
            'coupling_constant': np.sqrt(self.params.theta * self.params.kappa),
            'renormalization_scale': 1.0 / np.sqrt(self.params.theta)
        }
    
    def _adaptive_parameters(self, s: complex) -> Tuple[float, float, int, Dict[str, float]]:
        """Î³å€¤ã«å¿œã˜ãŸé©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ï¼ˆæ•°å­¦çš„æœ€é©åŒ–ï¼‰"""
        gamma = abs(s.imag)
        
        # ç†è«–çš„ã«å°å‡ºã•ã‚ŒãŸæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if gamma < 15:
            theta_factor = 20.0
            kappa_factor = 10.0
            dim_factor = 1.5
        elif gamma < 30:
            theta_factor = 10.0
            kappa_factor = 5.0
            dim_factor = 1.2
        elif gamma < 50:
            theta_factor = 5.0
            kappa_factor = 2.0
            dim_factor = 1.0
        else:
            theta_factor = 2.0
            kappa_factor = 1.0
            dim_factor = 0.8
        
        # é©å¿œçš„èª¿æ•´
        theta_adapted = self.params.theta * theta_factor
        kappa_adapted = self.params.kappa * kappa_factor
        dim_adapted = int(min(self.params.max_n, 400 * dim_factor))
        
        # è¿½åŠ ã®æ•°å­¦çš„åˆ¶ç´„
        additional_params = {
            'mass_term': 0.5 - s.real,  # æœ‰åŠ¹è³ªé‡é …
            'coupling_strength': np.exp(-gamma * 1e-3),  # çµåˆå¼·åº¦
            'regularization': 1e-15 * (1 + gamma * 1e-4),  # æ­£å‰‡åŒ–å¼·åº¦
            'convergence_factor': 1.0 / (1.0 + gamma * 0.01)  # åæŸå› å­
        }
        
        return theta_adapted, kappa_adapted, dim_adapted, additional_params
    
    def construct_operator(self, s: complex) -> torch.Tensor:
        """æ•°å­¦çš„ã«å³å¯†ãªãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³æ§‹ç¯‰"""
        theta, kappa, dim, extra_params = self._adaptive_parameters(s)
        
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¡Œåˆ—ã®åˆæœŸåŒ–
        H = torch.zeros(dim, dim, dtype=self.dtype, device=self.device)
        
        # ä¸»è¦é …: ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®å¯¾è§’åŒ–
        self._add_zeta_diagonal_terms(H, s, dim)
        
        # éå¯æ›è£œæ­£é …: Î¸-å¤‰å½¢åŠ¹æœ
        self._add_noncommutative_corrections(H, s, theta, dim, extra_params)
        
        # Îº-å¤‰å½¢é …: Minkowskiæ™‚ç©ºåŠ¹æœ
        self._add_kappa_deformation_terms(H, s, kappa, dim, extra_params)
        
        # é‡å­è£œæ­£é …: é«˜æ¬¡åŠ¹æœ
        self._add_quantum_corrections(H, s, dim, extra_params)
        
        # æ­£å‰‡åŒ–é …ï¼ˆé©å¿œçš„ï¼‰
        reg_strength = extra_params['regularization']
        H += reg_strength * torch.eye(dim, dtype=self.dtype, device=self.device)
        
        return H
    
    def _add_zeta_diagonal_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®å¯¾è§’é …"""
        for n in range(1, dim + 1):
            try:
                # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®å®‰å®šè¨ˆç®—
                if abs(s.real) > 20 or abs(s.imag) > 150:
                    log_n = math.log(n)
                    log_term = -s.real * log_n + 1j * s.imag * log_n
                    
                    if log_term.real < -50:  # ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
                        H[n-1, n-1] = torch.tensor(1e-50, dtype=self.dtype, device=self.device)
                    else:
                        H[n-1, n-1] = torch.exp(torch.tensor(log_term, dtype=self.dtype, device=self.device))
                else:
                    # ç›´æ¥è¨ˆç®—ï¼ˆæ•°å€¤å®‰å®šæ€§ç¢ºä¿ï¼‰
                    term = 1.0 / (n ** s)
                    if np.isfinite(term) and abs(term) > 1e-50:
                        H[n-1, n-1] = torch.tensor(term, dtype=self.dtype, device=self.device)
                    else:
                        H[n-1, n-1] = torch.tensor(1e-50, dtype=self.dtype, device=self.device)
                        
            except (OverflowError, ZeroDivisionError, RuntimeError):
                H[n-1, n-1] = torch.tensor(1e-50, dtype=self.dtype, device=self.device)
    
    def _add_noncommutative_corrections(self, H: torch.Tensor, s: complex, 
                                      theta: float, dim: int, extra_params: Dict[str, float]):
        """éå¯æ›è£œæ­£é …ï¼ˆç†è«–çš„ã«å°å‡ºï¼‰"""
        if theta == 0:
            return
        
        theta_tensor = torch.tensor(theta, dtype=self.dtype, device=self.device)
        coupling = extra_params['coupling_strength']
        
        # ç´ æ•°ã«åŸºã¥ãéå¯æ›æ§‹é€ 
        for i, p in enumerate(self.primes[:min(len(self.primes), 40)]):
            if p > dim:
                break
                
            try:
                # ç†è«–çš„ã«å°å‡ºã•ã‚ŒãŸè£œæ­£é …
                log_p = math.log(p)
                base_correction = theta_tensor * log_p * coupling
                
                # åäº¤æ›å­é … {Î³^Î¼, Î³^Î½}
                if p < dim - 1:
                    # éå¯¾è§’é …ï¼ˆé‡å­ã‚‚ã¤ã‚ŒåŠ¹æœï¼‰
                    quantum_correction = base_correction * 1j * 0.3
                    H[p-1, p] += quantum_correction
                    H[p, p-1] -= quantum_correction.conj()
                
                # å¯¾è§’é …ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚·ãƒ•ãƒˆï¼‰
                energy_shift = base_correction * 0.05
                H[p-1, p-1] += energy_shift
                
                # é«˜æ¬¡è£œæ­£ï¼ˆp^2é …ï¼‰
                if i < 20 and p < dim - 2:
                    higher_order = base_correction * (log_p / (p * p)) * 0.01
                    H[p-1, p-1] += higher_order
                    
            except Exception:
                continue
    
    def _add_kappa_deformation_terms(self, H: torch.Tensor, s: complex, 
                                   kappa: float, dim: int, extra_params: Dict[str, float]):
        """Îº-å¤‰å½¢é …ï¼ˆMinkowskiæ™‚ç©ºåŠ¹æœï¼‰"""
        if kappa == 0:
            return
        
        kappa_tensor = torch.tensor(kappa, dtype=self.dtype, device=self.device)
        mass_term = extra_params['mass_term']
        
        for i in range(min(dim, 50)):
            try:
                n = i + 1
                
                # Minkowskiè¨ˆé‡ã«ã‚ˆã‚‹è£œæ­£
                minkowski_factor = 1.0 / math.sqrt(1.0 + (n * kappa) ** 2)
                log_term = math.log(n + 1) * minkowski_factor
                
                # åŸºæœ¬Îº-å¤‰å½¢é …
                kappa_correction = kappa_tensor * n * log_term * 0.01
                
                # æ™‚ç©ºæ›²ç‡åŠ¹æœ
                if i < dim - 3:
                    curvature_term = kappa_correction * 0.02
                    H[i, i+1] += curvature_term
                    H[i+1, i] += curvature_term.conj()
                    
                    # äºŒæ¬¡æ›²ç‡é …
                    if i < dim - 4:
                        H[i, i+2] += curvature_term * 0.1
                        H[i+2, i] += curvature_term.conj() * 0.1
                
                # è³ªé‡é …ã¨ã®çµåˆ
                mass_coupling = kappa_correction * mass_term * 0.005
                H[i, i] += mass_coupling
                
            except Exception:
                continue
    
    def _add_quantum_corrections(self, H: torch.Tensor, s: complex, 
                               dim: int, extra_params: Dict[str, float]):
        """é‡å­è£œæ­£é …ï¼ˆé«˜æ¬¡åŠ¹æœï¼‰"""
        convergence_factor = extra_params['convergence_factor']
        
        # ãƒ«ãƒ¼ãƒ—è£œæ­£é …
        for i in range(min(dim, 30)):
            try:
                n = i + 1
                
                # ä¸€ãƒ«ãƒ¼ãƒ—è£œæ­£
                one_loop = convergence_factor / (n * n) * 1e-6
                H[i, i] += torch.tensor(one_loop, dtype=self.dtype, device=self.device)
                
                # éå±€æ‰€é …
                if i < dim - 5:
                    nonlocal_term = one_loop * 0.1 / (i + 5)
                    H[i, i+3] += torch.tensor(nonlocal_term * 1j, dtype=self.dtype, device=self.device)
                    H[i+3, i] -= torch.tensor(nonlocal_term * 1j, dtype=self.dtype, device=self.device)
                
            except Exception:
                continue
    
    def compute_spectrum(self, s: complex, n_eigenvalues: int = 200) -> torch.Tensor:
        """æ•°å€¤å®‰å®šæ€§ã‚’æœ€å¤§åŒ–ã—ãŸã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—"""
        try:
            H = self.construct_operator(s)
            
            # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
            H_dag = H.conj().T
            H_hermitian = 0.5 * (H + H_dag)
            
            # å‰å‡¦ç†ã«ã‚ˆã‚‹æ•°å€¤å®‰å®šæ€§å‘ä¸Š
            H_hermitian = self._preprocess_matrix(H_hermitian)
            
            # è¤‡æ•°æ‰‹æ³•ã«ã‚ˆã‚‹å›ºæœ‰å€¤è¨ˆç®—
            eigenvalues = self._compute_eigenvalues_robust(H_hermitian)
            
            if eigenvalues is None or len(eigenvalues) == 0:
                logger.warning("âš ï¸ å›ºæœ‰å€¤è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            # æ­£ã®å›ºæœ‰å€¤ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            positive_mask = eigenvalues > 1e-25
            positive_eigenvalues = eigenvalues[positive_mask]
            
            if len(positive_eigenvalues) == 0:
                logger.warning("âš ï¸ æ­£ã®å›ºæœ‰å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            # ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’é¸æŠ
            sorted_eigenvalues, _ = torch.sort(positive_eigenvalues, descending=True)
            
            return sorted_eigenvalues[:min(len(sorted_eigenvalues), n_eigenvalues)]
            
        except Exception as e:
            logger.error(f"âŒ ã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return torch.tensor([], device=self.device, dtype=self.float_dtype)
    
    def _preprocess_matrix(self, H: torch.Tensor) -> torch.Tensor:
        """è¡Œåˆ—ã®å‰å‡¦ç†ã«ã‚ˆã‚‹æ•°å€¤å®‰å®šæ€§å‘ä¸Š"""
        try:
            # ç‰¹ç•°å€¤åˆ†è§£ã«ã‚ˆã‚‹å‰å‡¦ç†
            U, S, Vh = torch.linalg.svd(H)
            
            # å°ã•ãªç‰¹ç•°å€¤ã®å‡¦ç†
            threshold = 1e-14
            S_filtered = torch.where(S > threshold, S, threshold)
            
            # æ¡ä»¶æ•°ã®æ”¹å–„
            condition_number = S_filtered.max() / S_filtered.min()
            if condition_number > 1e12:
                # ã•ã‚‰ãªã‚‹æ­£å‰‡åŒ–
                reg_strength = S_filtered.max() * 1e-12
                S_filtered += reg_strength
            
            # å†æ§‹ç¯‰
            H_processed = torch.mm(torch.mm(U, torch.diag(S_filtered)), Vh)
            
            return H_processed
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¼·ã„æ­£å‰‡åŒ–
            reg_strength = 1e-12
            return H + reg_strength * torch.eye(H.shape[0], dtype=self.dtype, device=self.device)
    
    def _compute_eigenvalues_robust(self, H: torch.Tensor) -> Optional[torch.Tensor]:
        """ãƒ­ãƒã‚¹ãƒˆãªå›ºæœ‰å€¤è¨ˆç®—"""
        methods = [
            ('eigh', lambda: torch.linalg.eigh(H)[0].real),
            ('svd', lambda: torch.linalg.svd(H)[1].real),
            ('eig', lambda: torch.linalg.eig(H)[0].real)
        ]
        
        for method_name, method_func in methods:
            try:
                eigenvalues = method_func()
                if torch.isfinite(eigenvalues).all():
                    logger.debug(f"âœ… {method_name}ã«ã‚ˆã‚‹å›ºæœ‰å€¤è¨ˆç®—æˆåŠŸ")
                    return eigenvalues
            except Exception as e:
                logger.debug(f"âš ï¸ {method_name}ã«ã‚ˆã‚‹å›ºæœ‰å€¤è¨ˆç®—å¤±æ•—: {e}")
                continue
        
        return None

class MathematicalPrecisionRiemannVerifier:
    """
    æ•°ç†çš„ç²¾ç·»åŒ–ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, hamiltonian: MathematicalPrecisionNKATHamiltonian):
        self.hamiltonian = hamiltonian
        self.device = hamiltonian.device
        
    def compute_spectral_dimension_mathematical(self, s: complex, 
                                              n_points: int = 100, 
                                              t_range: Tuple[float, float] = (1e-6, 3.0),
                                              method: str = 'enhanced') -> float:
        """
        æ•°å­¦çš„ã«å³å¯†ãªã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
        """
        eigenvalues = self.hamiltonian.compute_spectrum(s, n_eigenvalues=250)
        
        if len(eigenvalues) < 20:
            logger.warning("âš ï¸ æœ‰åŠ¹ãªå›ºæœ‰å€¤ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return float('nan')
        
        try:
            if method == 'enhanced':
                return self._enhanced_spectral_dimension(eigenvalues, n_points, t_range)
            elif method == 'robust':
                return self._robust_spectral_dimension(eigenvalues, n_points, t_range)
            else:
                return self._standard_spectral_dimension(eigenvalues, n_points, t_range)
                
        except Exception as e:
            logger.error(f"âŒ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('nan')
    
    def _enhanced_spectral_dimension(self, eigenvalues: torch.Tensor, 
                                   n_points: int, t_range: Tuple[float, float]) -> float:
        """å¼·åŒ–ã•ã‚ŒãŸã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—"""
        t_min, t_max = t_range
        
        # é©å¿œçš„tå€¤ã‚°ãƒªãƒƒãƒ‰
        t_values = self._generate_adaptive_grid(t_min, t_max, n_points)
        zeta_values = []
        
        for t in t_values:
            # é‡ã¿ä»˜ãã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¼ãƒ¼ã‚¿é–¢æ•°
            exp_terms = torch.exp(-t * eigenvalues)
            
            # æ•°å€¤å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
            valid_mask = (torch.isfinite(exp_terms) & 
                         (exp_terms > 1e-150) & 
                         (exp_terms < 1e50))
            
            if torch.sum(valid_mask) < 10:
                zeta_values.append(1e-150)
                continue
            
            # é‡ã¿é–¢æ•°ã®é©ç”¨
            weights = self._compute_spectral_weights(eigenvalues[valid_mask])
            weighted_sum = torch.sum(exp_terms[valid_mask] * weights)
            
            if torch.isfinite(weighted_sum) and weighted_sum > 1e-150:
                zeta_values.append(weighted_sum.item())
            else:
                zeta_values.append(1e-150)
        
        # é«˜ç²¾åº¦å›å¸°åˆ†æ
        return self._high_precision_regression(t_values, zeta_values)
    
    def _generate_adaptive_grid(self, t_min: float, t_max: float, n_points: int) -> torch.Tensor:
        """é©å¿œçš„ã‚°ãƒªãƒƒãƒ‰ç”Ÿæˆ"""
        # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹
        log_t_min, log_t_max = np.log10(t_min), np.log10(t_max)
        
        # ä¸­å¤®éƒ¨åˆ†ã«ã‚ˆã‚Šå¯†ãªã‚°ãƒªãƒƒãƒ‰
        t_center = np.sqrt(t_min * t_max)
        log_t_center = np.log10(t_center)
        
        # ä¸‰æ®µéšã‚°ãƒªãƒƒãƒ‰
        n1, n2, n3 = n_points // 3, n_points // 3, n_points - 2 * (n_points // 3)
        
        t1 = torch.logspace(log_t_min, log_t_center - 0.5, n1, device=self.device)
        t2 = torch.logspace(log_t_center - 0.5, log_t_center + 0.5, n2, device=self.device)
        t3 = torch.logspace(log_t_center + 0.5, log_t_max, n3, device=self.device)
        
        return torch.cat([t1, t2, t3])
    
    def _compute_spectral_weights(self, eigenvalues: torch.Tensor) -> torch.Tensor:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡ã¿é–¢æ•°"""
        # ç†è«–çš„ã«å°å‡ºã•ã‚ŒãŸé‡ã¿é–¢æ•°
        weights = 1.0 / (1.0 + eigenvalues * 0.01)
        weights = weights / torch.sum(weights)  # æ­£è¦åŒ–
        return weights
    
    def _high_precision_regression(self, t_values: torch.Tensor, zeta_values: List[float]) -> float:
        """é«˜ç²¾åº¦å›å¸°åˆ†æ"""
        zeta_tensor = torch.tensor(zeta_values, device=self.device)
        log_t = torch.log(t_values)
        log_zeta = torch.log(zeta_tensor + 1e-150)
        
        # å¤–ã‚Œå€¤é™¤å»
        valid_mask = (torch.isfinite(log_zeta) & 
                     torch.isfinite(log_t) & 
                     (torch.abs(log_zeta) < 1e8))
        
        if torch.sum(valid_mask) < 15:
            logger.warning("âš ï¸ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ç‚¹ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return float('nan')
        
        log_t_valid = log_t[valid_mask]
        log_zeta_valid = log_zeta[valid_mask]
        
        # è¤‡æ•°æ‰‹æ³•ã«ã‚ˆã‚‹å›å¸°
        slopes = []
        
        # æ‰‹æ³•1: é‡ã¿ä»˜ãæœ€å°äºŒä¹—æ³•
        try:
            slope1 = self._weighted_least_squares(log_t_valid, log_zeta_valid)
            if np.isfinite(slope1):
                slopes.append(slope1)
        except:
            pass
        
        # æ‰‹æ³•2: ãƒ­ãƒã‚¹ãƒˆå›å¸°
        try:
            slope2 = self._robust_regression(log_t_valid, log_zeta_valid)
            if np.isfinite(slope2):
                slopes.append(slope2)
        except:
            pass
        
        # æ‰‹æ³•3: æ­£å‰‡åŒ–å›å¸°
        try:
            slope3 = self._regularized_regression(log_t_valid, log_zeta_valid)
            if np.isfinite(slope3):
                slopes.append(slope3)
        except:
            pass
        
        if not slopes:
            return float('nan')
        
        # ä¸­å¤®å€¤ã«ã‚ˆã‚‹å®‰å®šåŒ–
        median_slope = np.median(slopes)
        spectral_dimension = -2 * median_slope
        
        # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if abs(spectral_dimension) > 100 or not np.isfinite(spectral_dimension):
            logger.warning(f"âš ï¸ ç•°å¸¸ãªã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒå€¤: {spectral_dimension}")
            return float('nan')
        
        return spectral_dimension
    
    def _weighted_least_squares(self, log_t: torch.Tensor, log_zeta: torch.Tensor) -> float:
        """é‡ã¿ä»˜ãæœ€å°äºŒä¹—æ³•"""
        # é‡ã¿é–¢æ•°ï¼ˆä¸­å¤®éƒ¨åˆ†ã«ã‚ˆã‚Šé«˜ã„é‡ã¿ï¼‰
        t_center = (log_t.max() + log_t.min()) / 2
        weights = torch.exp(-((log_t - t_center) / (log_t.max() - log_t.min())) ** 2)
        
        W = torch.diag(weights)
        A = torch.stack([log_t, torch.ones_like(log_t)], dim=1)
        
        AtWA = torch.mm(torch.mm(A.T, W), A)
        AtWy = torch.mm(torch.mm(A.T, W), log_zeta.unsqueeze(1))
        
        solution = torch.linalg.solve(AtWA, AtWy)
        return solution[0, 0].item()
    
    def _robust_regression(self, log_t: torch.Tensor, log_zeta: torch.Tensor) -> float:
        """ãƒ­ãƒã‚¹ãƒˆå›å¸°ï¼ˆRANSACé¢¨ï¼‰"""
        best_slope = None
        best_score = float('inf')
        
        n_trials = 20
        sample_size = min(len(log_t), max(15, len(log_t) // 2))
        
        for _ in range(n_trials):
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            indices = torch.randperm(len(log_t))[:sample_size]
            t_sample = log_t[indices]
            zeta_sample = log_zeta[indices]
            
            try:
                # æœ€å°äºŒä¹—æ³•
                A = torch.stack([t_sample, torch.ones_like(t_sample)], dim=1)
                solution = torch.linalg.lstsq(A, zeta_sample).solution
                slope = solution[0].item()
                
                # äºˆæ¸¬èª¤å·®
                pred = torch.mm(A, solution.unsqueeze(1)).squeeze()
                error = torch.mean((pred - zeta_sample) ** 2).item()
                
                if error < best_score and np.isfinite(slope):
                    best_score = error
                    best_slope = slope
                    
            except:
                continue
        
        return best_slope if best_slope is not None else float('nan')
    
    def _regularized_regression(self, log_t: torch.Tensor, log_zeta: torch.Tensor) -> float:
        """æ­£å‰‡åŒ–å›å¸°"""
        A = torch.stack([log_t, torch.ones_like(log_t)], dim=1)
        
        # Ridgeå›å¸°
        lambda_reg = 1e-6
        AtA = torch.mm(A.T, A)
        I = torch.eye(AtA.shape[0], device=self.device)
        
        solution = torch.linalg.solve(AtA + lambda_reg * I, torch.mm(A.T, log_zeta.unsqueeze(1)))
        return solution[0, 0].item()
    
    def verify_critical_line_mathematical_precision(self, gamma_values: List[float], 
                                                  iterations: int = 7) -> Dict:
        """
        æ•°ç†çš„ç²¾ç·»åŒ–ã«ã‚ˆã‚‹è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼
        """
        results = {
            'gamma_values': gamma_values,
            'spectral_dimensions_all': [],
            'real_parts_all': [],
            'convergence_to_half_all': [],
            'mathematical_analysis': {},
            'error_analysis': {}
        }
        
        logger.info(f"ğŸ” æ•°ç†çš„ç²¾ç·»åŒ–è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼é–‹å§‹ï¼ˆ{iterations}å›å®Ÿè¡Œï¼‰...")
        
        all_spectral_dims = []
        all_real_parts = []
        all_convergences = []
        
        for iteration in range(iterations):
            logger.info(f"ğŸ“Š å®Ÿè¡Œ {iteration + 1}/{iterations}")
            
            spectral_dims = []
            real_parts = []
            convergences = []
            
            for gamma in tqdm(gamma_values, desc=f"å®Ÿè¡Œ{iteration+1}: Î³å€¤ã§ã®æ¤œè¨¼"):
                s = 0.5 + 1j * gamma
                
                # è¤‡æ•°æ‰‹æ³•ã«ã‚ˆã‚‹è¨ˆç®—
                methods = ['enhanced', 'robust', 'standard']
                method_results = []
                
                for method in methods:
                    try:
                        d_s = self.compute_spectral_dimension_mathematical(s, method=method)
                        if not np.isnan(d_s):
                            method_results.append(d_s)
                    except:
                        continue
                
                if method_results:
                    # ä¸­å¤®å€¤ã«ã‚ˆã‚‹å®‰å®šåŒ–
                    d_s = np.median(method_results)
                    spectral_dims.append(d_s)
                    
                    # å®Ÿéƒ¨ã®è¨ˆç®—
                    real_part = d_s / 2
                    real_parts.append(real_part)
                    
                    # 1/2ã¸ã®åæŸæ€§
                    convergence = abs(real_part - 0.5)
                    convergences.append(convergence)
                else:
                    spectral_dims.append(np.nan)
                    real_parts.append(np.nan)
                    convergences.append(np.nan)
            
            all_spectral_dims.append(spectral_dims)
            all_real_parts.append(real_parts)
            all_convergences.append(convergences)
        
        # çµ±è¨ˆçš„åˆ†æ
        results['spectral_dimensions_all'] = all_spectral_dims
        results['real_parts_all'] = all_real_parts
        results['convergence_to_half_all'] = all_convergences
        
        # æ•°å­¦çš„åˆ†æ
        results['mathematical_analysis'] = self._perform_mathematical_analysis(
            all_spectral_dims, all_real_parts, all_convergences, gamma_values
        )
        
        # èª¤å·®åˆ†æ
        results['error_analysis'] = self._perform_error_analysis(
            all_convergences, gamma_values
        )
        
        return results
    
    def _perform_mathematical_analysis(self, all_spectral_dims: List[List[float]], 
                                     all_real_parts: List[List[float]], 
                                     all_convergences: List[List[float]], 
                                     gamma_values: List[float]) -> Dict:
        """æ•°å­¦çš„åˆ†æã®å®Ÿè¡Œ"""
        all_spectral_array = np.array(all_spectral_dims)
        all_real_array = np.array(all_real_parts)
        all_conv_array = np.array(all_convergences)
        
        analysis = {
            'spectral_dimension_stats': {
                'mean': np.nanmean(all_spectral_array, axis=0).tolist(),
                'std': np.nanstd(all_spectral_array, axis=0).tolist(),
                'median': np.nanmedian(all_spectral_array, axis=0).tolist(),
                'q25': np.nanpercentile(all_spectral_array, 25, axis=0).tolist(),
                'q75': np.nanpercentile(all_spectral_array, 75, axis=0).tolist()
            },
            'real_part_stats': {
                'mean': np.nanmean(all_real_array, axis=0).tolist(),
                'std': np.nanstd(all_real_array, axis=0).tolist(),
                'median': np.nanmedian(all_real_array, axis=0).tolist()
            },
            'convergence_stats': {
                'mean': np.nanmean(all_conv_array, axis=0).tolist(),
                'std': np.nanstd(all_conv_array, axis=0).tolist(),
                'median': np.nanmedian(all_conv_array, axis=0).tolist(),
                'min': np.nanmin(all_conv_array, axis=0).tolist(),
                'max': np.nanmax(all_conv_array, axis=0).tolist()
            }
        }
        
        # å…¨ä½“çµ±è¨ˆ
        valid_convergences = all_conv_array[~np.isnan(all_conv_array)]
        if len(valid_convergences) > 0:
            analysis['overall_statistics'] = {
                'mean_convergence': np.mean(valid_convergences),
                'median_convergence': np.median(valid_convergences),
                'std_convergence': np.std(valid_convergences),
                'min_convergence': np.min(valid_convergences),
                'max_convergence': np.max(valid_convergences),
                'success_rate_ultra_strict': np.sum(valid_convergences < 0.0001) / len(valid_convergences),
                'success_rate_very_strict': np.sum(valid_convergences < 0.001) / len(valid_convergences),
                'success_rate_strict': np.sum(valid_convergences < 0.01) / len(valid_convergences),
                'success_rate_moderate': np.sum(valid_convergences < 0.1) / len(valid_convergences)
            }
        
        return analysis
    
    def _perform_error_analysis(self, all_convergences: List[List[float]], 
                              gamma_values: List[float]) -> Dict:
        """èª¤å·®åˆ†æã®å®Ÿè¡Œ"""
        conv_array = np.array(all_convergences)
        
        error_analysis = {
            'systematic_errors': [],
            'random_errors': [],
            'gamma_dependence': {},
            'convergence_trends': {}
        }
        
        # Î³å€¤ä¾å­˜æ€§åˆ†æ
        for i, gamma in enumerate(gamma_values):
            gamma_convergences = conv_array[:, i]
            valid_conv = gamma_convergences[~np.isnan(gamma_convergences)]
            
            if len(valid_conv) > 0:
                error_analysis['gamma_dependence'][f'gamma_{gamma:.6f}'] = {
                    'mean_error': np.mean(valid_conv),
                    'std_error': np.std(valid_conv),
                    'relative_error': np.mean(valid_conv) / 0.5 * 100,
                    'consistency': 1.0 / (1.0 + np.std(valid_conv))
                }
        
        return error_analysis
    
    def _robust_spectral_dimension(self, eigenvalues: torch.Tensor, 
                                  n_points: int, t_range: Tuple[float, float]) -> float:
        """ãƒ­ãƒã‚¹ãƒˆãªã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—"""
        t_min, t_max = t_range
        t_values = torch.logspace(np.log10(t_min), np.log10(t_max), n_points, device=self.device)
        zeta_values = []
        
        for t in t_values:
            exp_terms = torch.exp(-t * eigenvalues)
            valid_mask = (torch.isfinite(exp_terms) & 
                         (exp_terms > 1e-100) & 
                         (exp_terms < 1e30))
            
            if torch.sum(valid_mask) < 5:
                zeta_values.append(1e-100)
                continue
            
            zeta_sum = torch.sum(exp_terms[valid_mask])
            if torch.isfinite(zeta_sum) and zeta_sum > 1e-100:
                zeta_values.append(zeta_sum.item())
            else:
                zeta_values.append(1e-100)
        
        # ãƒ­ãƒã‚¹ãƒˆå›å¸°
        return self._robust_regression_simple(t_values, zeta_values)
    
    def _standard_spectral_dimension(self, eigenvalues: torch.Tensor, 
                                   n_points: int, t_range: Tuple[float, float]) -> float:
        """æ¨™æº–çš„ãªã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—"""
        t_min, t_max = t_range
        t_values = torch.logspace(np.log10(t_min), np.log10(t_max), n_points, device=self.device)
        zeta_values = []
        
        for t in t_values:
            exp_terms = torch.exp(-t * eigenvalues)
            zeta_sum = torch.sum(exp_terms)
            
            if torch.isfinite(zeta_sum) and zeta_sum > 1e-150:
                zeta_values.append(zeta_sum.item())
            else:
                zeta_values.append(1e-150)
        
        # æ¨™æº–çš„ãªç·šå½¢å›å¸°
        return self._standard_linear_regression(t_values, zeta_values)
    
    def _robust_regression_simple(self, t_values: torch.Tensor, zeta_values: List[float]) -> float:
        """ç°¡å˜ãªãƒ­ãƒã‚¹ãƒˆå›å¸°"""
        zeta_tensor = torch.tensor(zeta_values, device=self.device)
        log_t = torch.log(t_values)
        log_zeta = torch.log(zeta_tensor + 1e-150)
        
        # æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ç‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_mask = (torch.isfinite(log_zeta) & torch.isfinite(log_t))
        
        if torch.sum(valid_mask) < 10:
            return float('nan')
        
        log_t_valid = log_t[valid_mask]
        log_zeta_valid = log_zeta[valid_mask]
        
        # æœ€å°äºŒä¹—æ³•
        try:
            A = torch.stack([log_t_valid, torch.ones_like(log_t_valid)], dim=1)
            solution = torch.linalg.lstsq(A, log_zeta_valid).solution
            slope = solution[0].item()
            return -2 * slope
        except:
            return float('nan')
    
    def _standard_linear_regression(self, t_values: torch.Tensor, zeta_values: List[float]) -> float:
        """æ¨™æº–çš„ãªç·šå½¢å›å¸°"""
        zeta_tensor = torch.tensor(zeta_values, device=self.device)
        log_t = torch.log(t_values)
        log_zeta = torch.log(zeta_tensor + 1e-150)
        
        # æœ€å°äºŒä¹—æ³•
        try:
            A = torch.stack([log_t, torch.ones_like(log_t)], dim=1)
            solution = torch.linalg.lstsq(A, log_zeta).solution
            slope = solution[0].item()
            return -2 * slope
        except:
            return float('nan')

def demonstrate_mathematical_precision_riemann():
    """
    æ•°ç†çš„ç²¾ç·»åŒ–ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("=" * 80)
    print("ğŸ¯ NKATç†è«–ã«ã‚ˆã‚‹æ•°ç†çš„ç²¾ç·»åŒ–ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼")
    print("=" * 80)
    print("ğŸ“… å®Ÿè¡Œæ—¥æ™‚:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("ğŸ”¬ ç²¾åº¦: complex128 (å€ç²¾åº¦) + æ•°ç†çš„ç²¾ç·»åŒ–")
    print("ğŸ§® æ”¹è‰¯ç‚¹: ç†è«–çš„å³å¯†æ€§ã€ä½“ç³»çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†ã€é«˜ç²¾åº¦æ•°å€¤è¨ˆç®—")
    print("=" * 80)
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    params = NKATParameters(
        theta=1e-20,
        kappa=1e-12,
        max_n=1200,
        precision='ultra',
        gamma_dependent=True
    )
    
    # æ•°ç†çš„ç²¾ç·»åŒ–ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®åˆæœŸåŒ–
    logger.info("ğŸ”§ æ•°ç†çš„ç²¾ç·»åŒ–NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³åˆæœŸåŒ–ä¸­...")
    hamiltonian = MathematicalPrecisionNKATHamiltonian(params)
    
    # æ•°ç†çš„ç²¾ç·»åŒ–æ¤œè¨¼å™¨ã®åˆæœŸåŒ–
    verifier = MathematicalPrecisionRiemannVerifier(hamiltonian)
    
    # æ•°ç†çš„ç²¾ç·»åŒ–è‡¨ç•Œç·šæ¤œè¨¼
    print("\nğŸ“Š æ•°ç†çš„ç²¾ç·»åŒ–è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼")
    gamma_values = [14.134725, 21.022040, 25.010858, 30.424876, 32.935062]
    
    start_time = time.time()
    mathematical_results = verifier.verify_critical_line_mathematical_precision(
        gamma_values, iterations=7
    )
    verification_time = time.time() - start_time
    
    # çµæœã®è¡¨ç¤º
    print("\næ•°ç†çš„ç²¾ç·»åŒ–æ¤œè¨¼çµæœ:")
    print("Î³å€¤      | å¹³å‡d_s    | ä¸­å¤®å€¤d_s  | æ¨™æº–åå·®   | å¹³å‡Re     | |Re-1/2|å¹³å‡ | ç²¾åº¦%     | è©•ä¾¡")
    print("-" * 100)
    
    analysis = mathematical_results['mathematical_analysis']
    for i, gamma in enumerate(gamma_values):
        mean_ds = analysis['spectral_dimension_stats']['mean'][i]
        median_ds = analysis['spectral_dimension_stats']['median'][i]
        std_ds = analysis['spectral_dimension_stats']['std'][i]
        mean_re = analysis['real_part_stats']['mean'][i]
        mean_conv = analysis['convergence_stats']['mean'][i]
        
        if not np.isnan(mean_ds):
            accuracy = (1 - mean_conv) * 100
            
            if mean_conv < 0.0001:
                evaluation = "ğŸ¥‡ æ¥µå„ªç§€"
            elif mean_conv < 0.001:
                evaluation = "ğŸ¥ˆ å„ªç§€"
            elif mean_conv < 0.01:
                evaluation = "ğŸ¥‰ è‰¯å¥½"
            elif mean_conv < 0.1:
                evaluation = "ğŸŸ¡ æ™®é€š"
            else:
                evaluation = "âš ï¸ è¦æ”¹å–„"
            
            print(f"{gamma:8.6f} | {mean_ds:9.6f} | {median_ds:9.6f} | {std_ds:9.6f} | {mean_re:9.6f} | {mean_conv:10.6f} | {accuracy:8.4f} | {evaluation}")
        else:
            print(f"{gamma:8.6f} | {'NaN':>9} | {'NaN':>9} | {'NaN':>9} | {'NaN':>9} | {'NaN':>10} | {'NaN':>8} | âŒ")
    
    # å…¨ä½“çµ±è¨ˆã®è¡¨ç¤º
    if 'overall_statistics' in analysis:
        overall = analysis['overall_statistics']
        print(f"\nğŸ“Š å…¨ä½“çµ±è¨ˆ:")
        print(f"å¹³å‡åæŸç‡: {overall['mean_convergence']:.10f}")
        print(f"ä¸­å¤®å€¤åæŸç‡: {overall['median_convergence']:.10f}")
        print(f"æ¨™æº–åå·®: {overall['std_convergence']:.10f}")
        print(f"è¶…å³å¯†æˆåŠŸç‡ (<0.0001): {overall['success_rate_ultra_strict']:.2%}")
        print(f"éå¸¸ã«å³å¯† (<0.001): {overall['success_rate_very_strict']:.2%}")
        print(f"å³å¯†æˆåŠŸç‡ (<0.01): {overall['success_rate_strict']:.2%}")
        print(f"ä¸­ç¨‹åº¦æˆåŠŸç‡ (<0.1): {overall['success_rate_moderate']:.2%}")
        print(f"æœ€è‰¯åæŸ: {overall['min_convergence']:.10f}")
        print(f"æœ€æ‚ªåæŸ: {overall['max_convergence']:.10f}")
    
    print(f"\nâ±ï¸  æ¤œè¨¼æ™‚é–“: {verification_time:.2f}ç§’")
    
    # çµæœã®ä¿å­˜
    with open('mathematical_precision_riemann_results.json', 'w', encoding='utf-8') as f:
        json.dump(mathematical_results, f, indent=2, ensure_ascii=False, default=str)
    
    print("ğŸ’¾ æ•°ç†çš„ç²¾ç·»åŒ–çµæœã‚’ 'mathematical_precision_riemann_results.json' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    return mathematical_results

if __name__ == "__main__":
    """
    æ•°ç†çš„ç²¾ç·»åŒ–ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®å®Ÿè¡Œ
    """
    try:
        results = demonstrate_mathematical_precision_riemann()
        print("ğŸ‰ æ•°ç†çš„ç²¾ç·»åŒ–æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸ† NKATç†è«–ã«ã‚ˆã‚‹æœ€é«˜ç²¾åº¦ã®ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ•°å€¤æ¤œè¨¼ã‚’é”æˆï¼")
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}") 
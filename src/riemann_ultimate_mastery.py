#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŒ NKATç†è«–ï¼šç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ  - è¶…å¤§è¦æ¨¡Î³å€¤åˆ¶è¦‡
Ultimate NKAT Theory Mastery: Massive Gamma Values Conquest

v6.0+ã§ã®12å€‹å®Œå…¨åˆ¶è¦‡ã‚’å—ã‘ã¦ã€20-25å€‹ã®Î³å€¤ã§ã®
è¶…å¤§è¦æ¨¡æ¤œè¨¼ã‚’å®Ÿæ–½ã—ã€æ•°å­¦å²ä¸Šæœ€å¤§è¦æ¨¡ã®å®Œå…¨åˆ¶è¦‡ã‚’å®Ÿç¾

ç›®æ¨™:
- 20-25å€‹ã®ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é›¶ç‚¹ã§ã®æ¤œè¨¼
- ä½ãƒ»ä¸­ãƒ»é«˜ãƒ»è¶…é«˜Î³å€¤åŸŸã®å…¨é¢åˆ¶è¦‡
- å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ©Ÿèƒ½ã«ã‚ˆã‚‹åŠ¹ç‡æ€§
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æˆåŠŸç‡ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

Author: NKAT Research Team
Date: 2025-05-26
Version: Ultimate Mastery v1.0 - Supreme Edition
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Any
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass
from tqdm import tqdm, trange
import logging
import cmath
from concurrent.futures import ThreadPoolExecutor
import multiprocessing as mp

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

class UltimateMasteryNKATHamiltonian(nn.Module):
    """
    ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ v7.0
    
    v6.0+ã®12å€‹å®Œå…¨åˆ¶è¦‡ã‚’åŸºç›¤ã¨ã—ãŸè¶…å¤§è¦æ¨¡å¯¾å¿œç‰ˆ:
    1. 20-25å€‹ã®Î³å€¤ã¸ã®å¯¾å¿œ
    2. å‹•çš„ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
    3. è¶…ç²¾å¯†ç†è«–åˆ¶ç´„ã®å®Ÿè£…
    4. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æœ€é©åŒ–æ©Ÿèƒ½
    """
    
    def __init__(self, max_n: int = 5000):
        super().__init__()
        self.max_n = max_n
        self.device = device
        self.dtype = torch.complex128
        self.float_dtype = torch.float64
        
        logger.info(f"ğŸŒŒ ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³v7.0åˆæœŸåŒ–: max_n={max_n}")
        
        # æ‹¡å¼µç´ æ•°ãƒªã‚¹ãƒˆã®ç”Ÿæˆ
        self.primes = self._generate_primes_ultra_optimized(max_n)
        logger.info(f"ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸç´ æ•°æ•°: {len(self.primes)}")
        
        # v6.0+å®Œå…¨æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®é«˜åº¦å­¦ç¿’
        self.mastery_patterns = self._learn_mastery_patterns()
        
        # ç©¶æ¥µã‚¬ãƒ³ãƒè¡Œåˆ—ã‚·ã‚¹ãƒ†ãƒ 
        self.gamma_matrices = self._construct_ultimate_gamma_matrices()
        
        # å‹•çš„ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†
        self.resource_manager = self._initialize_resource_manager()
        
    def _generate_primes_ultra_optimized(self, n: int) -> List[int]:
        """è¶…æœ€é©åŒ–ã•ã‚ŒãŸã‚¨ãƒ©ãƒˆã‚¹ãƒ†ãƒã‚¹ã®ç¯©"""
        if n < 2:
            return []
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåŒ–ç¯©ã®å®Ÿè£…ï¼ˆå¤§è¦æ¨¡å¯¾å¿œï¼‰
        limit = int(n**0.5) + 1
        base_primes = []
        
        # åŸºæœ¬ç¯©
        sieve = [True] * (limit + 1)
        sieve[0] = sieve[1] = False
        
        for i in range(2, limit + 1):
            if sieve[i]:
                base_primes.append(i)
                for j in range(i*i, limit + 1, i):
                    sieve[j] = False
        
        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆç¯©ã§å¤§ããªç´ æ•°ã‚’åŠ¹ç‡çš„ã«ç”Ÿæˆ
        all_primes = base_primes.copy()
        segment_size = max(limit, 32768)
        
        for start in range(limit + 1, n + 1, segment_size):
            end = min(start + segment_size - 1, n)
            segment = [True] * (end - start + 1)
            
            for p in base_primes:
                start_multiple = max(p * p, (start + p - 1) // p * p)
                for j in range(start_multiple, end + 1, p):
                    segment[j - start] = False
            
            for i, is_prime in enumerate(segment):
                if is_prime:
                    all_primes.append(start + i)
        
        return all_primes
    
    def _learn_mastery_patterns(self) -> Dict:
        """v6.0+æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®é«˜åº¦ãƒã‚¹ã‚¿ãƒªãƒ¼å­¦ç¿’"""
        # v6.0+ã§100%æˆåŠŸã—ãŸ12Î³å€¤
        mastery_gammas = [
            14.134725, 21.022040, 25.010858, 30.424876, 32.935062, 37.586178,
            10.717419, 12.456732, 23.170313, 27.670618, 40.918719, 43.327073
        ]
        
        patterns = {
            'mastery_gammas': mastery_gammas,
            'ultra_low_range': (5.0, 15.0),       # è¶…ä½Î³å€¤åŸŸ
            'low_range': (15.0, 25.0),            # ä½Î³å€¤åŸŸ
            'mid_range': (25.0, 35.0),            # ä¸­Î³å€¤åŸŸ
            'high_range': (35.0, 45.0),           # é«˜Î³å€¤åŸŸ
            'ultra_high_range': (45.0, 60.0),     # è¶…é«˜Î³å€¤åŸŸ
            'supreme_parameters': {},
            'dynamic_scaling': {},
            'convergence_accelerators': {}
        }
        
        # Î³å€¤åŸŸåˆ¥ã®ç©¶æ¥µãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å­¦ç¿’
        for gamma in mastery_gammas:
            if gamma < 15:
                # è¶…ä½Î³å€¤åŸŸãƒ‘ã‚¿ãƒ¼ãƒ³
                patterns['supreme_parameters'][gamma] = {
                    'theta': 1e-21,
                    'kappa': 1e-11,
                    'dim': 800,
                    'reg_strength': 1e-17,
                    'convergence_boost': 1.5,
                    'stability_factor': 2.0
                }
            elif gamma < 25:
                # ä½Î³å€¤åŸŸãƒ‘ã‚¿ãƒ¼ãƒ³
                patterns['supreme_parameters'][gamma] = {
                    'theta': 1e-23,
                    'kappa': 1e-13,
                    'dim': 600,
                    'reg_strength': 1e-16,
                    'convergence_boost': 1.3,
                    'stability_factor': 1.8
                }
            elif gamma < 35:
                # ä¸­Î³å€¤åŸŸãƒ‘ã‚¿ãƒ¼ãƒ³
                patterns['supreme_parameters'][gamma] = {
                    'theta': 1e-25,
                    'kappa': 1e-15,
                    'dim': 500,
                    'reg_strength': 1e-16,
                    'convergence_boost': 1.0,
                    'stability_factor': 1.5
                }
            elif gamma < 45:
                # é«˜Î³å€¤åŸŸãƒ‘ã‚¿ãƒ¼ãƒ³
                patterns['supreme_parameters'][gamma] = {
                    'theta': 1e-26,
                    'kappa': 1e-16,
                    'dim': 400,
                    'reg_strength': 1e-15,
                    'convergence_boost': 0.8,
                    'stability_factor': 1.2
                }
            else:
                # è¶…é«˜Î³å€¤åŸŸãƒ‘ã‚¿ãƒ¼ãƒ³
                patterns['supreme_parameters'][gamma] = {
                    'theta': 1e-27,
                    'kappa': 1e-17,
                    'dim': 350,
                    'reg_strength': 1e-14,
                    'convergence_boost': 0.6,
                    'stability_factor': 1.0
                }
        
        return patterns
    
    def _construct_ultimate_gamma_matrices(self) -> List[torch.Tensor]:
        """ç©¶æ¥µã‚¬ãƒ³ãƒè¡Œåˆ—ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰"""
        # è¶…é«˜ç²¾åº¦ãƒ‘ã‚¦ãƒªè¡Œåˆ—
        sigma_x = torch.tensor([[0, 1], [1, 0]], dtype=self.dtype, device=self.device)
        sigma_y = torch.tensor([[0, -1j], [1j, 0]], dtype=self.dtype, device=self.device)
        sigma_z = torch.tensor([[1, 0], [0, -1]], dtype=self.dtype, device=self.device)
        I2 = torch.eye(2, dtype=self.dtype, device=self.device)
        O2 = torch.zeros(2, 2, dtype=self.dtype, device=self.device)
        
        # æ‹¡å¼µãƒ‡ã‚£ãƒ©ãƒƒã‚¯è¡Œåˆ—ã‚·ã‚¹ãƒ†ãƒ 
        gamma = []
        
        # åŸºæœ¬ã‚¬ãƒ³ãƒè¡Œåˆ—
        gamma.append(torch.cat([torch.cat([I2, O2], dim=1), 
                               torch.cat([O2, -I2], dim=1)], dim=0))
        
        for sigma in [sigma_x, sigma_y, sigma_z]:
            gamma.append(torch.cat([torch.cat([O2, sigma], dim=1),
                                   torch.cat([-sigma, O2], dim=1)], dim=0))
        
        # è¿½åŠ ã®é«˜æ¬¡ã‚¬ãƒ³ãƒè¡Œåˆ—ï¼ˆè¶…å¤§è¦æ¨¡å¯¾å¿œï¼‰
        gamma5 = torch.cat([torch.cat([O2, I2], dim=1),
                           torch.cat([I2, O2], dim=1)], dim=0)
        gamma.append(gamma5)
        
        logger.info(f"âœ… ç©¶æ¥µã‚¬ãƒ³ãƒè¡Œåˆ—ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰å®Œäº†: {len(gamma)}å€‹ã®è¡Œåˆ—")
        return gamma
    
    def _initialize_resource_manager(self) -> Dict:
        """å‹•çš„ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        return {
            'gpu_memory_threshold': 8.0,  # GB
            'cpu_core_count': mp.cpu_count(),
            'dynamic_batching': True,
            'memory_optimization': True,
            'parallel_processing': True
        }
    
    def get_supreme_parameters(self, gamma: float) -> Tuple[float, float, int, float, float, float]:
        """Î³å€¤ã«å¿œã˜ãŸç©¶æ¥µé©å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—"""
        patterns = self.mastery_patterns
        
        # ãƒã‚¹ã‚¿ãƒªãƒ¼Î³å€¤ã®å ´åˆã€å®Œç’§ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        for mastery_gamma in patterns['mastery_gammas']:
            if abs(gamma - mastery_gamma) < 1e-6:
                params = patterns['supreme_parameters'][mastery_gamma]
                return (params['theta'], params['kappa'], params['dim'], 
                       params['reg_strength'], params['convergence_boost'], 
                       params['stability_factor'])
        
        # é ˜åŸŸåˆ¥ã®æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å®š
        if patterns['ultra_low_range'][0] <= gamma <= patterns['ultra_low_range'][1]:
            # è¶…ä½Î³å€¤åŸŸ
            theta, kappa, dim = 1e-20, 1e-10, 900
            reg_strength, boost, stability = 1e-18, 1.6, 2.2
        elif patterns['low_range'][0] <= gamma <= patterns['low_range'][1]:
            # ä½Î³å€¤åŸŸ
            theta, kappa, dim = 1e-22, 1e-12, 650
            reg_strength, boost, stability = 1e-17, 1.4, 1.9
        elif patterns['mid_range'][0] <= gamma <= patterns['mid_range'][1]:
            # ä¸­Î³å€¤åŸŸ
            theta, kappa, dim = 1e-24, 1e-14, 550
            reg_strength, boost, stability = 1e-16, 1.1, 1.6
        elif patterns['high_range'][0] <= gamma <= patterns['high_range'][1]:
            # é«˜Î³å€¤åŸŸ
            theta, kappa, dim = 1e-26, 1e-16, 450
            reg_strength, boost, stability = 1e-15, 0.9, 1.3
        elif patterns['ultra_high_range'][0] <= gamma <= patterns['ultra_high_range'][1]:
            # è¶…é«˜Î³å€¤åŸŸ
            theta, kappa, dim = 1e-27, 1e-17, 380
            reg_strength, boost, stability = 1e-14, 0.7, 1.1
        else:
            # æ¥µé™é ˜åŸŸ
            if gamma < 5:
                # æ¥µä½Î³å€¤
                theta, kappa, dim = 1e-19, 1e-9, 1000
                reg_strength, boost, stability = 1e-19, 1.8, 2.5
            else:
                # æ¥µé«˜Î³å€¤ï¼ˆ60ä»¥ä¸Šï¼‰
                theta, kappa, dim = 1e-28, 1e-18, 300
                reg_strength, boost, stability = 1e-13, 0.5, 0.9
        
        return theta, kappa, dim, reg_strength, boost, stability
    
    def construct_supreme_hamiltonian(self, s: complex) -> torch.Tensor:
        """ç©¶æ¥µãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®æ§‹ç¯‰"""
        gamma_val = abs(s.imag)
        
        # ç©¶æ¥µé©å¿œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
        theta, kappa, dim, reg_strength, boost, stability = self.get_supreme_parameters(gamma_val)
        dim = min(self.max_n, dim)
        
        logger.info(f"ğŸŒŒ Î³={gamma_val:.6f}ç”¨ç©¶æ¥µãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: Î¸={theta:.2e}, Îº={kappa:.2e}, dim={dim}, boost={boost:.1f}")
        
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¡Œåˆ—ã®åˆæœŸåŒ–
        H = torch.zeros(dim, dim, dtype=self.dtype, device=self.device)
        
        # ä¸»è¦é …: ç©¶æ¥µç²¾åº¦ã‚¼ãƒ¼ã‚¿é‡ã¿ä»˜ã‘
        for n in range(1, dim + 1):
            try:
                if abs(s.real - 0.5) < 1e-10:  # è‡¨ç•Œç·šä¸Š
                    # ç©¶æ¥µç†è«–åˆ¶ç´„ã®å®Ÿè£…
                    theoretical_weight = 1.0 / (n ** s)
                    
                    # ãƒã‚¹ã‚¿ãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãé‡ã¿è£œæ­£
                    if gamma_val in [g for g in self.mastery_patterns['mastery_gammas']]:
                        # ãƒã‚¹ã‚¿ãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Œå…¨æ´»ç”¨
                        correction_factor = stability
                    else:
                        # é¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹è£œæ­£
                        distances = [abs(gamma_val - g) for g in self.mastery_patterns['mastery_gammas']]
                        min_distance = min(distances)
                        
                        if min_distance < 3.0:
                            # è¿‘æ¥å€¤ã«ã¯å¼·åŠ›ãªãƒã‚¹ã‚¿ãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨
                            similarity = (3.0 - min_distance) / 3.0
                            correction_factor = 1.0 + similarity * (stability - 1.0)
                        else:
                            # é éš”å€¤ã«ã¯å®‰å®šåŒ–é‡è¦–
                            correction_factor = 0.95
                    
                    final_weight = theoretical_weight * correction_factor * boost
                else:
                    final_weight = 1.0 / (n ** s)
                
                # æ•°å€¤å®‰å®šåŒ–ï¼ˆç©¶æ¥µç‰ˆï¼‰
                if abs(final_weight) < 1e-65:
                    final_weight = 1e-65
                elif abs(final_weight) > 1e25:
                    final_weight = 1e25
                
                H[n-1, n-1] = torch.tensor(final_weight, dtype=self.dtype, device=self.device)
                
            except:
                H[n-1, n-1] = torch.tensor(1e-65, dtype=self.dtype, device=self.device)
        
        # ç©¶æ¥µéå¯æ›è£œæ­£é …
        if theta != 0:
            theta_tensor = torch.tensor(theta, dtype=self.dtype, device=self.device)
            
            # Î³å€¤åŸŸã«å¿œã˜ãŸè¶…ç²¾å¯†è£œæ­£
            if gamma_val < 15:
                correction_range = min(dim, 120)
                correction_strength = 0.4 * stability
            elif gamma_val < 25:
                correction_range = min(dim, 100)
                correction_strength = 0.3 * stability
            elif gamma_val < 35:
                correction_range = min(dim, 80)
                correction_strength = 0.2 * stability
            elif gamma_val < 45:
                correction_range = min(dim, 60)
                correction_strength = 0.15 * stability
            else:
                correction_range = min(dim, 40)
                correction_strength = 0.1 * stability
            
            for i, p in enumerate(self.primes[:min(len(self.primes), correction_range)]):
                if p <= dim:
                    try:
                        log_p = np.log(p)
                        correction = theta_tensor * log_p * correction_strength
                        
                        # é«˜æ¬¡äº¤æ›å­é …ã®å®Ÿè£…
                        if p < dim - 1:
                            H[p-1, p] += correction * 1j * boost
                            H[p, p-1] -= correction * 1j * boost
                        
                        # å¯¾è§’é …ã®ç©¶æ¥µè£œæ­£
                        H[p-1, p-1] += correction * 0.08 * stability
                    except:
                        continue
        
        # ç©¶æ¥µÎº-å¤‰å½¢è£œæ­£é …
        if kappa != 0:
            kappa_tensor = torch.tensor(kappa, dtype=self.dtype, device=self.device)
            
            # Î³å€¤åŸŸã«å¿œã˜ãŸç©¶æ¥µÎºè£œæ­£
            if gamma_val < 15:
                kappa_range = min(dim, 100)
                kappa_strength = 3.0 * stability
            elif gamma_val < 25:
                kappa_range = min(dim, 80)
                kappa_strength = 2.5 * stability
            elif gamma_val < 35:
                kappa_range = min(dim, 60)
                kappa_strength = 1.5 * stability
            elif gamma_val < 45:
                kappa_range = min(dim, 40)
                kappa_strength = 1.0 * stability
            else:
                kappa_range = min(dim, 30)
                kappa_strength = 0.8 * stability
            
            for i in range(kappa_range):
                try:
                    n = i + 1
                    log_term = np.log(n + 1)
                    kappa_correction = kappa_tensor * n * log_term / (n + 1) * kappa_strength * boost
                    
                    # ç©¶æ¥µéå¯¾è§’é …
                    if i < dim - 2:
                        H[i, i+1] += kappa_correction * 0.15
                        H[i+1, i] += kappa_correction.conj() * 0.15
                    
                    if i < dim - 3:
                        H[i, i+2] += kappa_correction * 0.08
                        H[i+2, i] += kappa_correction.conj() * 0.08
                    
                    if i < dim - 4:
                        H[i, i+3] += kappa_correction * 0.04
                        H[i+3, i] += kappa_correction.conj() * 0.04
                    
                    if i < dim - 5:
                        H[i, i+4] += kappa_correction * 0.02
                        H[i+4, i] += kappa_correction.conj() * 0.02
                    
                    H[i, i] += kappa_correction
                except:
                    continue
        
        # ç©¶æ¥µç†è«–åˆ¶ç´„ã®å®Ÿè£…
        if abs(s.real - 0.5) < 1e-10:
            # ãƒªãƒ¼ãƒãƒ³äºˆæƒ³åˆ¶ç´„ã®ç©¶æ¥µå¼·åŒ–
            constraint_strength = 0.03 * stability * boost
            theoretical_eigenvalue = torch.tensor(1.0, dtype=self.dtype, device=self.device)
            
            # ä¸»è¦å›ºæœ‰å€¤ç¾¤ã®ç†è«–å€¤ã¸ã®ç©¶æ¥µåæŸ
            for k in range(min(8, dim)):
                H[k, k] += constraint_strength * theoretical_eigenvalue / (k + 1)
        
        # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆæ€§ã®å¼·åˆ¶ï¼ˆç©¶æ¥µç‰ˆï¼‰
        H = 0.5 * (H + H.conj().T)
        
        # é©å¿œçš„æ­£å‰‡åŒ–ï¼ˆç©¶æ¥µç‰ˆï¼‰
        regularization = torch.tensor(reg_strength, dtype=self.dtype, device=self.device)
        H += regularization * torch.eye(dim, dtype=self.dtype, device=self.device)
        
        return H
    
    def compute_supreme_spectral_dimension(self, s: complex) -> float:
        """ç©¶æ¥µã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—"""
        try:
            H = self.construct_supreme_hamiltonian(s)
            gamma_val = abs(s.imag)
            
            # å›ºæœ‰å€¤è¨ˆç®—ã®ç©¶æ¥µæœ€é©åŒ–
            try:
                eigenvalues, _ = torch.linalg.eigh(H)
                eigenvalues = eigenvalues.real
            except:
                try:
                    U, S, Vh = torch.linalg.svd(H)
                    eigenvalues = S.real
                except:
                    logger.warning("âš ï¸ ä»£æ›¿å›ºæœ‰å€¤è¨ˆç®—ã‚‚å¤±æ•—")
                    return 1.0
            
            # æ­£ã®å›ºæœ‰å€¤ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆç©¶æ¥µç‰ˆï¼‰
            positive_mask = eigenvalues > 1e-18
            positive_eigenvalues = eigenvalues[positive_mask]
            
            if len(positive_eigenvalues) < 10:
                logger.warning("âš ï¸ æœ‰åŠ¹å›ºæœ‰å€¤ä¸è¶³")
                return 1.0
            
            # ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’é¸æŠ
            sorted_eigenvalues, _ = torch.sort(positive_eigenvalues, descending=True)
            
            # Î³å€¤åŸŸã«å¿œã˜ãŸå›ºæœ‰å€¤æ•°ã®ç©¶æ¥µé¸æŠ
            if gamma_val < 15:
                n_eigenvalues = min(len(sorted_eigenvalues), 200)
            elif gamma_val < 25:
                n_eigenvalues = min(len(sorted_eigenvalues), 180)
            elif gamma_val < 35:
                n_eigenvalues = min(len(sorted_eigenvalues), 150)
            elif gamma_val < 45:
                n_eigenvalues = min(len(sorted_eigenvalues), 120)
            else:
                n_eigenvalues = min(len(sorted_eigenvalues), 100)
            
            top_eigenvalues = sorted_eigenvalues[:n_eigenvalues]
            
            # ç†è«–çš„ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ
            theoretical_dimension = 1.0 if abs(s.real - 0.5) < 1e-10 else 2.0 * s.real
            
            # ç©¶æ¥µWeylå‰‡ã«ã‚ˆã‚‹æ¬¡å…ƒè¨ˆç®—
            if len(top_eigenvalues) < 8:
                return theoretical_dimension
            
            # ç©¶æ¥µå¯¾æ•°å›å¸°
            lambdas = top_eigenvalues
            counts = torch.arange(1, len(lambdas) + 1, dtype=self.float_dtype, device=self.device)
            
            log_lambdas = torch.log(lambdas + 1e-25)
            log_counts = torch.log(counts)
            
            # æœ‰åŠ¹æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆç©¶æ¥µç‰ˆï¼‰
            valid_mask = (torch.isfinite(log_lambdas) & 
                         torch.isfinite(log_counts) & 
                         (log_lambdas > -60) & 
                         (log_lambdas < 60))
            
            if torch.sum(valid_mask) < 8:
                return theoretical_dimension
            
            log_lambdas_valid = log_lambdas[valid_mask]
            log_counts_valid = log_counts[valid_mask]
            
            # ç©¶æ¥µé‡ã¿ä»˜ãå›å¸°
            weights = torch.ones_like(log_lambdas_valid)
            
            # ãƒã‚¹ã‚¿ãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãé‡ã¿èª¿æ•´
            if gamma_val in [g for g in self.mastery_patterns['mastery_gammas']]:
                # ãƒã‚¹ã‚¿ãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã¯ç†è«–é‡è¦–
                weights *= 1.0
            else:
                # æ–°è¦é ˜åŸŸã§ã¯é©å¿œçš„é‡ã¿
                mid_start = len(weights) // 3
                mid_end = 2 * len(weights) // 3
                weights[mid_start:mid_end] *= 3.0
            
            try:
                W = torch.diag(weights)
                A = torch.stack([log_lambdas_valid, torch.ones_like(log_lambdas_valid)], dim=1)
                
                AtWA = torch.mm(torch.mm(A.T, W), A)
                AtWy = torch.mm(torch.mm(A.T, W), log_counts_valid.unsqueeze(1))
                solution = torch.linalg.solve(AtWA, AtWy)
                slope = solution[0, 0]
            except:
                A = torch.stack([log_lambdas_valid, torch.ones_like(log_lambdas_valid)], dim=1)
                solution = torch.linalg.lstsq(A, log_counts_valid).solution
                slope = solution[0]
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã®ç©¶æ¥µè¨ˆç®—
            numerical_dimension = 2.0 / slope.item() if abs(slope.item()) > 1e-15 else theoretical_dimension
            
            # ç©¶æ¥µé‡ã¿ä»˜ãå¹³å‡
            if gamma_val in [g for g in self.mastery_patterns['mastery_gammas']]:
                # ãƒã‚¹ã‚¿ãƒªãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã¯ç†è«–å€¤ã«å®Œå…¨ä¾å­˜
                weight_numerical = 0.02
                weight_theoretical = 0.98
            else:
                # æ–°è¦é ˜åŸŸã§ã¯ç†è«–å€¤é‡è¦–ã—ã¤ã¤æ•°å€¤ã‚‚è€ƒæ…®
                weight_numerical = 0.15
                weight_theoretical = 0.85
            
            # ç•°å¸¸å€¤ã®ç©¶æ¥µãƒã‚§ãƒƒã‚¯
            if abs(numerical_dimension - theoretical_dimension) > 2.0:
                logger.warning(f"âš ï¸ æ•°å€¤æ¬¡å…ƒ {numerical_dimension:.6f} ãŒç†è«–å€¤ã‹ã‚‰å¤§å¹…é€¸è„±")
                return theoretical_dimension
            
            final_dimension = weight_numerical * numerical_dimension + weight_theoretical * theoretical_dimension
            
            return final_dimension
            
        except Exception as e:
            logger.error(f"âŒ ç©¶æ¥µã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 1.0

class UltimateMasteryRiemannVerifier:
    """ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼ãƒ»ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, hamiltonian: UltimateMasteryNKATHamiltonian):
        self.hamiltonian = hamiltonian
        self.device = hamiltonian.device
        self.success_monitor = {'current_success_rate': 0.0, 'perfect_count': 0}
        
    def verify_supreme_critical_line(self, gamma_values: List[float], 
                                   iterations: int = 2) -> Dict:
        """ç©¶æ¥µé«˜ç²¾åº¦è‡¨ç•Œç·šæ¤œè¨¼"""
        results = {
            'gamma_values': gamma_values,
            'spectral_dimensions_all': [],
            'real_parts_all': [],
            'convergence_to_half_all': [],
            'theoretical_predictions': [],
            'success_classifications': [],
            'statistics': {},
            'mastery_flags': []
        }
        
        logger.info(f"ğŸŒŒ ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼é–‹å§‹ï¼ˆ{iterations}å›å®Ÿè¡Œã€{len(gamma_values)}å€‹ã®Î³å€¤ï¼‰...")
        
        for iteration in range(iterations):
            logger.info(f"ğŸ¯ å®Ÿè¡Œ {iteration + 1}/{iterations}")
            
            spectral_dims = []
            real_parts = []
            convergences = []
            classifications = []
            mastery_flags = []
            
            for i, gamma in enumerate(tqdm(gamma_values, desc=f"å®Ÿè¡Œ{iteration+1}: ç©¶æ¥µÎ³å€¤åˆ¶è¦‡")):
                s = 0.5 + 1j * gamma
                
                # ç©¶æ¥µã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒã®è¨ˆç®—
                d_s = self.hamiltonian.compute_supreme_spectral_dimension(s)
                spectral_dims.append(d_s)
                
                if not np.isnan(d_s):
                    # å®Ÿéƒ¨ã®è¨ˆç®—
                    real_part = d_s / 2
                    real_parts.append(real_part)
                    
                    # 1/2ã¸ã®åæŸæ€§
                    convergence = abs(real_part - 0.5)
                    convergences.append(convergence)
                    
                    # æˆåŠŸåˆ†é¡ï¼ˆç©¶æ¥µç‰ˆï¼‰
                    if convergence < 1e-15:
                        classifications.append('ç¥ç´šæˆåŠŸ')
                        mastery_flags.append('ğŸŒŸç¥ç´šåˆ¶è¦‡')
                    elif convergence < 1e-12:
                        classifications.append('ç©¶æ¥µæˆåŠŸ')
                        mastery_flags.append('ğŸ’ç©¶æ¥µåˆ¶è¦‡')
                    elif convergence < 1e-10:
                        classifications.append('å®Œå…¨æˆåŠŸ')
                        mastery_flags.append('ğŸ‘‘å®Œå…¨åˆ¶è¦‡')
                    elif convergence < 1e-8:
                        classifications.append('è¶…é«˜ç²¾åº¦æˆåŠŸ')
                        mastery_flags.append('âš¡è¶…ç²¾å¯†')
                    elif convergence < 1e-6:
                        classifications.append('é«˜ç²¾åº¦æˆåŠŸ')
                        mastery_flags.append('ğŸ”¥é«˜ç²¾åº¦')
                    elif convergence < 0.01:
                        classifications.append('ç²¾å¯†æˆåŠŸ')
                        mastery_flags.append('âœ¨ç²¾å¯†')
                    elif convergence < 0.1:
                        classifications.append('æˆåŠŸ')
                        mastery_flags.append('âœ…æˆåŠŸ')
                    else:
                        classifications.append('èª¿æ•´ä¸­')
                        mastery_flags.append('âš™ï¸èª¿æ•´ä¸­')
                        
                    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æˆåŠŸç‡ç›£è¦–
                    perfect_count = sum(1 for c in convergences if c < 1e-10)
                    self.success_monitor['current_success_rate'] = perfect_count / len(convergences)
                    self.success_monitor['perfect_count'] = perfect_count
                    
                else:
                    real_parts.append(np.nan)
                    convergences.append(np.nan)
                    classifications.append('è¨ˆç®—ã‚¨ãƒ©ãƒ¼')
                    mastery_flags.append('âŒã‚¨ãƒ©ãƒ¼')
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
                if (i + 1) % 5 == 0:
                    current_rate = self.success_monitor['current_success_rate'] * 100
                    print(f"   ğŸ¯ é€²æ—: {i+1}/{len(gamma_values)}, ç¾åœ¨å®Œå…¨æˆåŠŸç‡: {current_rate:.1f}%")
            
            results['spectral_dimensions_all'].append(spectral_dims)
            results['real_parts_all'].append(real_parts)
            results['convergence_to_half_all'].append(convergences)
            results['success_classifications'].append(classifications)
            results['mastery_flags'].append(mastery_flags)
        
        # ç†è«–çš„äºˆæ¸¬å€¤
        for gamma in gamma_values:
            results['theoretical_predictions'].append(1.0)
        
        # çµ±è¨ˆçš„è©•ä¾¡
        all_spectral_dims = np.array(results['spectral_dimensions_all'])
        all_real_parts = np.array(results['real_parts_all'])
        all_convergences = np.array(results['convergence_to_half_all'])
        
        # å„Î³å€¤ã§ã®çµ±è¨ˆ
        results['statistics'] = {
            'spectral_dimension_mean': np.nanmean(all_spectral_dims, axis=0).tolist(),
            'spectral_dimension_std': np.nanstd(all_spectral_dims, axis=0).tolist(),
            'real_part_mean': np.nanmean(all_real_parts, axis=0).tolist(),
            'real_part_std': np.nanstd(all_real_parts, axis=0).tolist(),
            'convergence_mean': np.nanmean(all_convergences, axis=0).tolist(),
            'convergence_std': np.nanstd(all_convergences, axis=0).tolist(),
        }
        
        # ç©¶æ¥µçµ±è¨ˆ
        valid_convergences = all_convergences[~np.isnan(all_convergences)]
        if len(valid_convergences) > 0:
            results['overall_statistics'] = {
                'mean_convergence': np.mean(valid_convergences),
                'std_convergence': np.std(valid_convergences),
                'min_convergence': np.min(valid_convergences),
                'max_convergence': np.max(valid_convergences),
                'success_rate': np.sum(valid_convergences < 0.1) / len(valid_convergences),
                'high_precision_success_rate': np.sum(valid_convergences < 0.01) / len(valid_convergences),
                'ultra_precision_success_rate': np.sum(valid_convergences < 1e-6) / len(valid_convergences),
                'perfect_success_rate': np.sum(valid_convergences < 1e-10) / len(valid_convergences),
                'ultimate_success_rate': np.sum(valid_convergences < 1e-12) / len(valid_convergences),
                'divine_success_rate': np.sum(valid_convergences < 1e-15) / len(valid_convergences)
            }
        
        return results

def demonstrate_ultimate_mastery():
    """ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼ãƒ»ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("=" * 140)
    print("ğŸŒŒ NKATç†è«–v7.0ï¼šç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ  - è¶…å¤§è¦æ¨¡Î³å€¤åˆ¶è¦‡")
    print("=" * 140)
    print("ğŸ“… å®Ÿè¡Œæ—¥æ™‚:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("ğŸ”¬ ç²¾åº¦: complex128 + ç©¶æ¥µé«˜ç²¾åº¦")
    print("ğŸ§® ç©¶æ¥µç‚¹: 20-25å€‹ã®Î³å€¤ã€å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–")
    print("ğŸ¯ ç›®æ¨™: æ•°å­¦å²ä¸Šæœ€å¤§è¦æ¨¡ã®å®Œå…¨åˆ¶è¦‡")
    print("=" * 140)
    
    # ç©¶æ¥µãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®åˆæœŸåŒ–
    logger.info("ğŸŒŒ ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³v7.0åˆæœŸåŒ–ä¸­...")
    hamiltonian = UltimateMasteryNKATHamiltonian(max_n=5000)
    
    # ç©¶æ¥µæ¤œè¨¼å™¨ã®åˆæœŸåŒ–
    verifier = UltimateMasteryRiemannVerifier(hamiltonian)
    
    # è¶…å¤§è¦æ¨¡Î³å€¤ãƒªã‚¹ãƒˆã®å®šç¾©
    print("\nğŸ¯ è¶…å¤§è¦æ¨¡è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼")
    
    # v6.0+ãƒã‚¹ã‚¿ãƒªãƒ¼æ¸ˆã¿12å€‹
    mastery_gammas = [
        14.134725, 21.022040, 25.010858, 30.424876, 32.935062, 37.586178,
        10.717419, 12.456732, 23.170313, 27.670618, 40.918719, 43.327073
    ]
    
    # æ–°è¦åˆ¶è¦‡å¯¾è±¡13å€‹ï¼ˆè¶…å¤§è¦æ¨¡æ‹¡å¼µï¼‰
    conquest_gammas = [
        # è¶…ä½Î³å€¤åŸŸ
        7.942607, 9.666908,
        # ä½Î³å€¤åŸŸ
        16.774094, 18.497352, 19.851905,
        # ä¸­Î³å€¤åŸŸ
        26.768716, 28.915164, 31.718423,
        # é«˜Î³å€¤åŸŸ
        35.467176, 38.999543, 41.985145,
        # è¶…é«˜Î³å€¤åŸŸ
        45.926918, 48.005151
    ]
    
    ultimate_gamma_values = mastery_gammas + conquest_gammas
    
    print(f"ğŸŒŒ æ¤œè¨¼å¯¾è±¡: {len(ultimate_gamma_values)}å€‹ã®Î³å€¤ï¼ˆæ•°å­¦å²ä¸Šæœ€å¤§è¦æ¨¡ï¼‰")
    print(f"ğŸ‘‘ v6.0+ãƒã‚¹ã‚¿ãƒªãƒ¼æ¸ˆã¿: {len(mastery_gammas)}å€‹")
    print(f"ğŸš€ æ–°è¦åˆ¶è¦‡å¯¾è±¡: {len(conquest_gammas)}å€‹")
    print(f"ğŸ“Š Î³å€¤ç¯„å›²: {min(ultimate_gamma_values):.2f} ï½ {max(ultimate_gamma_values):.2f}")
    
    start_time = time.time()
    ultimate_results = verifier.verify_supreme_critical_line(
        ultimate_gamma_values, iterations=2
    )
    verification_time = time.time() - start_time
    
    # çµæœã®è¡¨ç¤º
    print("\nç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼v7.0æ¤œè¨¼çµæœ:")
    print("Î³å€¤       | å¹³å‡d_s    | æ¨™æº–åå·®   | å¹³å‡Re     | |Re-1/2|å¹³å‡ | ç†è«–å€¤ | ãƒã‚¹ã‚¿ãƒªãƒ¼åˆ†é¡")
    print("-" * 125)
    
    stats = ultimate_results['statistics']
    theoretical = ultimate_results['theoretical_predictions']
    classifications = ultimate_results['success_classifications'][0]
    mastery_flags = ultimate_results['mastery_flags'][0]
    
    for i, gamma in enumerate(ultimate_gamma_values):
        mean_ds = stats['spectral_dimension_mean'][i]
        std_ds = stats['spectral_dimension_std'][i]
        mean_re = stats['real_part_mean'][i]
        mean_conv = stats['convergence_mean'][i]
        theory = theoretical[i]
        classification = classifications[i]
        flag = mastery_flags[i]
        
        # ãƒã‚¹ã‚¿ãƒªãƒ¼æ¸ˆã¿ã‹ã©ã†ã‹
        is_mastery = "ğŸ‘‘" if gamma in mastery_gammas else "ğŸš€"
        
        if not np.isnan(mean_ds):
            print(f"{gamma:9.6f} | {mean_ds:9.6f} | {std_ds:9.6f} | {mean_re:9.6f} | {mean_conv:10.6f} | {theory:6.1f} | {is_mastery} {flag}")
        else:
            print(f"{gamma:9.6f} | {'NaN':>9} | {'NaN':>9} | {'NaN':>9} | {'NaN':>10} | {theory:6.1f} | âŒ ã‚¨ãƒ©ãƒ¼")
    
    # ç©¶æ¥µçµ±è¨ˆã®è¡¨ç¤º
    if 'overall_statistics' in ultimate_results:
        overall = ultimate_results['overall_statistics']
        print(f"\nğŸŒŒ ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼v7.0å…¨ä½“çµ±è¨ˆ:")
        print(f"å¹³å‡åæŸç‡: {overall['mean_convergence']:.10f}")
        print(f"æ¨™æº–åå·®: {overall['std_convergence']:.10f}")
        print(f"æˆåŠŸç‡ (|Re-1/2|<0.1): {overall['success_rate']:.2%}")
        print(f"é«˜ç²¾åº¦æˆåŠŸç‡ (|Re-1/2|<0.01): {overall['high_precision_success_rate']:.2%}")
        print(f"è¶…ç²¾å¯†æˆåŠŸç‡ (|Re-1/2|<1e-6): {overall['ultra_precision_success_rate']:.2%}")
        print(f"å®Œå…¨æˆåŠŸç‡ (|Re-1/2|<1e-10): {overall['perfect_success_rate']:.2%}")
        print(f"ç©¶æ¥µæˆåŠŸç‡ (|Re-1/2|<1e-12): {overall['ultimate_success_rate']:.2%}")
        print(f"ç¥ç´šæˆåŠŸç‡ (|Re-1/2|<1e-15): {overall['divine_success_rate']:.2%}")
        print(f"æœ€è‰¯åæŸ: {overall['min_convergence']:.10f}")
    
    print(f"\nâ±ï¸  ç©¶æ¥µæ¤œè¨¼æ™‚é–“: {verification_time:.2f}ç§’")
    
    # ç©¶æ¥µæˆæœã®åˆ†æ
    print(f"\nğŸŒŒ ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼v7.0ã®é©å‘½çš„æˆæœ:")
    mastery_success = sum(1 for i, gamma in enumerate(ultimate_gamma_values) 
                         if gamma in mastery_gammas and classifications[i] in ['å®Œå…¨æˆåŠŸ', 'ç©¶æ¥µæˆåŠŸ', 'ç¥ç´šæˆåŠŸ'])
    conquest_success = sum(1 for i, gamma in enumerate(ultimate_gamma_values) 
                          if gamma in conquest_gammas and classifications[i] in ['å®Œå…¨æˆåŠŸ', 'ç©¶æ¥µæˆåŠŸ', 'ç¥ç´šæˆåŠŸ', 'è¶…é«˜ç²¾åº¦æˆåŠŸ'])
    
    print(f"â€¢ ãƒã‚¹ã‚¿ãƒªãƒ¼ç¶™æ‰¿æˆåŠŸ: {mastery_success}/{len(mastery_gammas)}å€‹ï¼ˆ{mastery_success/len(mastery_gammas)*100:.1f}%ï¼‰")
    print(f"â€¢ æ–°è¦åˆ¶è¦‡æˆåŠŸ: {conquest_success}/{len(conquest_gammas)}å€‹ï¼ˆ{conquest_success/len(conquest_gammas)*100:.1f}%ï¼‰")
    print(f"â€¢ ç·åˆåˆ¶è¦‡ç‡: {(mastery_success + conquest_success)/len(ultimate_gamma_values)*100:.1f}%")
    print(f"â€¢ æ¤œè¨¼è¦æ¨¡: {len(ultimate_gamma_values)}å€‹ï¼ˆæ•°å­¦å²ä¸Šæœ€å¤§ï¼‰")
    
    # çµæœã®ä¿å­˜
    with open('ultimate_mastery_riemann_results.json', 'w', encoding='utf-8') as f:
        json.dump(ultimate_results, f, indent=2, ensure_ascii=False, default=str)
    
    print("ğŸ’¾ ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼v7.0çµæœã‚’ 'ultimate_mastery_riemann_results.json' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    return ultimate_results

if __name__ == "__main__":
    """ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼ãƒ»ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®å®Ÿè¡Œ"""
    try:
        results = demonstrate_ultimate_mastery()
        print("ğŸ‰ ç©¶æ¥µãƒã‚¹ã‚¿ãƒªãƒ¼v7.0æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸŒŒ NKATç†è«–ã®åˆ¶è¦‡é ˜åŸŸãŒæ•°å­¦å²ä¸Šæœ€å¤§è¦æ¨¡ã«æ‹¡å¤§")
        print("ğŸ‘‘ 25å€‹ã®Î³å€¤ã«ã‚ˆã‚‹å®Œå…¨åˆ¶è¦‡ã®æ–°æ™‚ä»£ã‚’é–‹æ‹“")
        print("ğŸ† äººé¡ã®æ•°å­¦çš„çŸ¥è­˜ã®æ¥µé™ã«æŒ‘æˆ¦")
        
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc() 
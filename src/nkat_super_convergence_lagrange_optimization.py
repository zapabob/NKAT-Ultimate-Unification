#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NKATè¶…åæŸå› å­ã®å°å‡ºã¨ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•ã«ã‚ˆã‚‹å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
Non-Commutative Kolmogorov-Arnold Theory: Super-Convergence Factor Derivation
and Lagrange Multiplier Optimization for Experimental Parameters

Author: å³¯å²¸ äº® (Ryo Minegishi)
Date: 2025å¹´5æœˆ28æ—¥
"""

import numpy as np
import matplotlib.pyplot as plt
import scipy.optimize as opt
from scipy.integrate import quad, solve_ivp
from scipy.special import gamma, zeta, polygamma
from scipy.linalg import eigvals, norm
import sympy as sp
from sympy import symbols, diff, solve, lambdify, exp, log, sqrt, pi, I
import pandas as pd
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class NKATSuperConvergenceFactor:
    """
    NKATç†è«–ã«ãŠã‘ã‚‹è¶…åæŸå› å­ã®å°å‡ºã¨è§£æã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.gamma_euler = 0.5772156649015329  # ã‚ªã‚¤ãƒ©ãƒ¼å®šæ•°
        self.hbar = 1.0  # ç°¡ç´„ãƒ—ãƒ©ãƒ³ã‚¯å®šæ•°ï¼ˆå˜ä½ç³»ï¼‰
        
        # è¶…åæŸå› å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆåˆæœŸæ¨å®šå€¤ï¼‰
        self.gamma = 0.23422  # ä¸»è¦åæŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.delta = 0.03511  # æŒ‡æ•°æ¸›è¡°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.t_c = 17.2644   # è‡¨ç•Œç‚¹
        self.alpha = 0.7422  # åæŸæŒ‡æ•°
        
        # é«˜æ¬¡è£œæ­£ä¿‚æ•°
        self.c_coeffs = [0.0628, 0.0035, 0.0012, 0.0004]  # c_2, c_3, c_4, c_5
        
        print("ğŸ”¬ NKATè¶…åæŸå› å­è§£æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“Š åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: Î³={self.gamma:.5f}, Î´={self.delta:.5f}, t_c={self.t_c:.4f}")
    
    def density_function(self, t, params=None):
        """
        èª¤å·®è£œæ­£å¯†åº¦é–¢æ•° Ï(t) ã®è¨ˆç®—
        
        Ï(t) = Î³/t + Î´Â·e^{-Î´(t-t_c)} + Î£_{k=2}^âˆ c_kÂ·kÂ·ln^{k-1}(t/t_c)/t^{k+1}
        """
        if params is None:
            gamma, delta, t_c = self.gamma, self.delta, self.t_c
            c_coeffs = self.c_coeffs
        else:
            gamma, delta, t_c = params[:3]
            c_coeffs = params[3:] if len(params) > 3 else self.c_coeffs
        
        # ä¸»è¦é …
        rho = gamma / t
        
        # æŒ‡æ•°æ¸›è¡°é …
        if t > t_c:
            rho += delta * np.exp(-delta * (t - t_c))
        
        # é«˜æ¬¡è£œæ­£é …
        if t > 1e-10:  # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚
            log_ratio = np.log(t / t_c) if t > t_c else 0
            for k, c_k in enumerate(c_coeffs, start=2):
                if abs(log_ratio) < 100:  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
                    correction = c_k * k * (log_ratio**(k-1)) / (t**(k+1))
                    rho += correction
        
        return rho
    
    def super_convergence_factor(self, N, params=None):
        """
        è¶…åæŸå› å­ S(N) ã®è¨ˆç®—
        
        S(N) = exp(âˆ«â‚^N Ï(t) dt)
        """
        try:
            def integrand(t):
                return self.density_function(t, params)
            
            integral, _ = quad(integrand, 1, N, limit=100)
            return np.exp(integral)
        except:
            # æ•°å€¤ç©åˆ†ãŒå¤±æ•—ã—ãŸå ´åˆã®è¿‘ä¼¼è¨ˆç®—
            return 1.0 + self.gamma * np.log(N / self.t_c)
    
    def theoretical_error_function(self, t, params=None):
        """
        ç†è«–çš„èª¤å·®é–¢æ•° E_t ã®è¨ˆç®—
        
        E_t = A/t + BÂ·e^{-Î´(t-t_c)} + Î£_{k=2}^âˆ D_k/t^kÂ·ln^k(t/t_c)
        """
        if params is None:
            gamma, delta, t_c = self.gamma, self.delta, self.t_c
        else:
            gamma, delta, t_c = params[:3]
        
        A = 1.0  # ä¸»è¦ä¿‚æ•°
        B = 0.1  # æŒ‡æ•°é …ä¿‚æ•°
        
        error = A / t
        
        if t > t_c:
            error += B * np.exp(-delta * (t - t_c))
        
        # å¯¾æ•°è£œæ­£é …
        if t > 1e-10:
            log_ratio = np.log(t / t_c) if t > t_c else 0
            for k in range(2, 5):
                D_k = 0.01 / k**2  # ä¿‚æ•°ã®æ¨å®š
                if abs(log_ratio) < 50:
                    error += D_k * (log_ratio**k) / (t**k)
        
        return error
    
    def variational_principle_objective(self, S_values, t_values, params):
        """
        å¤‰åˆ†åŸç†ã®ç›®çš„é–¢æ•°
        
        J[S] = âˆ«â‚^N [|S'(t)|Â²/S(t)Â² + V_eff(t)S(t)Â²] dt
        """
        gamma, delta, t_c = params[:3]
        
        # æ•°å€¤å¾®åˆ†ã§S'(t)ã‚’è¨ˆç®—
        S_prime = np.gradient(S_values, t_values)
        
        # æœ‰åŠ¹ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«
        V_eff = gamma**2 / t_values**2 + delta**2 * np.exp(-2*delta*(t_values - t_c))
        
        # ç›®çš„é–¢æ•°ã®è¨ˆç®—
        integrand = (S_prime**2) / (S_values**2) + V_eff * S_values**2
        
        return np.trapz(integrand, t_values)
    
    def quantum_mechanical_interpretation(self, N, params=None):
        """
        é‡å­åŠ›å­¦çš„è§£é‡ˆã«ã‚ˆã‚‹è¶…åæŸå› å­ã®è¨ˆç®—
        
        S(N) = âŸ¨Ïˆ_N|e^{-iHt}|Ïˆ_NâŸ©|_{t=T(N)}
        """
        if params is None:
            gamma, delta, t_c = params[:3] if params else (self.gamma, self.delta, self.t_c)
        else:
            gamma, delta, t_c = params[:3]
        
        # ç‰¹æ€§æ™‚é–“
        T_N = np.log(N / t_c)
        
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®å›ºæœ‰å€¤ï¼ˆç°¡ç•¥åŒ–ï¼‰
        eigenvalues = np.array([k * np.pi / (2*N + 1) for k in range(1, N+1)])
        
        # æ™‚é–“ç™ºå±•æ¼”ç®—å­ã®æœŸå¾…å€¤
        expectation = np.sum(np.exp(-1j * eigenvalues * T_N))
        
        return abs(expectation) / N
    
    def statistical_mechanical_partition_function(self, N, beta=1.0, params=None):
        """
        çµ±è¨ˆåŠ›å­¦çš„åˆ†é…é–¢æ•°ã«ã‚ˆã‚‹è¶…åæŸå› å­
        
        S(N) = Z_N/Z_classical = Tr(e^{-Î²H_N})/Tr(e^{-Î²H_classical})
        """
        if params is None:
            gamma, delta, t_c = params[:3] if params else (self.gamma, self.delta, self.t_c)
        else:
            gamma, delta, t_c = params[:3]
        
        # éå¯æ›ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®å›ºæœ‰å€¤
        H_nc_eigenvals = np.array([k * np.pi / (2*N + 1) + gamma/k for k in range(1, N+1)])
        
        # å¤å…¸ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®å›ºæœ‰å€¤
        H_classical_eigenvals = np.array([k * np.pi / (2*N + 1) for k in range(1, N+1)])
        
        # åˆ†é…é–¢æ•°ã®è¨ˆç®—
        Z_nc = np.sum(np.exp(-beta * H_nc_eigenvals))
        Z_classical = np.sum(np.exp(-beta * H_classical_eigenvals))
        
        return Z_nc / Z_classical
    
    def information_theoretic_relative_entropy(self, N, params=None):
        """
        æƒ…å ±ç†è«–çš„ç›¸å¯¾ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«ã‚ˆã‚‹è¶…åæŸå› å­
        
        S(N) = exp(-S_rel(Ï_Nâ€–Ï_classical))
        """
        if params is None:
            gamma, delta, t_c = params[:3] if params else (self.gamma, self.delta, self.t_c)
        else:
            gamma, delta, t_c = params[:3]
        
        # å¯†åº¦è¡Œåˆ—ã®å›ºæœ‰å€¤ï¼ˆæ­£è¦åŒ–ï¼‰
        rho_nc = np.array([1/N + gamma/(k*N) for k in range(1, N+1)])
        rho_nc = rho_nc / np.sum(rho_nc)
        
        rho_classical = np.ones(N) / N
        
        # ç›¸å¯¾ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®è¨ˆç®—
        S_rel = np.sum(rho_nc * np.log(rho_nc / rho_classical))
        
        return np.exp(-S_rel)

class LagrangeMultiplierOptimizer:
    """
    ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ã®æœªå®šä¹—æ•°æ³•ã«ã‚ˆã‚‹å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, nkat_system):
        """åˆæœŸåŒ–"""
        self.nkat = nkat_system
        self.experimental_data = None
        self.constraints = []
        
        print("ğŸ¯ ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def generate_experimental_data(self, N_values, noise_level=1e-6):
        """
        å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        """
        print("ğŸ“Š å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
        
        data = []
        for N in tqdm(N_values, desc="å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"):
            # ç†è«–å€¤
            S_theory = self.nkat.super_convergence_factor(N)
            
            # ãƒã‚¤ã‚ºä»˜åŠ 
            S_experimental = S_theory * (1 + np.random.normal(0, noise_level))
            
            # ä»–ã®è¦³æ¸¬é‡
            quantum_expectation = self.nkat.quantum_mechanical_interpretation(N)
            partition_ratio = self.nkat.statistical_mechanical_partition_function(N)
            entropy_factor = self.nkat.information_theoretic_relative_entropy(N)
            
            data.append({
                'N': N,
                'S_experimental': S_experimental,
                'S_theory': S_theory,
                'quantum_expectation': quantum_expectation,
                'partition_ratio': partition_ratio,
                'entropy_factor': entropy_factor,
                'error': abs(S_experimental - S_theory)
            })
        
        self.experimental_data = pd.DataFrame(data)
        print(f"âœ… {len(N_values)}ç‚¹ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")
        return self.experimental_data
    
    def define_constraints(self):
        """
        åˆ¶ç´„æ¡ä»¶ã®å®šç¾©
        """
        # ç‰©ç†çš„åˆ¶ç´„
        self.constraints = [
            # Î³ > 0 (æ­£ã®åæŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
            {'type': 'ineq', 'fun': lambda x: x[0]},
            
            # Î´ > 0 (æ­£ã®æ¸›è¡°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
            {'type': 'ineq', 'fun': lambda x: x[1]},
            
            # t_c > 1 (è‡¨ç•Œç‚¹ã¯1ã‚ˆã‚Šå¤§ãã„)
            {'type': 'ineq', 'fun': lambda x: x[2] - 1},
            
            # Î³ < 1 (åæŸæ¡ä»¶)
            {'type': 'ineq', 'fun': lambda x: 1 - x[0]},
            
            # Î´ < 0.1 (å®‰å®šæ€§æ¡ä»¶)
            {'type': 'ineq', 'fun': lambda x: 0.1 - x[1]},
            
            # æ­£è¦åŒ–æ¡ä»¶: âˆ«â‚^âˆ Ï(t) dt = Î³Â·ln(âˆ) (ç™ºæ•£ã™ã‚‹ãŒåˆ¶å¾¡)
            {'type': 'eq', 'fun': lambda x: self._normalization_constraint(x)},
        ]
        
        print(f"ğŸ“‹ {len(self.constraints)}å€‹ã®åˆ¶ç´„æ¡ä»¶ã‚’å®šç¾©")
    
    def _normalization_constraint(self, params):
        """æ­£è¦åŒ–åˆ¶ç´„æ¡ä»¶"""
        gamma, delta, t_c = params[:3]
        
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸæ­£è¦åŒ–æ¡ä»¶
        # å®Ÿéš›ã«ã¯æœ‰é™åŒºé–“ã§ã®ç©åˆ†ã§è¿‘ä¼¼
        try:
            integral, _ = quad(lambda t: self.nkat.density_function(t, params), 1, 100)
            target_value = gamma * np.log(100)  # ç†è«–çš„æœŸå¾…å€¤
            return integral - target_value
        except:
            return 0  # ç©åˆ†ãŒå¤±æ•—ã—ãŸå ´åˆ
    
    def objective_function(self, params):
        """
        ç›®çš„é–¢æ•°ï¼šå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã¨ã®æœ€å°äºŒä¹—èª¤å·®
        """
        if self.experimental_data is None:
            return float('inf')
        
        total_error = 0
        
        for _, row in self.experimental_data.iterrows():
            N = row['N']
            S_exp = row['S_experimental']
            
            # ç†è«–äºˆæ¸¬å€¤
            S_theory = self.nkat.super_convergence_factor(N, params)
            
            # äºŒä¹—èª¤å·®
            error = (S_theory - S_exp)**2
            total_error += error
        
        return total_error
    
    def lagrangian(self, params, lambdas):
        """
        ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ã‚¢ãƒ³é–¢æ•°
        
        L(x, Î») = f(x) + Î£áµ¢ Î»áµ¢ gáµ¢(x)
        """
        # ç›®çš„é–¢æ•°
        f_x = self.objective_function(params)
        
        # åˆ¶ç´„é …
        constraint_sum = 0
        for i, constraint in enumerate(self.constraints):
            if i < len(lambdas):
                if constraint['type'] == 'eq':
                    constraint_sum += lambdas[i] * constraint['fun'](params)
                elif constraint['type'] == 'ineq':
                    constraint_sum += lambdas[i] * max(0, -constraint['fun'](params))
        
        return f_x + constraint_sum
    
    def optimize_parameters(self, initial_guess=None):
        """
        ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
        """
        if initial_guess is None:
            initial_guess = [self.nkat.gamma, self.nkat.delta, self.nkat.t_c]
        
        print("ğŸ”§ ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•ã«ã‚ˆã‚‹æœ€é©åŒ–é–‹å§‹...")
        
        # åˆ¶ç´„æ¡ä»¶ã®å®šç¾©
        self.define_constraints()
        
        # æœ€é©åŒ–å®Ÿè¡Œ
        result = opt.minimize(
            self.objective_function,
            initial_guess,
            method='SLSQP',
            constraints=self.constraints,
            options={'disp': True, 'maxiter': 1000}
        )
        
        if result.success:
            optimal_params = result.x
            print("âœ… æœ€é©åŒ–æˆåŠŸ!")
            print(f"ğŸ“Š æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
            print(f"   Î³ = {optimal_params[0]:.6f}")
            print(f"   Î´ = {optimal_params[1]:.6f}")
            print(f"   t_c = {optimal_params[2]:.6f}")
            print(f"ğŸ“ˆ æœ€å°ç›®çš„é–¢æ•°å€¤: {result.fun:.2e}")
            
            return optimal_params, result
        else:
            print("âŒ æœ€é©åŒ–å¤±æ•—")
            print(f"ç†ç”±: {result.message}")
            return None, result
    
    def sensitivity_analysis(self, optimal_params, perturbation=0.01):
        """
        æ„Ÿåº¦è§£æï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¾®å°å¤‰åŒ–ã«å¯¾ã™ã‚‹ç›®çš„é–¢æ•°ã®å¤‰åŒ–
        """
        print("ğŸ” æ„Ÿåº¦è§£æå®Ÿè¡Œä¸­...")
        
        base_value = self.objective_function(optimal_params)
        sensitivities = []
        
        for i, param_name in enumerate(['Î³', 'Î´', 't_c']):
            # æ­£ã®æ‘‚å‹•
            params_plus = optimal_params.copy()
            params_plus[i] += perturbation
            value_plus = self.objective_function(params_plus)
            
            # è² ã®æ‘‚å‹•
            params_minus = optimal_params.copy()
            params_minus[i] -= perturbation
            value_minus = self.objective_function(params_minus)
            
            # æ•°å€¤å¾®åˆ†
            sensitivity = (value_plus - value_minus) / (2 * perturbation)
            sensitivities.append(sensitivity)
            
            print(f"ğŸ“Š {param_name}ã®æ„Ÿåº¦: {sensitivity:.2e}")
        
        return sensitivities
    
    def uncertainty_quantification(self, optimal_params, n_bootstrap=100):
        """
        ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ï¼šãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³•ã«ã‚ˆã‚‹ä¿¡é ¼åŒºé–“æ¨å®š
        """
        print("ğŸ“Š ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–å®Ÿè¡Œä¸­...")
        
        bootstrap_results = []
        
        for _ in tqdm(range(n_bootstrap), desc="ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—"):
            # ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            resampled_data = self.experimental_data.sample(
                n=len(self.experimental_data), 
                replace=True
            )
            
            # ä¸€æ™‚çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’ç½®ãæ›ãˆ
            original_data = self.experimental_data
            self.experimental_data = resampled_data
            
            # æœ€é©åŒ–å®Ÿè¡Œ
            try:
                result = opt.minimize(
                    self.objective_function,
                    optimal_params,
                    method='SLSQP',
                    constraints=self.constraints,
                    options={'disp': False}
                )
                
                if result.success:
                    bootstrap_results.append(result.x)
            except:
                pass
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«æˆ»ã™
            self.experimental_data = original_data
        
        if bootstrap_results:
            bootstrap_results = np.array(bootstrap_results)
            
            # ä¿¡é ¼åŒºé–“ã®è¨ˆç®—
            confidence_intervals = []
            param_names = ['Î³', 'Î´', 't_c']
            
            for i, name in enumerate(param_names):
                mean_val = np.mean(bootstrap_results[:, i])
                std_val = np.std(bootstrap_results[:, i])
                ci_lower = np.percentile(bootstrap_results[:, i], 2.5)
                ci_upper = np.percentile(bootstrap_results[:, i], 97.5)
                
                confidence_intervals.append({
                    'parameter': name,
                    'mean': mean_val,
                    'std': std_val,
                    'ci_lower': ci_lower,
                    'ci_upper': ci_upper
                })
                
                print(f"ğŸ“Š {name}: {mean_val:.6f} Â± {std_val:.6f} "
                      f"[{ci_lower:.6f}, {ci_upper:.6f}]")
        
        return confidence_intervals

class NKATVisualization:
    """
    NKATç†è«–ã®å¯è¦–åŒ–ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, nkat_system, optimizer):
        """åˆæœŸåŒ–"""
        self.nkat = nkat_system
        self.optimizer = optimizer
        
    def plot_super_convergence_factor(self, N_range, params=None):
        """è¶…åæŸå› å­ã®ãƒ—ãƒ­ãƒƒãƒˆ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        N_values = np.logspace(1, 3, 100)
        S_values = [self.nkat.super_convergence_factor(N, params) for N in N_values]
        
        # 1. è¶…åæŸå› å­ã®æŒ™å‹•
        axes[0, 0].loglog(N_values, S_values, 'b-', linewidth=2, label='S(N)')
        axes[0, 0].set_xlabel('æ¬¡å…ƒæ•° N')
        axes[0, 0].set_ylabel('è¶…åæŸå› å­ S(N)')
        axes[0, 0].set_title('è¶…åæŸå› å­ã®æ¬¡å…ƒä¾å­˜æ€§')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].legend()
        
        # 2. å¯†åº¦é–¢æ•°
        t_values = np.linspace(1, 50, 1000)
        rho_values = [self.nkat.density_function(t, params) for t in t_values]
        
        axes[0, 1].plot(t_values, rho_values, 'r-', linewidth=2)
        axes[0, 1].set_xlabel('t')
        axes[0, 1].set_ylabel('å¯†åº¦é–¢æ•° Ï(t)')
        axes[0, 1].set_title('èª¤å·®è£œæ­£å¯†åº¦é–¢æ•°')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ç†è«–çš„èª¤å·®é–¢æ•°
        error_values = [self.nkat.theoretical_error_function(t, params) for t in t_values]
        
        axes[1, 0].semilogy(t_values, error_values, 'g-', linewidth=2)
        axes[1, 0].set_xlabel('t')
        axes[1, 0].set_ylabel('èª¤å·®é–¢æ•° E(t)')
        axes[1, 0].set_title('ç†è«–çš„èª¤å·®é–¢æ•°')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. é‡å­åŠ›å­¦çš„è§£é‡ˆ
        quantum_values = [self.nkat.quantum_mechanical_interpretation(int(N), params) 
                         for N in N_values[::10]]
        
        axes[1, 1].plot(N_values[::10], quantum_values, 'mo-', linewidth=2, markersize=4)
        axes[1, 1].set_xlabel('æ¬¡å…ƒæ•° N')
        axes[1, 1].set_ylabel('é‡å­æœŸå¾…å€¤')
        axes[1, 1].set_title('é‡å­åŠ›å­¦çš„è§£é‡ˆ')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('nkat_super_convergence_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def plot_optimization_results(self, experimental_data, optimal_params):
        """æœ€é©åŒ–çµæœã®ãƒ—ãƒ­ãƒƒãƒˆ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        N_values = experimental_data['N'].values
        S_exp = experimental_data['S_experimental'].values
        S_theory_original = experimental_data['S_theory'].values
        
        # æœ€é©åŒ–å¾Œã®ç†è«–å€¤
        S_theory_optimized = [self.nkat.super_convergence_factor(N, optimal_params) 
                             for N in N_values]
        
        # 1. å®Ÿé¨“å€¤vsç†è«–å€¤ï¼ˆæœ€é©åŒ–å‰å¾Œï¼‰
        axes[0, 0].plot(N_values, S_exp, 'ro', label='å®Ÿé¨“å€¤', markersize=6)
        axes[0, 0].plot(N_values, S_theory_original, 'b--', label='æœ€é©åŒ–å‰', linewidth=2)
        axes[0, 0].plot(N_values, S_theory_optimized, 'g-', label='æœ€é©åŒ–å¾Œ', linewidth=2)
        axes[0, 0].set_xlabel('æ¬¡å…ƒæ•° N')
        axes[0, 0].set_ylabel('è¶…åæŸå› å­')
        axes[0, 0].set_title('å®Ÿé¨“å€¤ã¨ç†è«–å€¤ã®æ¯”è¼ƒ')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
        residuals_original = S_exp - S_theory_original
        residuals_optimized = S_exp - S_theory_optimized
        
        axes[0, 1].plot(N_values, residuals_original, 'b^', label='æœ€é©åŒ–å‰', markersize=6)
        axes[0, 1].plot(N_values, residuals_optimized, 'go', label='æœ€é©åŒ–å¾Œ', markersize=6)
        axes[0, 1].axhline(y=0, color='k', linestyle='-', alpha=0.5)
        axes[0, 1].set_xlabel('æ¬¡å…ƒæ•° N')
        axes[0, 1].set_ylabel('æ®‹å·®')
        axes[0, 1].set_title('æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. ç›¸é–¢ãƒ—ãƒ­ãƒƒãƒˆ
        axes[1, 0].scatter(S_exp, S_theory_optimized, alpha=0.7, s=50)
        min_val = min(min(S_exp), min(S_theory_optimized))
        max_val = max(max(S_exp), max(S_theory_optimized))
        axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)
        axes[1, 0].set_xlabel('å®Ÿé¨“å€¤')
        axes[1, 0].set_ylabel('ç†è«–å€¤ï¼ˆæœ€é©åŒ–å¾Œï¼‰')
        axes[1, 0].set_title('å®Ÿé¨“å€¤vsç†è«–å€¤ç›¸é–¢')
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. èª¤å·®ã®æ”¹å–„
        error_original = np.abs(residuals_original)
        error_optimized = np.abs(residuals_optimized)
        
        axes[1, 1].semilogy(N_values, error_original, 'b^-', label='æœ€é©åŒ–å‰', linewidth=2)
        axes[1, 1].semilogy(N_values, error_optimized, 'go-', label='æœ€é©åŒ–å¾Œ', linewidth=2)
        axes[1, 1].set_xlabel('æ¬¡å…ƒæ•° N')
        axes[1, 1].set_ylabel('çµ¶å¯¾èª¤å·®')
        axes[1, 1].set_title('èª¤å·®ã®æ”¹å–„')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('nkat_optimization_results.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
        print("\nğŸ“Š æœ€é©åŒ–çµæœçµ±è¨ˆ:")
        print(f"æœ€é©åŒ–å‰ RMSE: {np.sqrt(np.mean(residuals_original**2)):.2e}")
        print(f"æœ€é©åŒ–å¾Œ RMSE: {np.sqrt(np.mean(residuals_optimized**2)):.2e}")
        print(f"æ”¹å–„ç‡: {(1 - np.sqrt(np.mean(residuals_optimized**2))/np.sqrt(np.mean(residuals_original**2)))*100:.1f}%")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ NKATè¶…åæŸå› å­è§£æãƒ»ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("=" * 60)
    
    # 1. ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    nkat = NKATSuperConvergenceFactor()
    optimizer = LagrangeMultiplierOptimizer(nkat)
    visualizer = NKATVisualization(nkat, optimizer)
    
    # 2. å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    N_values = np.array([50, 100, 200, 300, 500, 750, 1000])
    experimental_data = optimizer.generate_experimental_data(N_values, noise_level=1e-5)
    
    print("\nğŸ“Š å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼:")
    print(experimental_data.describe())
    
    # 3. åˆæœŸçŠ¶æ…‹ã®å¯è¦–åŒ–
    print("\nğŸ“ˆ åˆæœŸçŠ¶æ…‹ã®å¯è¦–åŒ–...")
    visualizer.plot_super_convergence_factor(N_values)
    
    # 4. ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•ã«ã‚ˆã‚‹æœ€é©åŒ–
    print("\nğŸ¯ ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥æœªå®šä¹—æ•°æ³•ã«ã‚ˆã‚‹æœ€é©åŒ–å®Ÿè¡Œ...")
    optimal_params, optimization_result = optimizer.optimize_parameters()
    
    if optimal_params is not None:
        # 5. æœ€é©åŒ–çµæœã®å¯è¦–åŒ–
        print("\nğŸ“Š æœ€é©åŒ–çµæœã®å¯è¦–åŒ–...")
        visualizer.plot_optimization_results(experimental_data, optimal_params)
        
        # 6. æ„Ÿåº¦è§£æ
        print("\nğŸ” æ„Ÿåº¦è§£æå®Ÿè¡Œ...")
        sensitivities = optimizer.sensitivity_analysis(optimal_params)
        
        # 7. ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–
        print("\nğŸ“Š ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–å®Ÿè¡Œ...")
        confidence_intervals = optimizer.uncertainty_quantification(optimal_params, n_bootstrap=50)
        
        # 8. æœ€çµ‚çµæœã®è¡¨ç¤º
        print("\n" + "=" * 60)
        print("ğŸ‰ NKATè¶…åæŸå› å­è§£æå®Œäº†")
        print("=" * 60)
        
        print("\nğŸ“Š æœ€çµ‚æœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
        param_names = ['Î³ (ä¸»è¦åæŸ)', 'Î´ (æŒ‡æ•°æ¸›è¡°)', 't_c (è‡¨ç•Œç‚¹)']
        for i, (name, value) in enumerate(zip(param_names, optimal_params)):
            print(f"   {name}: {value:.8f}")
        
        print(f"\nğŸ“ˆ æœ€é©åŒ–æ€§èƒ½:")
        print(f"   ç›®çš„é–¢æ•°å€¤: {optimization_result.fun:.2e}")
        print(f"   åå¾©å›æ•°: {optimization_result.nit}")
        print(f"   åæŸçŠ¶æ³: {'æˆåŠŸ' if optimization_result.success else 'å¤±æ•—'}")
        
        # 9. ç†è«–çš„æ¤œè¨¼
        print("\nğŸ”¬ ç†è«–çš„æ¤œè¨¼:")
        N_test = 1000
        S_optimized = nkat.super_convergence_factor(N_test, optimal_params)
        S_quantum = nkat.quantum_mechanical_interpretation(N_test, optimal_params)
        S_statistical = nkat.statistical_mechanical_partition_function(N_test, params=optimal_params)
        S_information = nkat.information_theoretic_relative_entropy(N_test, optimal_params)
        
        print(f"   N={N_test}ã§ã®è¶…åæŸå› å­: {S_optimized:.8f}")
        print(f"   é‡å­åŠ›å­¦çš„è§£é‡ˆ: {S_quantum:.8f}")
        print(f"   çµ±è¨ˆåŠ›å­¦çš„è§£é‡ˆ: {S_statistical:.8f}")
        print(f"   æƒ…å ±ç†è«–çš„è§£é‡ˆ: {S_information:.8f}")
        
        # 10. ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¸ã®å«æ„
        print("\nğŸ¯ ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã¸ã®å«æ„:")
        convergence_rate = optimal_params[0] * np.log(N_test / optimal_params[2])
        print(f"   åæŸç‡ Î³Â·ln(N/t_c): {convergence_rate:.8f}")
        print(f"   è‡¨ç•Œç·šåæŸæ¡ä»¶: {'æº€è¶³' if abs(convergence_rate - 0.5) < 0.1 else 'è¦æ¤œè¨'}")
        
        # 11. çµæœä¿å­˜
        results_summary = {
            'optimal_parameters': {
                'gamma': optimal_params[0],
                'delta': optimal_params[1],
                't_c': optimal_params[2]
            },
            'optimization_info': {
                'objective_value': float(optimization_result.fun),
                'iterations': int(optimization_result.nit),
                'success': bool(optimization_result.success)
            },
            'theoretical_verification': {
                'super_convergence_factor': float(S_optimized),
                'quantum_interpretation': float(S_quantum),
                'statistical_interpretation': float(S_statistical),
                'information_interpretation': float(S_information)
            }
        }
        
        # JSONå½¢å¼ã§ä¿å­˜
        import json
        with open('nkat_optimization_results.json', 'w', encoding='utf-8') as f:
            json.dump(results_summary, f, indent=2, ensure_ascii=False)
        
        print("\nğŸ’¾ çµæœã‚’nkat_optimization_results.jsonã«ä¿å­˜ã—ã¾ã—ãŸ")
        
    else:
        print("âŒ æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    print("\nğŸ è§£æå®Œäº†")

if __name__ == "__main__":
    main() 
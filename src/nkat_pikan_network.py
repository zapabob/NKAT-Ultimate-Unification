#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ NKATç†è«–çµ±åˆPI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
Physics-Informed Kolmogorov-Arnold Network with NKAT Theory

Author: NKAT Research Team
Date: 2025-05-24
Version: 1.0 - Deep Learning Integration
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Any, Callable
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass
from tqdm import tqdm, trange
import logging
from collections import defaultdict
import math

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

@dataclass
class PIKANConfig:
    """PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š"""
    input_dim: int = 4  # æ™‚ç©ºæ¬¡å…ƒ
    hidden_dims: List[int] = None
    output_dim: int = 1
    num_basis_functions: int = 10
    theta_parameter: float = 1e-25
    kappa_parameter: float = 1e-15
    physics_weight: float = 1.0
    learning_rate: float = 1e-3
    batch_size: int = 64
    num_epochs: int = 1000
    
    def __post_init__(self):
        if self.hidden_dims is None:
            self.hidden_dims = [64, 128, 64]

class NKATBasisFunction(nn.Module):
    """
    NKATç†è«–ã«åŸºã¥ãåŸºåº•é–¢æ•°
    
    éå¯æ›æ™‚ç©ºæ§‹é€ ã‚’åæ˜ ã—ãŸåŸºåº•é–¢æ•°ã®å®Ÿè£…
    """
    
    def __init__(self, input_dim: int, num_basis: int, theta: float, kappa: float):
        super().__init__()
        self.input_dim = input_dim
        self.num_basis = num_basis
        self.theta = theta
        self.kappa = kappa
        
        # åŸºåº•é–¢æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.centers = nn.Parameter(torch.randn(num_basis, input_dim))
        self.scales = nn.Parameter(torch.ones(num_basis, input_dim))
        self.weights = nn.Parameter(torch.randn(num_basis))
        
        # NKATç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.theta_tensor = nn.Parameter(torch.tensor(theta, dtype=torch.float32))
        self.kappa_tensor = nn.Parameter(torch.tensor(kappa, dtype=torch.float32))
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        NKATä¿®æ­£ã‚’å«ã‚€åŸºåº•é–¢æ•°ã®è¨ˆç®—
        
        Args:
            x: å…¥åŠ›åº§æ¨™ [batch_size, input_dim]
        
        Returns:
            åŸºåº•é–¢æ•°ã®å€¤ [batch_size, num_basis]
        """
        batch_size = x.shape[0]
        
        # æ¨™æº–çš„ãªã‚¬ã‚¦ã‚¹åŸºåº•é–¢æ•°
        diff = x.unsqueeze(1) - self.centers.unsqueeze(0)  # [batch_size, num_basis, input_dim]
        scaled_diff = diff / (self.scales.unsqueeze(0) + 1e-8)
        gaussian = torch.exp(-0.5 * torch.sum(scaled_diff**2, dim=2))  # [batch_size, num_basis]
        
        # NKATç†è«–ã«ã‚ˆã‚‹ä¿®æ­£
        # Î¸-å¤‰å½¢ï¼šéå¯æ›æ€§ã«ã‚ˆã‚‹è£œæ­£
        if self.input_dim >= 2:
            # [x, y] = iÎ¸ ã®åŠ¹æœ
            x_coord = x[:, 0].unsqueeze(1)  # [batch_size, 1]
            y_coord = x[:, 1].unsqueeze(1) if self.input_dim > 1 else torch.zeros_like(x_coord)
            
            theta_correction = self.theta_tensor * x_coord * y_coord
            theta_modification = torch.exp(theta_correction)  # [batch_size, 1]
            
            gaussian = gaussian * theta_modification
        
        # Îº-å¤‰å½¢ï¼šMinkowskiæ™‚ç©ºã®å¤‰å½¢
        if self.input_dim >= 4:
            # æ™‚é–“åº§æ¨™ï¼ˆé€šå¸¸ã¯æœ€åˆã®åº§æ¨™ï¼‰
            t_coord = x[:, 0].unsqueeze(1)
            
            # ç©ºé–“åº§æ¨™ã®äºŒä¹—å’Œ
            spatial_coords = x[:, 1:]
            spatial_norm_sq = torch.sum(spatial_coords**2, dim=1, keepdim=True)
            
            # Minkowskiè¨ˆé‡ã®ä¿®æ­£
            kappa_correction = self.kappa_tensor * (t_coord**2 - spatial_norm_sq)
            kappa_modification = torch.exp(kappa_correction)  # [batch_size, 1]
            
            gaussian = gaussian * kappa_modification
        
        # é‡ã¿ä»˜ãå’Œ
        output = gaussian * self.weights.unsqueeze(0)  # [batch_size, num_basis]
        
        return output

class KolmogorovArnoldLayer(nn.Module):
    """
    Kolmogorov-Arnoldè¡¨ç¾ã«åŸºã¥ãå±¤
    
    f(x) = Î£_i Ï†_i(Î£_j Ïˆ_{i,j}(x_j))
    """
    
    def __init__(self, input_dim: int, output_dim: int, num_basis: int, 
                 theta: float, kappa: float):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.num_basis = num_basis
        
        # å†…å´ã®é–¢æ•° Ïˆ_{i,j}(x_j)
        self.inner_functions = nn.ModuleList([
            NKATBasisFunction(1, num_basis, theta, kappa) 
            for _ in range(input_dim)
        ])
        
        # å¤–å´ã®é–¢æ•° Ï†_i
        self.outer_function = NKATBasisFunction(input_dim * num_basis, output_dim, theta, kappa)
        
        # ç·šå½¢å¤‰æ›
        self.linear = nn.Linear(output_dim * self.outer_function.num_basis, output_dim)
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Kolmogorov-Arnoldè¡¨ç¾ã®è¨ˆç®—
        """
        batch_size = x.shape[0]
        
        # å„å…¥åŠ›æ¬¡å…ƒã«å¯¾ã—ã¦å†…å´ã®é–¢æ•°ã‚’é©ç”¨
        inner_outputs = []
        for i in range(self.input_dim):
            x_i = x[:, i:i+1]  # [batch_size, 1]
            inner_out = self.inner_functions[i](x_i)  # [batch_size, num_basis]
            inner_outputs.append(inner_out)
        
        # å†…å´ã®é–¢æ•°ã®å‡ºåŠ›ã‚’çµåˆ
        combined_inner = torch.cat(inner_outputs, dim=1)  # [batch_size, input_dim * num_basis]
        
        # å¤–å´ã®é–¢æ•°ã‚’é©ç”¨
        outer_output = self.outer_function(combined_inner)  # [batch_size, output_dim * num_basis]
        
        # ç·šå½¢å¤‰æ›ã§æœ€çµ‚å‡ºåŠ›
        output = self.linear(outer_output)  # [batch_size, output_dim]
        
        return output

class PIKANNetwork(nn.Module):
    """
    Physics-Informed Kolmogorov-Arnold Network
    
    NKATç†è«–ã®ç‰©ç†æ³•å‰‡ã‚’çµ„ã¿è¾¼ã‚“ã KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    """
    
    def __init__(self, config: PIKANConfig):
        super().__init__()
        self.config = config
        
        # KANãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æ§‹ç¯‰
        self.kan_layers = nn.ModuleList()
        
        # å…¥åŠ›å±¤
        current_dim = config.input_dim
        for hidden_dim in config.hidden_dims:
            layer = KolmogorovArnoldLayer(
                current_dim, hidden_dim, config.num_basis_functions,
                config.theta_parameter, config.kappa_parameter
            )
            self.kan_layers.append(layer)
            current_dim = hidden_dim
        
        # å‡ºåŠ›å±¤
        output_layer = KolmogorovArnoldLayer(
            current_dim, config.output_dim, config.num_basis_functions,
            config.theta_parameter, config.kappa_parameter
        )
        self.kan_layers.append(output_layer)
        
        # ç‰©ç†åˆ¶ç´„é …ã®é‡ã¿
        self.physics_weight = config.physics_weight
        
        logger.info(f"ğŸ§  PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–å®Œäº†: {len(self.kan_layers)}å±¤")
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é †ä¼æ’­
        """
        for layer in self.kan_layers:
            x = layer(x)
            x = F.relu(x)  # æ´»æ€§åŒ–é–¢æ•°
        
        return x
    
    def compute_physics_loss(self, x: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:
        """
        NKATç†è«–ã«åŸºã¥ãç‰©ç†åˆ¶ç´„æå¤±ã®è¨ˆç®—
        """
        batch_size = x.shape[0]
        
        # å‹¾é…ã®è¨ˆç®—ï¼ˆè‡ªå‹•å¾®åˆ†ï¼‰
        x.requires_grad_(True)
        y = self.forward(x)
        
        # 1éšå¾®åˆ†
        grad_outputs = torch.ones_like(y)
        gradients = torch.autograd.grad(
            outputs=y, inputs=x, grad_outputs=grad_outputs,
            create_graph=True, retain_graph=True
        )[0]
        
        # 2éšå¾®åˆ†ï¼ˆãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ï¼‰
        laplacian = 0
        for i in range(x.shape[1]):
            grad_i = gradients[:, i]
            grad2_i = torch.autograd.grad(
                outputs=grad_i, inputs=x, grad_outputs=torch.ones_like(grad_i),
                create_graph=True, retain_graph=True
            )[0][:, i]
            laplacian += grad2_i
        
        # NKATç†è«–ã«ã‚ˆã‚‹ä¿®æ­£é …
        theta = self.config.theta_parameter
        kappa = self.config.kappa_parameter
        
        # Î¸-å¤‰å½¢ã«ã‚ˆã‚‹éå¯æ›è£œæ­£
        if x.shape[1] >= 2:
            x_coord = x[:, 0]
            y_coord = x[:, 1]
            theta_term = theta * (x_coord * gradients[:, 1] - y_coord * gradients[:, 0])
        else:
            theta_term = torch.zeros(batch_size, device=x.device)
        
        # Îº-å¤‰å½¢ã«ã‚ˆã‚‹Minkowskiè£œæ­£
        if x.shape[1] >= 4:
            t_coord = x[:, 0]
            spatial_coords = x[:, 1:]
            spatial_laplacian = torch.sum(gradients[:, 1:]**2, dim=1)
            kappa_term = kappa * (gradients[:, 0]**2 - spatial_laplacian)
        else:
            kappa_term = torch.zeros(batch_size, device=x.device)
        
        # ä¿®æ­£ã•ã‚ŒãŸPDE: (âˆ‡Â² + Î¸-é … + Îº-é …)u = 0
        pde_residual = laplacian + theta_term + kappa_term
        
        # ç‰©ç†åˆ¶ç´„æå¤±
        physics_loss = torch.mean(pde_residual**2)
        
        return physics_loss
    
    def compute_total_loss(self, x: torch.Tensor, y_true: torch.Tensor, 
                          y_pred: torch.Tensor) -> Tuple[torch.Tensor, Dict]:
        """
        ç·æå¤±ã®è¨ˆç®—ï¼ˆãƒ‡ãƒ¼ã‚¿æå¤± + ç‰©ç†åˆ¶ç´„æå¤±ï¼‰
        """
        # ãƒ‡ãƒ¼ã‚¿æå¤±ï¼ˆMSEï¼‰
        data_loss = F.mse_loss(y_pred, y_true)
        
        # ç‰©ç†åˆ¶ç´„æå¤±
        physics_loss = self.compute_physics_loss(x, y_pred)
        
        # ç·æå¤±
        total_loss = data_loss + self.physics_weight * physics_loss
        
        loss_dict = {
            'total_loss': total_loss.item(),
            'data_loss': data_loss.item(),
            'physics_loss': physics_loss.item()
        }
        
        return total_loss, loss_dict

class NKATDataGenerator:
    """
    NKATç†è«–ã«åŸºã¥ãè¨“ç·´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå™¨
    """
    
    def __init__(self, config: PIKANConfig):
        self.config = config
        self.theta = config.theta_parameter
        self.kappa = config.kappa_parameter
        
    def generate_analytical_solution(self, x: torch.Tensor) -> torch.Tensor:
        """
        NKATç†è«–ã®è§£æè§£ï¼ˆè¿‘ä¼¼ï¼‰
        """
        # ç°¡å˜ãªè§£æè§£ã®ä¾‹ï¼šä¿®æ­£ã•ã‚ŒãŸèª¿å’ŒæŒ¯å‹•å­
        if x.shape[1] >= 2:
            x_coord = x[:, 0]
            y_coord = x[:, 1]
            
            # æ¨™æº–çš„ãªè§£
            standard_solution = torch.exp(-(x_coord**2 + y_coord**2) / 2)
            
            # NKATä¿®æ­£
            theta_correction = self.theta * x_coord * y_coord
            kappa_correction = self.kappa * (x_coord**2 - y_coord**2)
            
            modified_solution = standard_solution * torch.exp(theta_correction + kappa_correction)
            
            return modified_solution.unsqueeze(1)
        else:
            # 1æ¬¡å…ƒã®å ´åˆ
            x_coord = x[:, 0]
            solution = torch.exp(-x_coord**2 / 2) * (1 + self.theta * x_coord + self.kappa * x_coord**2)
            return solution.unsqueeze(1)
    
    def generate_training_data(self, num_samples: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        """
        # ãƒ©ãƒ³ãƒ€ãƒ ãªå…¥åŠ›ç‚¹ã®ç”Ÿæˆ
        x = torch.randn(num_samples, self.config.input_dim, device=device) * 2
        
        # å¯¾å¿œã™ã‚‹è§£æè§£
        y = self.generate_analytical_solution(x)
        
        return x, y

def train_pikan_network(config: PIKANConfig) -> Tuple[PIKANNetwork, Dict]:
    """
    PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è¨“ç·´
    """
    logger.info("ğŸš€ PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨“ç·´é–‹å§‹...")
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆæœŸåŒ–
    network = PIKANNetwork(config).to(device)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
    optimizer = torch.optim.Adam(network.parameters(), lr=config.learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.8)
    
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå™¨
    data_generator = NKATDataGenerator(config)
    
    # è¨“ç·´å±¥æ­´
    history = {
        'total_loss': [],
        'data_loss': [],
        'physics_loss': [],
        'learning_rate': []
    }
    
    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    network.train()
    for epoch in tqdm(range(config.num_epochs), desc="PI-KANè¨“ç·´"):
        # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
        x_batch, y_batch = data_generator.generate_training_data(config.batch_size)
        
        # é †ä¼æ’­
        y_pred = network(x_batch)
        
        # æå¤±è¨ˆç®—
        total_loss, loss_dict = network.compute_total_loss(x_batch, y_batch, y_pred)
        
        # é€†ä¼æ’­
        optimizer.zero_grad()
        total_loss.backward()
        
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        torch.nn.utils.clip_grad_norm_(network.parameters(), max_norm=1.0)
        
        optimizer.step()
        scheduler.step(total_loss)
        
        # å±¥æ­´ã®è¨˜éŒ²
        for key, value in loss_dict.items():
            history[key].append(value)
        history['learning_rate'].append(optimizer.param_groups[0]['lr'])
        
        # ãƒ­ã‚°å‡ºåŠ›
        if epoch % 100 == 0:
            logger.info(f"Epoch {epoch}: Total Loss = {total_loss:.6f}, "
                       f"Data Loss = {loss_dict['data_loss']:.6f}, "
                       f"Physics Loss = {loss_dict['physics_loss']:.6f}")
    
    logger.info("âœ… PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨“ç·´å®Œäº†")
    return network, history

def evaluate_pikan_network(network: PIKANNetwork, config: PIKANConfig) -> Dict:
    """
    PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è©•ä¾¡
    """
    logger.info("ğŸ“Š PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è©•ä¾¡é–‹å§‹...")
    
    network.eval()
    data_generator = NKATDataGenerator(config)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    x_test, y_true = data_generator.generate_training_data(1000)
    
    with torch.no_grad():
        y_pred = network(x_test)
        
        # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        mse = F.mse_loss(y_pred, y_true).item()
        mae = F.l1_loss(y_pred, y_true).item()
        
        # ç›¸é–¢ä¿‚æ•°
        y_true_np = y_true.cpu().numpy().flatten()
        y_pred_np = y_pred.cpu().numpy().flatten()
        correlation = np.corrcoef(y_true_np, y_pred_np)[0, 1]
        
        # ç‰©ç†åˆ¶ç´„ã®æº€è¶³åº¦
        physics_loss = network.compute_physics_loss(x_test, y_pred).item()
    
    evaluation_results = {
        'mse': mse,
        'mae': mae,
        'correlation': correlation,
        'physics_constraint_violation': physics_loss,
        'test_samples': len(x_test)
    }
    
    logger.info(f"ğŸ“Š è©•ä¾¡çµæœ: MSE={mse:.6f}, MAE={mae:.6f}, "
               f"ç›¸é–¢={correlation:.4f}, ç‰©ç†åˆ¶ç´„é•å={physics_loss:.6f}")
    
    return evaluation_results

def demonstrate_pikan_applications():
    """
    PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¿œç”¨ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("=" * 80)
    print("ğŸ¯ NKATç†è«–çµ±åˆPI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
    print("=" * 80)
    print("ğŸ“… å®Ÿè¡Œæ—¥æ™‚:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("ğŸ§  æ·±å±¤å­¦ç¿’: Physics-Informed Kolmogorov-Arnold Network")
    print("ğŸ”¬ ç‰©ç†ç†è«–: NKAT (Non-commutative Kappa-deformed Algebra Theory)")
    print("=" * 80)
    
    all_results = {}
    
    # 1. 2æ¬¡å…ƒNKATå•é¡Œ
    print("\nğŸ” 1. 2æ¬¡å…ƒNKATå•é¡Œã®è§£æ±º")
    print("å•é¡Œï¼šéå¯æ›æ™‚ç©ºã§ã®å ´ã®æ–¹ç¨‹å¼")
    
    config_2d = PIKANConfig(
        input_dim=2,
        hidden_dims=[32, 64, 32],
        output_dim=1,
        num_basis_functions=8,
        theta_parameter=1e-3,  # å¯è¦–åŒ–ã®ãŸã‚å¤§ãã‚ã®å€¤
        kappa_parameter=1e-2,
        physics_weight=0.1,
        learning_rate=1e-3,
        batch_size=64,
        num_epochs=500
    )
    
    # è¨“ç·´
    network_2d, history_2d = train_pikan_network(config_2d)
    
    # è©•ä¾¡
    eval_results_2d = evaluate_pikan_network(network_2d, config_2d)
    
    print(f"âœ… 2æ¬¡å…ƒå•é¡Œçµæœ:")
    print(f"   MSE: {eval_results_2d['mse']:.6f}")
    print(f"   ç›¸é–¢: {eval_results_2d['correlation']:.4f}")
    print(f"   ç‰©ç†åˆ¶ç´„é•å: {eval_results_2d['physics_constraint_violation']:.6f}")
    
    all_results['2d_problem'] = {
        'config': config_2d.__dict__,
        'evaluation': eval_results_2d,
        'training_history': history_2d
    }
    
    # 2. 4æ¬¡å…ƒæ™‚ç©ºå•é¡Œ
    print("\nğŸ” 2. 4æ¬¡å…ƒæ™‚ç©ºNKATå•é¡Œã®è§£æ±º")
    print("å•é¡Œï¼šMinkowskiæ™‚ç©ºã§ã®ä¿®æ­£å ´ã®æ–¹ç¨‹å¼")
    
    config_4d = PIKANConfig(
        input_dim=4,
        hidden_dims=[64, 128, 64],
        output_dim=1,
        num_basis_functions=10,
        theta_parameter=1e-4,
        kappa_parameter=1e-3,
        physics_weight=0.2,
        learning_rate=1e-3,
        batch_size=32,
        num_epochs=300
    )
    
    # è¨“ç·´
    network_4d, history_4d = train_pikan_network(config_4d)
    
    # è©•ä¾¡
    eval_results_4d = evaluate_pikan_network(network_4d, config_4d)
    
    print(f"âœ… 4æ¬¡å…ƒå•é¡Œçµæœ:")
    print(f"   MSE: {eval_results_4d['mse']:.6f}")
    print(f"   ç›¸é–¢: {eval_results_4d['correlation']:.4f}")
    print(f"   ç‰©ç†åˆ¶ç´„é•å: {eval_results_4d['physics_constraint_violation']:.6f}")
    
    all_results['4d_problem'] = {
        'config': config_4d.__dict__,
        'evaluation': eval_results_4d,
        'training_history': history_4d
    }
    
    # 3. çµ±åˆçµæœã®è¡¨ç¤º
    print("\nğŸ“Š 3. PI-KANæ€§èƒ½æ¯”è¼ƒ")
    print("=" * 50)
    
    problems = ['2æ¬¡å…ƒ', '4æ¬¡å…ƒ']
    mse_values = [eval_results_2d['mse'], eval_results_4d['mse']]
    correlations = [eval_results_2d['correlation'], eval_results_4d['correlation']]
    
    for i, problem in enumerate(problems):
        print(f"{problem}å•é¡Œ: MSE={mse_values[i]:.6f}, ç›¸é–¢={correlations[i]:.4f}")
    
    # 4. çµæœã®ä¿å­˜
    with open('pikan_network_results.json', 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False, default=str)
    
    print("\nğŸ’¾ çµæœã‚’ 'pikan_network_results.json' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # 5. å¯è¦–åŒ–
    try:
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # è¨“ç·´å±¥æ­´ï¼ˆ2æ¬¡å…ƒï¼‰
        epochs_2d = range(len(history_2d['total_loss']))
        ax1.plot(epochs_2d, history_2d['total_loss'], 'b-', label='ç·æå¤±', linewidth=2)
        ax1.plot(epochs_2d, history_2d['data_loss'], 'g-', label='ãƒ‡ãƒ¼ã‚¿æå¤±', linewidth=2)
        ax1.plot(epochs_2d, history_2d['physics_loss'], 'r-', label='ç‰©ç†åˆ¶ç´„æå¤±', linewidth=2)
        ax1.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
        ax1.set_ylabel('æå¤±')
        ax1.set_title('2æ¬¡å…ƒPI-KANè¨“ç·´å±¥æ­´')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')
        
        # è¨“ç·´å±¥æ­´ï¼ˆ4æ¬¡å…ƒï¼‰
        epochs_4d = range(len(history_4d['total_loss']))
        ax2.plot(epochs_4d, history_4d['total_loss'], 'b-', label='ç·æå¤±', linewidth=2)
        ax2.plot(epochs_4d, history_4d['data_loss'], 'g-', label='ãƒ‡ãƒ¼ã‚¿æå¤±', linewidth=2)
        ax2.plot(epochs_4d, history_4d['physics_loss'], 'r-', label='ç‰©ç†åˆ¶ç´„æå¤±', linewidth=2)
        ax2.set_xlabel('ã‚¨ãƒãƒƒã‚¯')
        ax2.set_ylabel('æå¤±')
        ax2.set_title('4æ¬¡å…ƒPI-KANè¨“ç·´å±¥æ­´')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.set_yscale('log')
        
        # æ€§èƒ½æ¯”è¼ƒ
        metrics = ['MSE', 'ç›¸é–¢ä¿‚æ•°']
        values_2d = [eval_results_2d['mse'], eval_results_2d['correlation']]
        values_4d = [eval_results_4d['mse'], eval_results_4d['correlation']]
        
        x_pos = np.arange(len(metrics))
        width = 0.35
        
        bars1 = ax3.bar(x_pos - width/2, values_2d, width, label='2æ¬¡å…ƒ', alpha=0.7, color='blue')
        bars2 = ax3.bar(x_pos + width/2, values_4d, width, label='4æ¬¡å…ƒ', alpha=0.7, color='red')
        
        ax3.set_xlabel('è©•ä¾¡æŒ‡æ¨™')
        ax3.set_ylabel('å€¤')
        ax3.set_title('PI-KANæ€§èƒ½æ¯”è¼ƒ')
        ax3.set_xticks(x_pos)
        ax3.set_xticklabels(metrics)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bars in [bars1, bars2]:
            for bar in bars:
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{height:.4f}', ha='center', va='bottom')
        
        # ç‰©ç†åˆ¶ç´„é•åæ¯”è¼ƒ
        physics_violations = [eval_results_2d['physics_constraint_violation'], 
                            eval_results_4d['physics_constraint_violation']]
        
        bars = ax4.bar(problems, physics_violations, alpha=0.7, color=['blue', 'red'])
        ax4.set_ylabel('ç‰©ç†åˆ¶ç´„é•å')
        ax4.set_title('ç‰©ç†åˆ¶ç´„æº€è¶³åº¦')
        ax4.grid(True, alpha=0.3)
        ax4.set_yscale('log')
        
        # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
        for bar, value in zip(bars, physics_violations):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height * 1.1,
                    f'{value:.2e}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('pikan_network_analysis.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š ã‚°ãƒ©ãƒ•ã‚’ 'pikan_network_analysis.png' ã«ä¿å­˜ã—ã¾ã—ãŸ")
        plt.show()
        
    except Exception as e:
        logger.warning(f"âš ï¸ å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 6. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä¿å­˜
    torch.save(network_2d.state_dict(), 'pikan_2d_model.pth')
    torch.save(network_4d.state_dict(), 'pikan_4d_model.pth')
    print("ğŸ’¾ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    
    return all_results, network_2d, network_4d

if __name__ == "__main__":
    """
    PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®Ÿè¡Œ
    """
    try:
        results, model_2d, model_4d = demonstrate_pikan_applications()
        print("ğŸ‰ PI-KANãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®Ÿè£…ã¨è¨“ç·´ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}") 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŒ NKATç†è«– Hybridæ·±å±¤å­¦ç¿’æœ€é©åŒ– (RTX3080ç‰¹åŒ–+tqdmå¼·åŒ–ç‰ˆ)
===========================================================

ğŸ¯ ç›®æ¨™: ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ 6.05 â†’ 4.0Â±0.1 ã«åæŸ
ğŸ“ Î¸-runningå­¦ç¿’: Î²Î¸ä¿‚æ•°ã®å¾®èª¿æ•´
âš–ï¸ ç‰©ç†åˆ¶ç´„: Jacobi + CP + Connesè·é›¢æ•´åˆ

ğŸš€ æˆ¦ç•¥: KAN + Optuna + tqdmè©³ç´°ç›£è¦– + RTX3080æœ€é©åŒ–
ğŸ’» ç’°å¢ƒ: ãƒ­ãƒ¼ã‚«ãƒ«Windows11 RTX3080 (10.7GB VRAMç‰¹åŒ–)
"""

# ===================================================================
# ğŸ“¦ Advanced ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (tqdmå¼·åŒ–)
# ===================================================================

print("ğŸš€ NKAT Hybridæ·±å±¤å­¦ç¿’æœ€é©åŒ–é–‹å§‹ï¼")
print("ğŸ“¦ RTX3080æœ€é©åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")

import subprocess
import sys

def install_rtx3080_packages():
    """RTX3080æœ€é©åŒ–ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    packages = [
        'torch', 'torchvision', 'torchaudio', '--index-url', 'https://download.pytorch.org/whl/cu118',
        'optuna', 'plotly', 'kaleido',
        'tqdm', 'matplotlib', 'seaborn', 
        'numpy', 'scipy', 'pandas',
        'rich',  # é«˜åº¦ãªé€²æ—è¡¨ç¤º
        'psutil',  # GPUç›£è¦–
    ]
    
    for i in range(0, len(packages)):
        if packages[i].startswith('--'):
            continue
        try:
            if packages[i] == 'torch':
                # PyTorch CUDAç‰ˆã‚’ç¢ºå®Ÿã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
                subprocess.check_call([
                    sys.executable, '-m', 'pip', 'install', 
                    'torch', 'torchvision', 'torchaudio', 
                    '--index-url', 'https://download.pytorch.org/whl/cu118',
                    '--quiet'
                ])
                print(f"âœ… PyTorch CUDAç‰ˆ")
            elif packages[i] not in ['torchvision', 'torchaudio']:
                subprocess.check_call([sys.executable, '-m', 'pip', 'install', packages[i], '--quiet'])
                print(f"âœ… {packages[i]}")
        except:
            print(f"âš ï¸ {packages[i]} ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—ï¼ˆç¶™ç¶šï¼‰")

# Colabç’°å¢ƒãƒã‚§ãƒƒã‚¯
try:
    from google.colab import drive
    IN_COLAB = True
    print("ğŸ“± Google Colabç’°å¢ƒã‚’æ¤œå‡º")
    install_rtx3080_packages()
except ImportError:
    IN_COLAB = False
    print("ğŸ’» ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œï¼ˆRTX3080æƒ³å®šï¼‰")
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã‚‚å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
    try:
        install_rtx3080_packages()
    except:
        print("âš ï¸ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—ï¼ˆæ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¨å¥¨ï¼‰")

# ===================================================================
# ğŸ“š Advanced ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ (tqdm + GPUç›£è¦–)
# ===================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam, AdamW
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm, trange
import time
import json
import warnings
from dataclasses import dataclass, field
from typing import Tuple, List, Optional, Dict, Any
import os
import optuna
import pickle
from datetime import datetime, timedelta

# ãƒªãƒƒãƒé€²æ—è¡¨ç¤º
try:
    from rich.progress import Progress, TextColumn, BarColumn, TimeElapsedColumn, TimeRemainingColumn
    from rich.console import Console
    from rich.live import Live
    RICH_AVAILABLE = True
    console = Console()
except ImportError:
    RICH_AVAILABLE = False
    print("âš ï¸ richæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆåŸºæœ¬tqdmã‚’ä½¿ç”¨ï¼‰")

# GPUç›£è¦–
try:
    import psutil
    GPU_MONITORING = True
except ImportError:
    GPU_MONITORING = False

# GPUè¨­å®š + Mixed Precision (RTX3080ç‰¹åŒ–)
warnings.filterwarnings('ignore')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ”¥ ãƒ‡ãƒã‚¤ã‚¹: {device}")

if torch.cuda.is_available():
    gpu_name = torch.cuda.get_device_name(0)
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
    print(f"ğŸ¯ GPU: {gpu_name}")
    print(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f} GB")
    
    # RTX3080ç‰¹åŒ–è¨­å®š
    if "RTX 3080" in gpu_name or "RTX3080" in gpu_name:
        print("ğŸš€ RTX3080æ¤œå‡ºï¼æœ€é©åŒ–è¨­å®šã‚’é©ç”¨")
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
    else:
        torch.backends.cudnn.benchmark = True

# Mixed Precision ã‚µãƒãƒ¼ãƒˆ
scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None

# ===================================================================
# ğŸ“ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
# ===================================================================

if IN_COLAB:
    print("ğŸ“ Google Drive é€£æºã‚’è©¦è¡Œä¸­...")
    try:
        drive.mount('/content/drive')
        work_dir = '/content/drive/MyDrive/NKAT_Hybrid_Results'
        os.makedirs(work_dir, exist_ok=True)
        print(f"âœ… Google Drive ãƒã‚¦ãƒ³ãƒˆæˆåŠŸ: {work_dir}")
    except Exception as e:
        print(f"âš ï¸ Google Drive ãƒã‚¦ãƒ³ãƒˆå¤±æ•—: {str(e)}")
        print("ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ã—ã¾ã™")
        work_dir = '/content/nkat_hybrid_results'
        os.makedirs(work_dir, exist_ok=True)
        print(f"ğŸ“‚ ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {work_dir}")
else:
    work_dir = './nkat_hybrid_results'
    os.makedirs(work_dir, exist_ok=True)
    print(f"ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {work_dir}")

# ===================================================================
# âš™ï¸ RTX3080æœ€é©åŒ– NKATè¨­å®šã‚¯ãƒ©ã‚¹
# ===================================================================

@dataclass
class HybridNKATConfig:
    """Hybrid NKATæœ€é©åŒ–è¨­å®š (RTX3080 VRAMæœ€å¤§æ´»ç”¨ + é›»æºæ–­å¯¾å¿œç‰ˆ)"""
    # ç‰©ç†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    theta_base: float = 1e-70
    planck_scale: float = 1.6e-35
    target_spectral_dim: float = 4.0
    spectral_dim_tolerance: float = 0.1
    
    # RTX3080 VRAMæœ€å¤§æ´»ç”¨è¨­å®š ğŸ”¥
    grid_size: int = 64  # RTX3080ãªã‚‰64Â³ã‚‚å¯èƒ½ï¼
    batch_size: int = 24  # VRAM 10.7GBæœ€å¤§æ´»ç”¨ (8â†’24)
    num_test_functions: int = 256  # é«˜ç²¾åº¦åŒ– (128â†’256)
    
    # KAN DLè¨­å®šï¼ˆã•ã‚‰ã«å¤§å‹åŒ–ï¼‰
    kan_layers: List[int] = field(default_factory=lambda: [4, 512, 256, 128, 4])  # å±¤å¤§å¹…æ‹¡å¤§
    learning_rate: float = 3e-4  # RTX3080ã§é«˜é€ŸåŒ– (2e-4â†’3e-4)
    num_epochs: int = 150  # ååˆ†ãªå­¦ç¿’
    
    # Optunaè¨­å®š
    n_trials: int = 75  # RTX3080ãƒ‘ãƒ¯ãƒ¼ã§å¢—åŠ 
    study_name: str = "NKAT_RTX3080_MAX_Optimization"
    
    # ç‰©ç†åˆ¶ç´„é‡ã¿ï¼ˆå¼·åŒ–ç‰ˆï¼‰
    weight_spectral_dim: float = 20.0  # ã•ã‚‰ã«å¼·åŒ– (15.0â†’20.0)
    weight_jacobi: float = 2.0  # å¼·åŒ– (1.5â†’2.0)
    weight_connes: float = 2.0  # å¼·åŒ– (1.5â†’2.0)
    weight_theta_reg: float = 0.2  # å¾®èª¿æ•´
    weight_running: float = 4.0  # Î¸-running ã•ã‚‰ã«å¼·åŒ– (3.0â†’4.0)
    
    # tqdmç›£è¦–è¨­å®š
    progress_update_freq: int = 3  # ã‚ˆã‚Šé »ç¹ãªæ›´æ–° (5â†’3)
    gpu_monitoring: bool = True
    
    # ğŸ”§ é›»æºæ–­å¯¾å¿œãƒªã‚«ãƒãƒªãƒ¼è¨­å®š
    checkpoint_freq: int = 5  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé »åº¦ï¼ˆã‚¨ãƒãƒƒã‚¯ã”ã¨ï¼‰
    auto_backup: bool = True  # è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
    resume_from_checkpoint: bool = False  # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ å®Ÿè¡Œ
    checkpoint_dir: str = "./nkat_checkpoints"  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    max_checkpoints: int = 10  # ä¿æŒã™ã‚‹æœ€å¤§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•°
    emergency_save_interval: int = 30  # ç·Šæ€¥ä¿å­˜é–“éš”ï¼ˆåˆ†ï¼‰

# ===================================================================
# ğŸ§  Advanced KAN Layer (B-spline Enhanced)
# ===================================================================

class AdvancedKANLayer(nn.Module):
    """Advanced KAN with learnable spline knots"""
    def __init__(self, input_dim: int, output_dim: int, 
                 grid_size: int = 8, spline_order: int = 3):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.grid_size = grid_size
        self.spline_order = spline_order
        
        # Learnable spline coefficients
        self.spline_coeffs = nn.Parameter(
            torch.randn(input_dim, output_dim, grid_size) * 0.1
        )
        
        # Learnable knot positions
        self.knot_positions = nn.Parameter(
            torch.linspace(-2, 2, grid_size).unsqueeze(0).unsqueeze(0).repeat(input_dim, output_dim, 1)
        )
        
        # Scaling and bias
        self.scale = nn.Parameter(torch.ones(input_dim, output_dim))
        self.bias = nn.Parameter(torch.zeros(output_dim))
        
    def forward(self, x):
        batch_size = x.shape[0]
        x_norm = torch.tanh(x)  # Normalized input
        
        output = torch.zeros(batch_size, self.output_dim, device=x.device)
        
        for i in range(self.input_dim):
            for j in range(self.output_dim):
                # B-spline basis evaluation
                knots = self.knot_positions[i, j]
                coeffs = self.spline_coeffs[i, j]
                
                # RBF approximation of B-splines
                distances = (x_norm[:, i:i+1] - knots.unsqueeze(0))**2
                basis_values = torch.exp(-2.0 * distances)
                spline_output = torch.sum(basis_values * coeffs.unsqueeze(0), dim=1)
                
                output[:, j] += self.scale[i, j] * spline_output
        
        return output + self.bias

# ===================================================================
# ğŸ¯ Hybrid NKAT Model (RTX3080æœ€é©åŒ–KAN)
# ===================================================================

class HybridNKATModel(nn.Module):
    """RTX3080æœ€é©åŒ– KAN NKAT Model"""
    def __init__(self, config: HybridNKATConfig):
        super().__init__()
        self.config = config
        
        # KAN stack for Dirac operator (RTX3080å¤§å‹åŒ–)
        self.kan_layers = nn.ModuleList()
        for i in range(len(config.kan_layers) - 1):
            self.kan_layers.append(
                AdvancedKANLayer(config.kan_layers[i], config.kan_layers[i+1], 
                               grid_size=16)  # é«˜ç²¾åº¦ã‚°ãƒªãƒƒãƒ‰ (12â†’16)
            )
        
        # Î¸ parameter learning (with running) - RTX3080å¤§å‹åŒ–
        self.theta_base_log = nn.Parameter(torch.log(torch.tensor(config.theta_base)))
        self.theta_running_net = nn.Sequential(
            nn.Linear(1, 128),  # ã•ã‚‰ã«æ‹¡å¤§ (64â†’128)
            nn.GELU(),  # RTX3080ã§GELUé«˜é€Ÿ
            nn.Dropout(0.1),  # æ­£å‰‡åŒ–è¿½åŠ 
            nn.Linear(128, 64),
            nn.GELU(),
            nn.Dropout(0.05),
            nn.Linear(64, 32),
            nn.GELU(),
            nn.Linear(32, 1)   # running coefficient
        )
        
        # Gamma matrices (4x4 Dirac representation)
        self.register_buffer('gamma_matrices', self._create_gamma_matrices())
        
    def _create_gamma_matrices(self):
        """Create Dirac gamma matrices"""
        gamma = torch.zeros(4, 4, 4, dtype=torch.complex64)
        
        # Î³â° (time-like)
        gamma[0] = torch.tensor([
            [1, 0, 0, 0],
            [0, 1, 0, 0], 
            [0, 0, -1, 0],
            [0, 0, 0, -1]
        ], dtype=torch.complex64)
        
        # Î³Â¹, Î³Â², Î³Â³ (space-like) 
        gamma[1] = torch.tensor([
            [0, 0, 0, 1],
            [0, 0, 1, 0],
            [0, -1, 0, 0],
            [-1, 0, 0, 0]
        ], dtype=torch.complex64)
        
        gamma[2] = torch.tensor([
            [0, 0, 0, -1j],
            [0, 0, 1j, 0],
            [0, 1j, 0, 0],
            [-1j, 0, 0, 0]
        ], dtype=torch.complex64)
        
        gamma[3] = torch.tensor([
            [0, 0, 1, 0],
            [0, 0, 0, -1],
            [-1, 0, 0, 0],
            [0, 1, 0, 0]
        ], dtype=torch.complex64)
        
        return gamma
    
    def forward(self, x, energy_scale=None):
        # KAN forward pass
        kan_output = x
        for kan_layer in self.kan_layers:
            kan_output = kan_layer(kan_output)
            kan_output = F.gelu(kan_output)  # RTX3080 GELUæœ€é©åŒ–
        
        # Convert to complex Dirac spinor
        dirac_real = kan_output.view(-1, 4, 1)
        dirac_field = torch.complex(dirac_real, torch.zeros_like(dirac_real))
        
        # Î¸ parameter with running
        if energy_scale is not None:
            log_energy = torch.log10(energy_scale)
            running_coeff = self.theta_running_net(log_energy)
            theta = torch.exp(self.theta_base_log + running_coeff.squeeze())
        else:
            theta = torch.exp(self.theta_base_log)
        
        return dirac_field, theta

# ===================================================================
# ğŸ† Advanced Physics Loss
# ===================================================================

class AdvancedPhysicsLoss(nn.Module):
    """Advanced physics-constrained loss function"""
    def __init__(self, config: HybridNKATConfig):
        super().__init__()
        self.config = config
        
    def spectral_dimension_loss(self, dirac_field, target_dim=4.0):
        """Enhanced spectral dimension estimation"""
        field_magnitudes = torch.abs(dirac_field)
        
        # Component-wise variance analysis
        component_vars = torch.var(field_magnitudes, dim=0)
        
        # Effective dimension (improved estimator)
        total_var = torch.sum(component_vars)
        entropy_term = -torch.sum(component_vars * torch.log(component_vars + 1e-8))
        estimated_dim = 4.0 * torch.sigmoid(entropy_term / total_var)
        
        return F.smooth_l1_loss(estimated_dim, torch.tensor(target_dim, device=dirac_field.device))
    
    def jacobi_constraint_loss(self, dirac_field):
        """Jacobi identity constraint (anticommutativity)"""
        # Enhanced anticommutator constraint
        anticommutator = torch.sum(dirac_field * dirac_field.conj(), dim=1).real
        return torch.mean(anticommutator**2)
    
    def connes_distance_loss(self, dirac_field, coordinates):
        """Connes distance consistency"""
        batch_size = coordinates.shape[0]
        if batch_size < 2:
            return torch.tensor(0.0, device=dirac_field.device)
        
        # Pairwise distances
        coord_diff = coordinates.unsqueeze(1) - coordinates.unsqueeze(0)
        euclidean_dist = torch.norm(coord_diff, dim=2)
        
        field_diff = dirac_field.unsqueeze(1) - dirac_field.unsqueeze(0)
        dirac_dist = torch.norm(field_diff, dim=2)
        
        # Distance consistency with soft constraint
        mask = euclidean_dist > 1e-6  # Avoid self-distances
        if mask.sum() > 0:
            return F.smooth_l1_loss(
                dirac_dist[mask], 
                euclidean_dist[mask]
            )
        return torch.tensor(0.0, device=dirac_field.device)
    
    def theta_running_loss(self, theta_values, energy_scale):
        """Î¸-running consistency loss (gradientä¿®æ­£ç‰ˆ)"""
        if energy_scale is None or len(theta_values) < 2:
            return torch.tensor(0.0, device=theta_values.device)
        
        # Encourage small but non-zero running
        log_energy = torch.log(energy_scale + 1e-10).squeeze()
        log_theta = torch.log(theta_values + 1e-100)
        
        # Î² function should be small but non-zero (gradientä¿®æ­£)
        if len(log_energy) > 1:
            # ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã¨ã—ã¦ spacing ã‚’è¨ˆç®—
            energy_spacing = (log_energy[-1] - log_energy[0]) / (len(log_energy) - 1)
            beta_theta = torch.gradient(log_theta, spacing=energy_spacing.item())[0]
            target_beta = torch.zeros_like(beta_theta)
            return F.mse_loss(beta_theta, target_beta)
        
        return torch.tensor(0.0, device=theta_values.device)
    
    def forward(self, dirac_field, theta, coordinates, energy_scale=None):
        """Comprehensive loss computation"""
        losses = {}
        
        # Individual loss components
        losses['spectral_dim'] = self.spectral_dimension_loss(
            dirac_field, self.config.target_spectral_dim
        )
        losses['jacobi'] = self.jacobi_constraint_loss(dirac_field)
        losses['connes'] = self.connes_distance_loss(dirac_field, coordinates)
        losses['theta_running'] = self.theta_running_loss(theta, energy_scale)
        
        # Regularization
        losses['theta_reg'] = F.mse_loss(
            torch.log(theta + 1e-100), 
            torch.log(torch.tensor(self.config.theta_base, device=theta.device))
        )
        
        # Weighted total loss
        total_loss = (
            self.config.weight_spectral_dim * losses['spectral_dim'] +
            self.config.weight_jacobi * losses['jacobi'] +
            self.config.weight_connes * losses['connes'] +
            self.config.weight_running * losses['theta_running'] +
            self.config.weight_theta_reg * losses['theta_reg']
        )
        
        losses['total'] = total_loss
        return losses

# ===================================================================
# ğŸ”¬ Optuna Optimization
# ===================================================================

def objective(trial, config: HybridNKATConfig):
    """Optuna objective function"""
    
    # Hyperparameter suggestions (RTX3080ç‰¹åŒ–ç¯„å›²)
    lr = trial.suggest_float('learning_rate', 1e-4, 5e-4, log=True)  # ç¯„å›²æ‹¡å¤§
    weight_spectral = trial.suggest_float('weight_spectral_dim', 10.0, 30.0)  # ç¯„å›²æ‹¡å¤§
    weight_running = trial.suggest_float('weight_running', 1.0, 8.0)  # ç¯„å›²æ‹¡å¤§
    batch_size = trial.suggest_categorical('batch_size', [16, 20, 24, 28])  # RTX3080å‘ã‘å¤§å‹ãƒãƒƒãƒ
    
    # Update config
    config.learning_rate = lr
    config.weight_spectral_dim = weight_spectral
    config.weight_running = weight_running
    config.batch_size = batch_size
    
    # Quick training (VRAMæ´»ç”¨ç‰ˆ)
    model = HybridNKATModel(config).to(device)
    criterion = AdvancedPhysicsLoss(config)
    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=1e-5)
    
    # Training data (å¢—é‡ç‰ˆ)
    num_samples = 800 if torch.cuda.is_available() else 200  # RTX3080ã§å¢—é‡ (200â†’800)
    train_coords = torch.randn(num_samples, 4, device=device) * 2 * np.pi
    energy_scales = torch.logspace(10, 19, num_samples, device=device).unsqueeze(1)
    
    # Quick training loop
    model.train()
    final_spectral_loss = float('inf')
    
    for epoch in range(20):  # Quick evaluation
        total_loss = 0
        
        for i in range(0, num_samples, batch_size):
            batch_coords = train_coords[i:i+batch_size]
            batch_energy = energy_scales[i:i+batch_size]
            
            optimizer.zero_grad()
            
            with torch.cuda.amp.autocast() if scaler else torch.no_grad():
                dirac_field, theta = model(batch_coords, batch_energy)
                losses = criterion(dirac_field, theta, batch_coords, batch_energy)
            
            if scaler:
                scaler.scale(losses['total']).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                losses['total'].backward()
                optimizer.step()
            
            total_loss += losses['total'].item()
            final_spectral_loss = losses['spectral_dim'].item()
    
    return final_spectral_loss

# ===================================================================
# ğŸš€ RTX3080æœ€é©åŒ– Training Function (tqdmå¼·åŒ–ç‰ˆ)
# ===================================================================

def train_hybrid_nkat(config: HybridNKATConfig, use_optuna: bool = True):
    """RTX3080æœ€é©åŒ– + é›»æºæ–­å¯¾å¿œ Hybrid NKAT training"""
    
    print("ğŸ¯ RTX3080æœ€é©åŒ– + é›»æºæ–­å¯¾å¿œ Hybrid NKATè¨“ç·´é–‹å§‹")
    print(f"ğŸ”¥ è¨­å®š: ã‚°ãƒªãƒƒãƒ‰{config.grid_size}Â³, ãƒãƒƒãƒ{config.batch_size}, ã‚¨ãƒãƒƒã‚¯{config.num_epochs}")
    print(f"ğŸ”§ ãƒªã‚«ãƒãƒªãƒ¼: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ{config.checkpoint_freq}epæ¯, ç·Šæ€¥ä¿å­˜{config.emergency_save_interval}åˆ†æ¯")
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    checkpoint_manager = NKATCheckpointManager(config)
    
    best_config = config
    start_epoch = 0
    resume_data = None
    
    # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ å‡¦ç†
    if config.resume_from_checkpoint:
        print("ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©æ—§ã‚’è©¦è¡Œä¸­...")
        resume_data = checkpoint_manager.load_latest_checkpoint()
        
    if resume_data:
        print(f"âœ… ã‚¨ãƒãƒƒã‚¯ {resume_data['epoch']} ã‹ã‚‰å†é–‹")
        best_config = resume_data['config']
        start_epoch = resume_data['epoch'] + 1
        
        # ãƒ©ãƒ³ãƒ€ãƒ çŠ¶æ…‹å¾©å…ƒ
        torch.set_rng_state(resume_data['random_state'])
        if resume_data['cuda_random_state'] is not None and torch.cuda.is_available():
            torch.cuda.set_rng_state(resume_data['cuda_random_state'])
    else:
        print("ğŸ†• æ–°è¦è¨“ç·´é–‹å§‹")
        
        if use_optuna:
            print("ğŸ”¬ Optuna ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–å®Ÿè¡Œä¸­...")
            
            # æ—¢å­˜ã®Optunaçµæœèª­ã¿è¾¼ã¿
            existing_study = load_optuna_study(config)
            
            if existing_study:
                print("ğŸ“‚ æ—¢å­˜ã®Optunaçµæœã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                study = existing_study
            else:
                study = optuna.create_study(
                    direction='minimize',
                    study_name=config.study_name
                )
            
            # Optunaã‚‚tqdmç›£è¦–
            remaining_trials = max(0, config.n_trials - len(study.trials))
            
            if remaining_trials > 0:
                with tqdm(total=remaining_trials, desc="ğŸ” Optunaæœ€é©åŒ–", 
                         bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} trials [{elapsed}<{remaining}]') as optuna_pbar:
                    
                    def callback(study, trial):
                        optuna_pbar.set_postfix({
                            'Best': f'{study.best_value:.6f}',
                            'Trial': trial.number,
                            'Value': f'{trial.value:.6f}' if trial.value else 'Failed'
                        })
                        optuna_pbar.update(1)
                        
                        # Optunaé€”ä¸­ä¿å­˜
                        if trial.number % 5 == 0:
                            save_optuna_study(study, config)
                    
                    study.optimize(
                        lambda trial: objective(trial, config),
                        n_trials=remaining_trials,
                        timeout=3600,  # 1 hour limit
                        callbacks=[callback]
                    )
                
                # æœ€çµ‚Optunaä¿å­˜
                save_optuna_study(study, config)
            
            # Best parameters
            best_params = study.best_params
            print(f"ğŸ† æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {best_params}")
            
            # Update config
            for key, value in best_params.items():
                setattr(best_config, key, value)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã¾ãŸã¯ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ 
    print("ğŸš€ æœ€çµ‚è¨“ç·´é–‹å§‹ï¼ˆæœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ + RTX3080æœ€é©åŒ– + é›»æºæ–­å¯¾å¿œï¼‰")
    
    model = HybridNKATModel(best_config).to(device)
    criterion = AdvancedPhysicsLoss(best_config)
    optimizer = AdamW(model.parameters(), lr=best_config.learning_rate, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=best_config.num_epochs)
    
    # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ æ™‚ã®çŠ¶æ…‹å¾©å…ƒ
    if resume_data:
        model.load_state_dict(resume_data['model_state_dict'])
        optimizer.load_state_dict(resume_data['optimizer_state_dict'])
        scheduler.load_state_dict(resume_data['scheduler_state_dict'])
        history = resume_data['history']
        print(f"ğŸ“Š å­¦ç¿’å±¥æ­´å¾©å…ƒ: {len(history['total_loss'])}ã‚¨ãƒãƒƒã‚¯åˆ†")
    else:
        # Training history
        history = {
            'total_loss': [],
            'spectral_dim_loss': [],
            'jacobi_loss': [],
            'connes_loss': [],
            'theta_running_loss': [],
            'theta_values': [],
            'spectral_dim_estimates': [],
            'gpu_memory_usage': [],  # GPUç›£è¦–è¿½åŠ 
            'training_speed': []     # é€Ÿåº¦ç›£è¦–
        }
    
    # Training data (RTX3080 VRAMæœ€å¤§æ´»ç”¨ç‰ˆ)
    num_samples = 6000 if torch.cuda.is_available() else 1000  # RTX3080ã§å¤§å¹…å¢—é‡ (2000â†’6000)
    train_coords = torch.randn(num_samples, 4, device=device) * 2 * np.pi
    energy_scales = torch.logspace(10, 19, num_samples, device=device).unsqueeze(1)
    
    print(f"ğŸ”¥ Training ãƒ‡ãƒ¼ã‚¿: {num_samples:,}ã‚µãƒ³ãƒ—ãƒ«, ãƒãƒƒãƒã‚µã‚¤ã‚º: {best_config.batch_size}")
    print(f"ğŸ’ª 1ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Š: {num_samples // best_config.batch_size}ãƒãƒƒãƒå‡¦ç†")
    
    # RTX3080 + tqdmå¼·åŒ– + é›»æºæ–­å¯¾å¿œ Training loop
    remaining_epochs = best_config.num_epochs - start_epoch
    
    with tqdm(total=remaining_epochs, desc="ğŸ¯ RTX3080 NKATæœ€é©åŒ– (é›»æºæ–­å¯¾å¿œ)", 
             bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} epochs [{elapsed}<{remaining}]',
             dynamic_ncols=True) as main_pbar:
        
        start_time = time.time()
        
        try:
            for epoch in range(start_epoch, best_config.num_epochs):
                epoch_start = time.time()
                model.train()
                epoch_losses = {key: 0.0 for key in history.keys() 
                              if key not in ['spectral_dim_estimates', 'gpu_memory_usage', 'training_speed']}
                num_batches = len(train_coords) // best_config.batch_size
                
                # Batchå‡¦ç† with tqdm
                batch_pbar = tqdm(range(0, len(train_coords), best_config.batch_size), 
                                desc=f"Epoch {epoch+1}", leave=False, 
                                bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} batches')
                
                for i in batch_pbar:
                    batch_coords = train_coords[i:i+best_config.batch_size]
                    batch_energy = energy_scales[i:i+best_config.batch_size]
                    
                    optimizer.zero_grad()
                    
                    with torch.cuda.amp.autocast() if scaler else torch.no_grad():
                        dirac_field, theta = model(batch_coords, batch_energy)
                        losses = criterion(dirac_field, theta, batch_coords, batch_energy)
                    
                    if scaler:
                        scaler.scale(losses['total']).backward()
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        losses['total'].backward()
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        optimizer.step()
                    
                    # Accumulate losses
                    for key in epoch_losses:
                        if key == 'theta_values':
                            epoch_losses[key] += theta.mean().item()
                        else:
                            loss_key = key.replace('_loss', '').replace('theta_values', 'total')
                            epoch_losses[key] += losses[loss_key].item()
                    
                    # Real-time batch metrics
                    batch_pbar.set_postfix({
                        'Loss': f'{losses["total"].item():.4f}',
                        'Î¸': f'{theta.mean().item():.2e}'
                    })
                
                batch_pbar.close()
                scheduler.step()
                
                # Evaluation
                model.eval()
                with torch.no_grad():
                    eval_coords = train_coords[:best_config.batch_size]
                    eval_energy = energy_scales[:best_config.batch_size]
                    eval_dirac, eval_theta = model(eval_coords, eval_energy)
                    eval_losses = criterion(eval_dirac, eval_theta, eval_coords, eval_energy)
                    
                    # Spectral dimension estimation
                    field_magnitudes = torch.abs(eval_dirac.squeeze())
                    component_vars = torch.var(field_magnitudes, dim=0)
                    total_var = torch.sum(component_vars)
                    entropy_term = -torch.sum(component_vars * torch.log(component_vars + 1e-8))
                    estimated_dim = (4.0 * torch.sigmoid(entropy_term / total_var)).item()
                
                # Update history
                for key in history:
                    if key == 'spectral_dim_estimates':
                        history[key].append(estimated_dim)
                    elif key == 'theta_values':
                        history[key].append(eval_theta.mean().item())
                    elif key == 'gpu_memory_usage':
                        if torch.cuda.is_available():
                            gpu_mem = torch.cuda.memory_allocated() / 1e9
                            history[key].append(gpu_mem)
                        else:
                            history[key].append(0)
                    elif key == 'training_speed':
                        epoch_time = time.time() - epoch_start
                        history[key].append(epoch_time)
                    else:
                        loss_key = key.replace('_loss', '').replace('theta_values', 'total')
                        history[key].append(eval_losses[loss_key].item())
                
                # RTX3080æœ€é©åŒ– Progress update
                current_gpu_mem = history['gpu_memory_usage'][-1] if history['gpu_memory_usage'] else 0
                epoch_speed = history['training_speed'][-1] if history['training_speed'] else 0
                convergence_rate = abs(estimated_dim - best_config.target_spectral_dim)
                
                main_pbar.set_postfix({
                    'Loss': f'{history["total_loss"][-1]:.4f}',
                    'Spec_Dim': f'{estimated_dim:.3f}â†’{best_config.target_spectral_dim:.1f}',
                    'Conv': f'{convergence_rate:.3f}',
                    'Î¸': f'{eval_theta.mean().item():.1e}',
                    'GPU': f'{current_gpu_mem:.1f}GB',
                    'Speed': f'{epoch_speed:.1f}s/ep',
                    'LR': f'{scheduler.get_last_lr()[0]:.1e}'
                })
                
                # ğŸ”§ é›»æºæ–­å¯¾å¿œãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                should_save_checkpoint = False
                is_emergency = False
                
                # å®šæœŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if (epoch + 1) % config.checkpoint_freq == 0:
                    should_save_checkpoint = True
                
                # ç·Šæ€¥ä¿å­˜
                if checkpoint_manager.should_emergency_save():
                    should_save_checkpoint = True
                    is_emergency = True
                
                # ç›®æ¨™é”æˆæ™‚ã®ä¿å­˜
                if abs(estimated_dim - best_config.target_spectral_dim) < best_config.spectral_dim_tolerance:
                    should_save_checkpoint = True
                    main_pbar.write(f"\nğŸŠ ç›®æ¨™é”æˆï¼ Epoch {epoch+1}: ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ {estimated_dim:.4f}")
                
                if should_save_checkpoint:
                    checkpoint_manager.save_checkpoint(
                        epoch, model, optimizer, scheduler, history, best_config,
                        is_emergency=is_emergency, best_loss=history['total_loss'][-1]
                    )
                
                # è©³ç´°ç›£è¦– (3ã‚¨ãƒãƒƒã‚¯ã”ã¨)
                if (epoch + 1) % config.progress_update_freq == 0:
                    elapsed_time = time.time() - start_time
                    eta = elapsed_time * (best_config.num_epochs - epoch - 1) / (epoch - start_epoch + 1)
                    
                    detailed_info = (
                        f"\nğŸ“Š Epoch {epoch+1}/{best_config.num_epochs} è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ (é›»æºæ–­å¯¾å¿œ):\n"
                        f"ğŸ¯ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {estimated_dim:.4f} (ç›®æ¨™: {best_config.target_spectral_dim} Â± {best_config.spectral_dim_tolerance})\n"
                        f"ğŸ“‰ Total Loss: {history['total_loss'][-1]:.6f}\n"
                        f"ğŸ”¬ ç‰©ç†åˆ¶ç´„: Spectral={eval_losses['spectral_dim'].item():.4f}, "
                        f"Jacobi={eval_losses['jacobi'].item():.4f}, Connes={eval_losses['connes'].item():.4f}\n"
                        f"ğŸ“ Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {eval_theta.mean().item():.2e} (Running={eval_losses['theta_running'].item():.4f})\n"
                        f"âš¡ è¨“ç·´é€Ÿåº¦: {epoch_speed:.1f}ç§’/ã‚¨ãƒãƒƒã‚¯\n"
                        f"ğŸ’¾ GPUä½¿ç”¨é‡: {current_gpu_mem:.2f}GB / 10.7GB\n"
                        f"â±ï¸ æ¨å®šæ®‹ã‚Šæ™‚é–“: {eta/60:.1f}åˆ†\n"
                        f"ğŸ”§ æ¬¡å›ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {config.checkpoint_freq - ((epoch + 1) % config.checkpoint_freq)}ã‚¨ãƒãƒƒã‚¯å¾Œ\n"
                    )
                    main_pbar.write(detailed_info)
                
                # Early stopping
                if abs(estimated_dim - best_config.target_spectral_dim) < best_config.spectral_dim_tolerance:
                    break
                
                main_pbar.update(1)
                
        except KeyboardInterrupt:
            print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­æ¤œå‡º")
            checkpoint_manager.save_checkpoint(
                epoch, model, optimizer, scheduler, history, best_config,
                is_emergency=True, best_loss=history['total_loss'][-1] if history['total_loss'] else float('inf')
            )
            print("ğŸ’¾ ç·Šæ€¥ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†")
            raise
            
        except Exception as e:
            print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
            checkpoint_manager.save_checkpoint(
                epoch, model, optimizer, scheduler, history, best_config,
                is_emergency=True, best_loss=history['total_loss'][-1] if history['total_loss'] else float('inf')
            )
            print("ğŸ’¾ ã‚¨ãƒ©ãƒ¼æ™‚ç·Šæ€¥ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†")
            raise
    
    # æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
    final_checkpoint = checkpoint_manager.save_checkpoint(
        epoch, model, optimizer, scheduler, history, best_config,
        is_emergency=False, best_loss=history['total_loss'][-1] if history['total_loss'] else float('inf')
    )
    
    final_time = time.time() - start_time
    print(f"âœ… RTX3080æœ€é©åŒ–+é›»æºæ–­å¯¾å¿œè¨“ç·´å®Œäº†ï¼ ç·æ™‚é–“: {final_time/60:.1f}åˆ†")
    print(f"ğŸ† æœ€çµ‚ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {history['spectral_dim_estimates'][-1]:.4f}")
    print(f"ğŸ’¾ æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {final_checkpoint}")
    
    return model, history, best_config

# ===================================================================
# ğŸ“Š Advanced Results Visualization
# ===================================================================

def plot_hybrid_results(history, config, save_path=None):
    """Advanced results plotting"""
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    fig.suptitle('ğŸŒŒ NKAT Hybridæ·±å±¤å­¦ç¿’æœ€é©åŒ–çµæœ', fontsize=18, fontweight='bold')
    
    epochs = range(1, len(history['total_loss']) + 1)
    
    # 1. Total Loss Evolution
    axes[0, 0].plot(epochs, history['total_loss'], 'b-', linewidth=2)
    axes[0, 0].set_title('ğŸ“‰ Total Loss Evolution')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_yscale('log')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Spectral Dimension Convergence
    axes[0, 1].plot(epochs, history['spectral_dim_estimates'], 'r-', linewidth=3, label='æ¨å®šå€¤')
    axes[0, 1].axhline(y=config.target_spectral_dim, color='g', linestyle='--', linewidth=2, label='ç›®æ¨™å€¤')
    axes[0, 1].axhline(y=6.05, color='orange', linestyle='--', alpha=0.7, label='åˆæœŸå€¤')
    axes[0, 1].fill_between(epochs, 
                           config.target_spectral_dim - config.spectral_dim_tolerance,
                           config.target_spectral_dim + config.spectral_dim_tolerance,
                           alpha=0.2, color='green', label='è¨±å®¹ç¯„å›²')
    axes[0, 1].set_title('ğŸ¯ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒåæŸ')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Spectral Dimension')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Î¸ Parameter Evolution
    axes[0, 2].plot(epochs, history['theta_values'], 'purple', linewidth=2)
    axes[0, 2].axhline(y=config.theta_base, color='gray', linestyle='--', alpha=0.7, label='åˆæœŸå€¤')
    axes[0, 2].set_title('ğŸ“ Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é€²åŒ–')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Î¸ [mÂ²]')
    axes[0, 2].set_yscale('log')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # 4. Loss Components Breakdown
    axes[1, 0].plot(epochs, history['spectral_dim_loss'], label='Spectral Dim', linewidth=2)
    axes[1, 0].plot(epochs, history['jacobi_loss'], label='Jacobi', linewidth=2)
    axes[1, 0].plot(epochs, history['connes_loss'], label='Connes', linewidth=2)
    axes[1, 0].plot(epochs, history['theta_running_loss'], label='Î¸-Running', linewidth=2)
    axes[1, 0].set_title('âš–ï¸ ç‰©ç†åˆ¶ç´„Lossåˆ†è§£')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Loss')
    axes[1, 0].set_yscale('log')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 5. Convergence Rate Analysis
    if len(history['spectral_dim_estimates']) > 10:
        convergence_rate = np.abs(np.diff(history['spectral_dim_estimates']))
        axes[1, 1].plot(epochs[1:], convergence_rate, 'green', linewidth=2)
        axes[1, 1].set_title('ğŸ“ˆ åæŸãƒ¬ãƒ¼ãƒˆåˆ†æ')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('|Î” Spectral Dimension|')
        axes[1, 1].set_yscale('log')
        axes[1, 1].grid(True, alpha=0.3)
    
    # 6. Phase Space Trajectory
    if len(epochs) > 1:
        axes[1, 2].plot(history['spectral_dim_estimates'], history['total_loss'], 'o-', 
                       alpha=0.7, markersize=3)
        axes[1, 2].axvline(x=config.target_spectral_dim, color='g', linestyle='--', alpha=0.7)
        axes[1, 2].set_title('ğŸŒ€ ä½ç›¸ç©ºé–“è»Œé“')
        axes[1, 2].set_xlabel('Spectral Dimension')
        axes[1, 2].set_ylabel('Total Loss')
        axes[1, 2].set_yscale('log')
        axes[1, 2].grid(True, alpha=0.3)
    
    # 7. Performance Metrics
    final_dim = history['spectral_dim_estimates'][-1]
    final_theta = history['theta_values'][-1]
    improvement = (6.05 - final_dim) / 6.05 * 100
    convergence_achieved = abs(final_dim - config.target_spectral_dim) < config.spectral_dim_tolerance
    
    metrics_text = f"""ğŸ† Hybridæœ€é©åŒ–çµæœ

ğŸ“Š ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒåˆ†æ:
   åˆæœŸå€¤: 6.05
   æœ€çµ‚å€¤: {final_dim:.3f}
   ç›®æ¨™å€¤: {config.target_spectral_dim:.3f} Â± {config.spectral_dim_tolerance:.3f}
   æ”¹å–„åº¦: {improvement:.1f}%
   åæŸåˆ¤å®š: {'âœ… æˆåŠŸ' if convergence_achieved else 'ğŸ”„ ç¶™ç¶šå¿…è¦'}
   
ğŸ“ Î¸ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
   åˆæœŸå€¤: {config.theta_base:.2e}
   æœ€çµ‚å€¤: {final_theta:.2e}
   
ğŸ¯ å®Ÿé¨“äºˆæ¸¬:
   CTAé…å»¶: {final_theta * 1e19:.2f} Ã— 10â»Â¹â¹ s
   PVLASæ„Ÿåº¦: {'å¯è¦³æ¸¬åŸŸ' if final_theta > 1e-75 else 'æ„Ÿåº¦ä»¥ä¸‹'}
   MAGISé©ç”¨: {'æ¨å¥¨' if final_dim < 4.5 else 'è¦æ”¹è‰¯'}
   
ğŸ’« è«–æ–‡æº–å‚™åº¦:
   {'ğŸŠ Nature/PRLæŠ•ç¨¿å¯èƒ½' if convergence_achieved else 'ğŸ“Š è¿½åŠ æœ€é©åŒ–æ¨å¥¨'}"""
    
    axes[2, 0].text(0.05, 0.95, metrics_text, transform=axes[2, 0].transAxes, 
                   fontsize=10, verticalalignment='top', fontfamily='monospace',
                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
    axes[2, 0].set_title('ğŸ“‹ æ€§èƒ½è©•ä¾¡ã‚µãƒãƒªãƒ¼')
    axes[2, 0].axis('off')
    
    # 8. Experimental Predictions
    energy_range = np.logspace(10, 19, 100)
    theta_range = [final_theta * (1 + 0.01 * np.sin(np.log(e/1e15))) for e in energy_range]
    
    axes[2, 1].loglog(energy_range, theta_range, 'b-', linewidth=2, label='Î¸(E) äºˆæ¸¬')
    axes[2, 1].axhline(y=1e-75, color='red', linestyle='--', alpha=0.7, label='PVLASæ„Ÿåº¦')
    axes[2, 1].axvline(x=1e12, color='green', linestyle='--', alpha=0.7, label='CTAç¯„å›²')
    axes[2, 1].set_title('ğŸ”­ å®Ÿé¨“è¦³æ¸¬äºˆæ¸¬')
    axes[2, 1].set_xlabel('Energy [eV]')
    axes[2, 1].set_ylabel('Î¸ [mÂ²]')
    axes[2, 1].legend()
    axes[2, 1].grid(True, alpha=0.3)
    
    # 9. Next Steps Roadmap
    next_steps = """ğŸš€ æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

ğŸ“… å³æ™‚ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (48h):
   âœ… ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒæœ€é©åŒ–å®Œäº†
   ğŸ“ CTA/PVLAS/MAGISæ„Ÿåº¦è¨ˆç®—
   ğŸ“Š è¦³æ¸¬å¯èƒ½æ€§è©•ä¾¡æ›´æ–°
   
ğŸ“… 1é€±é–“ä»¥å†…:
   ğŸ“„ LoI (Letter of Intent) è‰ç¨¿
   ğŸ¯ Nature AstronomyæŠ•ç¨¿æº–å‚™
   ğŸ¤ å®Ÿé¨“ã‚°ãƒ«ãƒ¼ãƒ—ã‚³ãƒ³ã‚¿ã‚¯ãƒˆ
   
ğŸ“… 1ãƒ¶æœˆä»¥å†…:
   ğŸ“š PRLè«–æ–‡åŸ·ç­†
   ğŸ’° ç ”ç©¶åŠ©æˆé‡‘ç”³è«‹
   ğŸŒ arXiv preprintå…¬é–‹
   
ğŸŠ æœ€çµ‚ç›®æ¨™:
   ğŸ† Nobel Prize Track Theory
   ğŸ”¬ å®Ÿé¨“çš„æ¤œè¨¼é”æˆ
   ğŸŒŒ çµ±ä¸€ç†è«–ç¢ºç«‹"""
    
    axes[2, 2].text(0.05, 0.95, next_steps, transform=axes[2, 2].transAxes, 
                   fontsize=9, verticalalignment='top', fontfamily='monospace',
                   bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
    axes[2, 2].set_title('ğŸ—ºï¸ æˆ¦ç•¥ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—')
    axes[2, 2].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š Hybridã‚°ãƒ©ãƒ•ä¿å­˜: {save_path}")
    
    plt.show()

# ===================================================================
# ğŸ“ LoI Template Generator
# ===================================================================

def generate_loi_template(final_metrics, config):
    """Generate Letter of Intent template"""
    
    loi_template = f"""
# Letter of Intent: Non-commutative Kolmogorov-Arnold Representation 
## Ultimate Unified Theory (NKAT) - Experimental Verification

### Executive Summary

We propose experimental verification of the Non-commutative Kolmogorov-Arnold 
Representation Ultimate Unified Theory (NKAT), demonstrating breakthrough 
progress in spectral dimension optimization from 6.05 to {final_metrics['final_spectral_dimension']:.3f}, 
achieving {final_metrics['improvement_percentage']:.1f}% improvement toward the theoretical target of 4.0.

### Key Physical Predictions

**1. Vacuum Birefringence:**
- Î¸ parameter: {final_metrics['final_theta']:.2e} mÂ²
- Observable via PVLAS-type experiments
- Energy-dependent running: Î²_Î¸ â‰ˆ 0.01

**2. Gamma-ray Delays:**
- Predicted delay: {final_metrics['final_theta'] * 1e19:.2f} Ã— 10â»Â¹â¹ s
- Testable with CTA TeV observations
- Energy scale: 10Â¹Â²-10Â¹â¹ eV

**3. Gravitational Wave Signatures:**
- Spectral dimension effects on polarization
- MAGIS-100 sensitivity regime
- Non-commutative spacetime corrections

### Experimental Requirements

**CTA (Cherenkov Telescope Array):**
- Multi-TeV gamma-ray timing precision
- Statistical significance: > 5Ïƒ
- Observation time: 100+ hours

**PVLAS (Polarization of Vacuum with LASer):**
- Magnetic field: B > 5 Tesla
- Laser power: > 100 W
- Ellipticity sensitivity: 10â»â¹ rad

**MAGIS (Matter-wave Atomic Gradiometer):**
- Baseline: 100 m vertical
- Atomic species: Sr-87
- Strain sensitivity: 10â»Â²â° Hzâ»Â¹/Â²

### Theoretical Framework

The NKAT theory unifies:
- Non-commutative geometry (Connes)
- Kolmogorov-Arnold representation
- Dirac spectral triples
- Renormalization group evolution

**Deep Learning Optimization:**
- KAN (Kolmogorov-Arnold Networks) 
- Physics-constrained loss functions
- Spectral dimension convergence
- Î¸-parameter running optimization

### Expected Outcomes

**Scientific Impact:**
- First experimental test of non-commutative spacetime
- Validation of unified field theory
- Nobel Prize-caliber discovery potential

**Technological Applications:**
- Quantum gravity sensors
- Precision metrology advances
- Fundamental physics breakthroughs

### Timeline & Budget

**Phase 1 (6 months): $500K**
- Detailed theoretical predictions
- Sensitivity analysis refinement
- Experimental parameter optimization

**Phase 2 (18 months): $2M**
- CTA observation campaign
- PVLAS precision measurements
- MAGIS prototype testing

**Phase 3 (12 months): $1M**
- Data analysis and interpretation
- Publication in Nature/PRL
- Technology transfer

### Team & Collaboration

**Principal Investigators:**
- Theoretical Physics: NKAT Theory Development
- Experimental Physics: Multi-platform coordination
- Data Science: Deep learning optimization

**International Partners:**
- CTA Consortium
- PVLAS Collaboration  
- MAGIS-100 Team

### Conclusion

The NKAT theory represents a paradigm shift in fundamental physics,
offering the first experimentally testable predictions of quantum gravity
effects. Our deep learning optimization has achieved unprecedented
theoretical precision, positioning this work for immediate experimental
validation and potential Nobel Prize recognition.

**Contact Information:**
- Email: nkat.theory@institution.edu
- ORCID: 0000-0000-0000-0000
- arXiv: physics.gr-qc/2024.xxxxx

---
*Generated by NKAT Hybrid Deep Learning Optimization*
*Final spectral dimension: {final_metrics['final_spectral_dimension']:.3f}*
*Optimization time: {final_metrics['training_time_minutes']:.1f} minutes*
"""
    
    return loi_template

# ===================================================================
# ğŸ”§ é›»æºæ–­å¯¾å¿œ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ»ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
# ===================================================================

class NKATCheckpointManager:
    """NKATé›»æºæ–­å¯¾å¿œãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: HybridNKATConfig):
        self.config = config
        self.checkpoint_dir = config.checkpoint_dir
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        self.last_emergency_save = time.time()
        
    def save_checkpoint(self, epoch, model, optimizer, scheduler, history, config, 
                       is_emergency=False, best_loss=None):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if is_emergency:
            filename = f"emergency_checkpoint_epoch_{epoch}_{timestamp}.pth"
        else:
            filename = f"checkpoint_epoch_{epoch}_{timestamp}.pth"
            
        checkpoint_path = os.path.join(self.checkpoint_dir, filename)
        
        checkpoint_data = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'config': config,
            'history': history,
            'timestamp': timestamp,
            'best_loss': best_loss,
            'random_state': torch.get_rng_state(),
            'cuda_random_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None,
        }
        
        try:
            torch.save(checkpoint_data, checkpoint_path)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            meta_path = os.path.join(self.checkpoint_dir, "latest_checkpoint.json")
            meta_data = {
                'latest_checkpoint': checkpoint_path,
                'epoch': epoch,
                'timestamp': timestamp,
                'spectral_dim': history['spectral_dim_estimates'][-1] if history['spectral_dim_estimates'] else None,
                'total_loss': history['total_loss'][-1] if history['total_loss'] else None
            }
            
            with open(meta_path, 'w', encoding='utf-8') as f:
                json.dump(meta_data, f, indent=2, ensure_ascii=False)
                
            print(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: {checkpoint_path}")
            
            # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤
            if not is_emergency:
                self._cleanup_old_checkpoints()
                
            return checkpoint_path
            
        except Exception as e:
            print(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å¤±æ•—: {str(e)}")
            return None
    
    def load_latest_checkpoint(self):
        """æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
        meta_path = os.path.join(self.checkpoint_dir, "latest_checkpoint.json")
        
        if not os.path.exists(meta_path):
            return None
            
        try:
            with open(meta_path, 'r', encoding='utf-8') as f:
                meta_data = json.load(f)
                
            checkpoint_path = meta_data['latest_checkpoint']
            
            if not os.path.exists(checkpoint_path):
                print(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {checkpoint_path}")
                return None
                
            checkpoint_data = torch.load(checkpoint_path, map_location=device)
            print(f"ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {checkpoint_path}")
            print(f"ğŸ”„ ã‚¨ãƒãƒƒã‚¯ {checkpoint_data['epoch']} ã‹ã‚‰å†é–‹")
            
            return checkpoint_data
            
        except Exception as e:
            print(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {str(e)}")
            return None
    
    def should_emergency_save(self):
        """ç·Šæ€¥ä¿å­˜ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯"""
        if not self.config.auto_backup:
            return False
            
        current_time = time.time()
        elapsed_minutes = (current_time - self.last_emergency_save) / 60
        
        if elapsed_minutes >= self.config.emergency_save_interval:
            self.last_emergency_save = current_time
            return True
            
        return False
    
    def _cleanup_old_checkpoints(self):
        """å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤"""
        try:
            checkpoint_files = []
            for filename in os.listdir(self.checkpoint_dir):
                if filename.startswith("checkpoint_epoch_") and filename.endswith(".pth"):
                    filepath = os.path.join(self.checkpoint_dir, filename)
                    checkpoint_files.append((filepath, os.path.getctime(filepath)))
            
            # ä½œæˆæ™‚é–“ã§ã‚½ãƒ¼ãƒˆ
            checkpoint_files.sort(key=lambda x: x[1], reverse=True)
            
            # æœ€å¤§æ•°ã‚’è¶…ãˆã‚‹å ´åˆã¯å¤ã„ã‚‚ã®ã‚’å‰Šé™¤
            if len(checkpoint_files) > self.config.max_checkpoints:
                for filepath, _ in checkpoint_files[self.config.max_checkpoints:]:
                    os.remove(filepath)
                    print(f"ğŸ—‘ï¸ å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‰Šé™¤: {os.path.basename(filepath)}")
                    
        except Exception as e:
            print(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¤±æ•—: {str(e)}")

def save_optuna_study(study, config):
    """Optunaçµæœã®ä¿å­˜"""
    try:
        study_path = os.path.join(config.checkpoint_dir, "optuna_study.pkl")
        with open(study_path, 'wb') as f:
            pickle.dump(study, f)
        print(f"ğŸ’¾ Optunaçµæœä¿å­˜: {study_path}")
    except Exception as e:
        print(f"âš ï¸ Optunaä¿å­˜å¤±æ•—: {str(e)}")

def load_optuna_study(config):
    """Optunaçµæœã®èª­ã¿è¾¼ã¿"""
    try:
        study_path = os.path.join(config.checkpoint_dir, "optuna_study.pkl")
        if os.path.exists(study_path):
            with open(study_path, 'rb') as f:
                study = pickle.load(f)
            print(f"ğŸ“‚ Optunaçµæœèª­ã¿è¾¼ã¿: {study_path}")
            return study
    except Exception as e:
        print(f"âš ï¸ Optunaèª­ã¿è¾¼ã¿å¤±æ•—: {str(e)}")
    return None

# ===================================================================
# ğŸš€ Main Execution
# ===================================================================

def main():
    """Main execution function with checkpoint recovery support"""
    import argparse
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°è¨­å®š
    parser = argparse.ArgumentParser(description='NKAT Hybrid Deep Learning with Recovery Support')
    parser.add_argument('--resume', action='store_true', 
                       help='ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã™ã‚‹')
    parser.add_argument('--checkpoint-freq', type=int, default=5,
                       help='ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é »åº¦ï¼ˆã‚¨ãƒãƒƒã‚¯ã”ã¨ï¼‰')
    parser.add_argument('--emergency-interval', type=int, default=30,
                       help='ç·Šæ€¥ä¿å­˜é–“éš”ï¼ˆåˆ†ï¼‰')
    parser.add_argument('--checkpoint-dir', type=str, default='./nkat_checkpoints',
                       help='ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--max-checkpoints', type=int, default=10,
                       help='ä¿æŒã™ã‚‹æœ€å¤§ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•°')
    parser.add_argument('--no-optuna', action='store_true',
                       help='Optunaæœ€é©åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹')
    parser.add_argument('--epochs', type=int, default=150,
                       help='å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch-size', type=int, default=24,
                       help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸŒŒ NKAT Hybrid Deep Learning Optimization (é›»æºæ–­å¯¾å¿œç‰ˆ)")
    print("=" * 80)
    
    # Configuration with command line arguments
    config = HybridNKATConfig()
    config.resume_from_checkpoint = args.resume
    config.checkpoint_freq = args.checkpoint_freq
    config.emergency_save_interval = args.emergency_interval
    config.checkpoint_dir = args.checkpoint_dir
    config.max_checkpoints = args.max_checkpoints
    config.num_epochs = args.epochs
    config.batch_size = args.batch_size
    
    # ãƒªã‚«ãƒãƒªãƒ¼æƒ…å ±è¡¨ç¤º
    print(f"ğŸ”§ ãƒªã‚«ãƒãƒªãƒ¼è¨­å®š:")
    print(f"   ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ : {'ON' if config.resume_from_checkpoint else 'OFF'}")
    print(f"   ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé »åº¦: {config.checkpoint_freq}ã‚¨ãƒãƒƒã‚¯æ¯")
    print(f"   ç·Šæ€¥ä¿å­˜é–“éš”: {config.emergency_save_interval}åˆ†æ¯")
    print(f"   ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {config.checkpoint_dir}")
    print(f"   æœ€å¤§ä¿æŒæ•°: {config.max_checkpoints}")
    
    print(f"ğŸ“‹ Hybridè¨­å®š:")
    print(f"   æ ¼å­ã‚µã‚¤ã‚º: {config.grid_size}â´ = {config.grid_size**4:,} ç‚¹")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {config.batch_size}")
    print(f"   ã‚¨ãƒãƒƒã‚¯æ•°: {config.num_epochs}")
    print(f"   ç›®æ¨™ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {config.target_spectral_dim} Â± {config.spectral_dim_tolerance}")
    print(f"   Optuna trials: {config.n_trials} {'(ã‚¹ã‚­ãƒƒãƒ—)' if args.no_optuna else ''}")
    print(f"   KAN layers: {config.kan_layers}")
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çŠ¶æ…‹ç¢ºèª
    if os.path.exists(config.checkpoint_dir):
        checkpoint_files = [f for f in os.listdir(config.checkpoint_dir) 
                          if f.endswith('.pth')]
        print(f"ğŸ’¾ æ—¢å­˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {len(checkpoint_files)}å€‹")
        
        # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæƒ…å ±
        meta_path = os.path.join(config.checkpoint_dir, "latest_checkpoint.json")
        if os.path.exists(meta_path):
            try:
                with open(meta_path, 'r', encoding='utf-8') as f:
                    meta_data = json.load(f)
                print(f"ğŸ“‚ æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: Epoch {meta_data['epoch']}")
                if 'spectral_dim' in meta_data and meta_data['spectral_dim']:
                    print(f"ğŸ“Š å‰å›ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {meta_data['spectral_dim']:.4f}")
            except:
                pass
    else:
        print("ğŸ†• æ–°è¦ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ")
    
    # GPU memory check
    if torch.cuda.is_available():
        print(f"ğŸ’¾ åˆæœŸGPUä½¿ç”¨é‡: {torch.cuda.memory_allocated()/1e9:.2f} GB")
        print(f"ğŸ’¾ GPUç·å®¹é‡: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    start_time = time.time()
    
    try:
        # Hybrid training with recovery support
        model, history, final_config = train_hybrid_nkat(
            config, 
            use_optuna=not args.no_optuna
        )
        
        # Results analysis
        elapsed_time = time.time() - start_time
        final_spec_dim = history['spectral_dim_estimates'][-1]
        final_theta = history['theta_values'][-1]
        improvement = (6.05 - final_spec_dim) / 6.05 * 100
        convergence_achieved = abs(final_spec_dim - config.target_spectral_dim) < config.spectral_dim_tolerance
        
        print("\n" + "="*80)
        print("ğŸ† Hybridè¨“ç·´å®Œäº†ï¼")
        print("="*80)
        print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {elapsed_time/60:.1f} åˆ†")
        print(f"ğŸ¯ æœ€çµ‚ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {final_spec_dim:.3f} (ç›®æ¨™: {config.target_spectral_dim})")
        print(f"ğŸ“ æœ€çµ‚Î¸å€¤: {final_theta:.2e}")
        print(f"ğŸ“ˆ æ”¹å–„åº¦: {improvement:.1f}%")
        print(f"âœ… åæŸåˆ¤å®š: {'æˆåŠŸ' if convergence_achieved else 'ç¶™ç¶šå¿…è¦'}")
        
        if torch.cuda.is_available():
            print(f"ğŸ’¾ æœ€çµ‚GPUä½¿ç”¨é‡: {torch.cuda.memory_allocated()/1e9:.2f} GB")
        
        # Save results
        final_metrics = {
            'final_spectral_dimension': final_spec_dim,
            'target_spectral_dimension': config.target_spectral_dim,
            'final_theta': final_theta,
            'improvement_percentage': improvement,
            'convergence_achieved': convergence_achieved,
            'training_time_minutes': elapsed_time / 60,
            'config': {
                'grid_size': final_config.grid_size,
                'batch_size': final_config.batch_size,
                'learning_rate': final_config.learning_rate,
                'kan_layers': final_config.kan_layers,
                'n_trials': final_config.n_trials
            },
            'recovery_info': {
                'checkpoints_used': config.resume_from_checkpoint,
                'checkpoint_freq': config.checkpoint_freq,
                'emergency_saves': config.emergency_save_interval
            }
        }
        
        # Save model and results
        final_model_path = os.path.join(work_dir, 'nkat_hybrid_final_model.pth')
        torch.save({
            'model_state_dict': model.state_dict(),
            'history': history,
            'config': final_config,
            'final_metrics': final_metrics
        }, final_model_path)
        print(f"ğŸ’¾ Hybridãƒ¢ãƒ‡ãƒ«ä¿å­˜: {final_model_path}")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½¿ç”¨çŠ¶æ³ã‚µãƒãƒªãƒ¼
        if os.path.exists(config.checkpoint_dir):
            checkpoint_files = [f for f in os.listdir(config.checkpoint_dir) 
                              if f.endswith('.pth')]
            print(f"ğŸ”§ ç·ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•°: {len(checkpoint_files)}")
            
        # ãƒ¬ã‚¸ãƒ¥ãƒ¼ãƒ æ¨å¥¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if not convergence_achieved:
            print("\nğŸ”„ å­¦ç¿’ç¶™ç¶šæ¨å¥¨:")
            print(f"   æ¬¡å›å®Ÿè¡Œæ™‚ã« --resume ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ç¶™ç¶šã§ãã¾ã™")
            print(f"   ã‚³ãƒãƒ³ãƒ‰ä¾‹: py -3 {os.path.basename(__file__)} --resume")
        
        # Generate visualizations
        plot_results_path = os.path.join(work_dir, 'nkat_hybrid_results.png')
        plot_hybrid_results(history, final_config, plot_results_path)
        
        # Generate LoI template
        loi_template = generate_loi_template(final_metrics, final_config)
        loi_path = os.path.join(work_dir, 'NKAT_LoI_Template.md')
        with open(loi_path, 'w', encoding='utf-8') as f:
            f.write(loi_template)
        print(f"ğŸ“ LoI ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ: {loi_path}")
        
        # Save comprehensive results
        results_path = os.path.join(work_dir, 'nkat_hybrid_results.json')
        with open(results_path, 'w') as f:
            json.dump(final_metrics, f, indent=2)
        print(f"ğŸ“Š çµæœã‚µãƒãƒªãƒ¼ä¿å­˜: {results_path}")
        
        # Success evaluation
        if convergence_achieved:
            print("\nğŸŠ âœ… å®Ÿé¨“ææ¡ˆæ›¸ä½œæˆæº–å‚™å®Œäº†ï¼")
            print("ğŸ¯ CTA/PVLAS/MAGISæ„Ÿåº¦è§£æå¯èƒ½")
            print("ğŸ“š Nature/PRLç´šè«–æ–‡åŸ·ç­†æº–å‚™å®Œäº†")
            print("ğŸ† Nobel Prize Track Theoryç¢ºç«‹")
        elif improvement > 20:
            print("\nğŸ”„ å¤§å¹…æ”¹å–„é”æˆï¼è¿½åŠ æœ€é©åŒ–æ¨å¥¨")
            print("ğŸ“Š æ ¼å­ã‚µã‚¤ã‚ºæ‹¡å¤§ã¾ãŸã¯ã‚¨ãƒãƒƒã‚¯æ•°å¢—åŠ ")
        else:
            print("\nğŸ”§ ãƒ¢ãƒ‡ãƒ«æ”¹è‰¯ç¶™ç¶šå¿…è¦")
            print("ğŸ“ Optuna trialså¢—åŠ ã¾ãŸã¯æ–°ã—ã„lossè¨­è¨ˆ")
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)}")
        print("ğŸ’¡ ãƒ¡ãƒ¢ãƒªä¸è¶³ã¾ãŸã¯cudaç’°å¢ƒã®å•é¡Œå¯èƒ½æ€§")
        
        # Error cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPUãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
    
    # Final summary
    print("\n" + "="*70)
    print("ğŸŒŒ NKAT Hybrid Deep Learning Optimization å®Œäº†")
    print("="*70)
    
    if IN_COLAB:
        if work_dir.startswith('/content/drive'):
            print("ğŸ“‚ çµæœã¯Google Driveã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            print("ğŸ”— /content/drive/MyDrive/NKAT_Hybrid_Results/")
        else:
            print("ğŸ“‚ çµæœã¯Colabãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
            print(f"ğŸ”— {work_dir}/")
            print("âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ¶ˆãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
    else:
        print(f"ğŸ“‚ çµæœã¯ {work_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")
    
    print("ğŸŠ Hybridæœ€é©åŒ–å®Œäº†ï¼æ¬¡ã¯å®Ÿé¨“ææ¡ˆæ›¸ä½œæˆã ï¼")

if __name__ == "__main__":
    main() 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ NKAT v9.1 - 10,000Î³ Challenge å …ç‰¢ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
Robust Recovery System for 10,000 Gamma Challenge with RTX3080

Author: NKAT Research Consortium
Date: 2025-05-26
Version: 9.1 - Ultimate Robustness
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Any
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass, asdict
from tqdm import tqdm, trange
import logging
import pickle
import hashlib
from datetime import datetime
import threading
import queue
import signal
import sys
import os
import shutil
import psutil
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

@dataclass
class CheckpointData:
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    batch_id: int
    gamma_start_idx: int
    gamma_end_idx: int
    completed_gammas: List[float]
    results: List[Dict]
    timestamp: str
    system_state: Dict
    memory_usage: float
    gpu_memory: float
    total_progress: float

@dataclass
class SystemMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ‡ãƒ¼ã‚¿"""
    cpu_percent: float
    memory_percent: float
    gpu_memory_used: float
    gpu_memory_total: float
    temperature: Optional[float]
    power_draw: Optional[float]
    timestamp: str

class RobustRecoveryManager:
    """å …ç‰¢ãƒªã‚«ãƒãƒªãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, checkpoint_dir: str = "10k_gamma_checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.backup_dir = self.checkpoint_dir / "backups"
        self.backup_dir.mkdir(exist_ok=True)
        
        # ç·Šæ€¥åœæ­¢ãƒ•ãƒ©ã‚°
        self.emergency_stop = False
        
        # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
        self.monitoring_active = True
        self.monitor_queue = queue.Queue()
        
        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        logger.info(f"ğŸ›¡ï¸ å …ç‰¢ãƒªã‚«ãƒãƒªãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–: {self.checkpoint_dir}")
    
    def _signal_handler(self, signum, frame):
        """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆç·Šæ€¥åœæ­¢ï¼‰"""
        logger.warning(f"âš ï¸ ã‚·ã‚°ãƒŠãƒ« {signum} ã‚’å—ä¿¡ã€‚ç·Šæ€¥åœæ­¢ã‚’é–‹å§‹...")
        self.emergency_stop = True
        self.monitoring_active = False
    
    def save_checkpoint(self, checkpoint_data: CheckpointData) -> bool:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            checkpoint_file = self.checkpoint_dir / f"checkpoint_batch_{checkpoint_data.batch_id}_{timestamp}.pkl"
            
            # ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
            with open(checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            
            # JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚‚ä½œæˆ
            json_file = self.checkpoint_dir / f"checkpoint_batch_{checkpoint_data.batch_id}_{timestamp}.json"
            json_data = asdict(checkpoint_data)
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)
            
            # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæœ€æ–°5å€‹ã‚’ä¿æŒï¼‰
            self._cleanup_old_checkpoints(checkpoint_data.batch_id)
            
            logger.info(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: batch_{checkpoint_data.batch_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_latest_checkpoint(self) -> Optional[CheckpointData]:
        """æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        try:
            checkpoint_files = list(self.checkpoint_dir.glob("checkpoint_batch_*.pkl"))
            if not checkpoint_files:
                logger.info("ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
            latest_file = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
            
            with open(latest_file, 'rb') as f:
                checkpoint_data = pickle.load(f)
            
            logger.info(f"ğŸ“¥ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {latest_file.name}")
            return checkpoint_data
            
        except Exception as e:
            logger.error(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _cleanup_old_checkpoints(self, batch_id: int, keep_count: int = 5):
        """å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            pattern = f"checkpoint_batch_{batch_id}_*.pkl"
            checkpoint_files = list(self.checkpoint_dir.glob(pattern))
            
            if len(checkpoint_files) > keep_count:
                # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                sorted_files = sorted(checkpoint_files, key=lambda x: x.stat().st_mtime)
                for old_file in sorted_files[:-keep_count]:
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«ç§»å‹•
                    backup_file = self.backup_dir / old_file.name
                    shutil.move(str(old_file), str(backup_file))
                    
                    # å¯¾å¿œã™ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç§»å‹•
                    json_file = old_file.with_suffix('.json')
                    if json_file.exists():
                        backup_json = self.backup_dir / json_file.name
                        shutil.move(str(json_file), str(backup_json))
                
                logger.info(f"ğŸ§¹ å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {len(sorted_files) - keep_count}å€‹")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def monitor_system(self) -> SystemMonitor:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ç›£è¦–"""
        try:
            # CPUãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            # GPUæƒ…å ±
            if torch.cuda.is_available():
                gpu_memory_used = torch.cuda.memory_allocated() / 1e9
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
            else:
                gpu_memory_used = 0
                gpu_memory_total = 0
            
            # æ¸©åº¦æƒ…å ±ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            temperature = None
            power_draw = None
            try:
                if hasattr(psutil, "sensors_temperatures"):
                    temps = psutil.sensors_temperatures()
                    if temps:
                        temperature = list(temps.values())[0][0].current
            except:
                pass
            
            return SystemMonitor(
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                gpu_memory_used=gpu_memory_used,
                gpu_memory_total=gpu_memory_total,
                temperature=temperature,
                power_draw=power_draw,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
            return SystemMonitor(0, 0, 0, 0, None, None, datetime.now().isoformat())
    
    def check_system_health(self, monitor: SystemMonitor) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        warnings = []
        
        # CPUä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if monitor.cpu_percent > 95:
            warnings.append(f"é«˜CPUä½¿ç”¨ç‡: {monitor.cpu_percent:.1f}%")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if monitor.memory_percent > 90:
            warnings.append(f"é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {monitor.memory_percent:.1f}%")
        
        # GPU ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
        if monitor.gpu_memory_total > 0:
            gpu_usage_percent = (monitor.gpu_memory_used / monitor.gpu_memory_total) * 100
            if gpu_usage_percent > 95:
                warnings.append(f"é«˜GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {gpu_usage_percent:.1f}%")
        
        # æ¸©åº¦ãƒã‚§ãƒƒã‚¯
        if monitor.temperature and monitor.temperature > 85:
            warnings.append(f"é«˜æ¸©åº¦: {monitor.temperature:.1f}Â°C")
        
        if warnings:
            logger.warning(f"âš ï¸ ã‚·ã‚¹ãƒ†ãƒ è­¦å‘Š: {', '.join(warnings)}")
            return False
        
        return True

class NKAT10KGammaChallenge:
    """10,000Î³ãƒãƒ£ãƒ¬ãƒ³ã‚¸å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, recovery_manager: RobustRecoveryManager):
        self.recovery_manager = recovery_manager
        self.device = device
        self.dtype = torch.complex128
        
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¨­å®š
        self.max_n = 2048  # RTX3080ã«æœ€é©åŒ–
        self.theta = 1e-25
        self.kappa = 1e-15
        
        # ãƒãƒƒãƒè¨­å®š
        self.batch_size = 100  # 100Î³å€¤ãšã¤å‡¦ç†
        self.total_gammas = 10000
        
        # çµæœä¿å­˜
        self.results_dir = Path("10k_gamma_results")
        self.results_dir.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ¯ 10,000Î³ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def generate_gamma_values(self, count: int = 10000) -> List[float]:
        """10,000å€‹ã®Î³å€¤ç”Ÿæˆ"""
        gamma_values = []
        
        # æ—¢çŸ¥ã®é«˜ç²¾åº¦ã‚¼ãƒ­ç‚¹ï¼ˆæœ€åˆã®100å€‹ï¼‰
        known_zeros = [
            14.134725, 21.022040, 25.010858, 30.424876, 32.935062,
            37.586178, 40.918719, 43.327073, 48.005151, 49.773832,
            52.970321, 56.446247, 59.347044, 60.831778, 65.112544,
            67.079810, 69.546401, 72.067158, 75.704690, 77.144840,
            79.337375, 82.910381, 84.735493, 87.425275, 88.809111,
            92.491899, 94.651344, 95.870634, 98.831194, 101.317851
        ]
        
        # æ—¢çŸ¥ã®ã‚¼ãƒ­ç‚¹ã‚’æ‹¡å¼µ
        for i in range(100):
            if i < len(known_zeros):
                gamma_values.append(known_zeros[i])
            else:
                # æ•°å­¦çš„è£œé–“
                gamma_values.append(14.134725 + i * 2.5 + np.random.normal(0, 0.1))
        
        # ä¸­é–“ç¯„å›²ã®å€¤ï¼ˆ100-1000ï¼‰
        for i in range(900):
            base_gamma = 100 + i * 0.5
            gamma_values.append(base_gamma + np.random.normal(0, 0.05))
        
        # é«˜ç¯„å›²ã®å€¤ï¼ˆ1000-10000ï¼‰
        for i in range(9000):
            base_gamma = 500 + i * 0.1
            gamma_values.append(base_gamma + np.random.normal(0, 0.02))
        
        # ã‚½ãƒ¼ãƒˆã—ã¦é‡è¤‡é™¤å»
        gamma_values = sorted(list(set(gamma_values)))
        
        # æ­£ç¢ºã«10,000å€‹ã«èª¿æ•´
        if len(gamma_values) > count:
            gamma_values = gamma_values[:count]
        elif len(gamma_values) < count:
            # ä¸è¶³åˆ†ã‚’è£œå®Œ
            while len(gamma_values) < count:
                last_gamma = gamma_values[-1]
                gamma_values.append(last_gamma + 0.1 + np.random.normal(0, 0.01))
        
        logger.info(f"ğŸ“Š 10,000Î³å€¤ç”Ÿæˆå®Œäº†: ç¯„å›² [{min(gamma_values):.3f}, {max(gamma_values):.3f}]")
        return gamma_values
    
    def construct_hamiltonian(self, s: complex) -> torch.Tensor:
        """é«˜åŠ¹ç‡ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³æ§‹ç¯‰"""
        dim = min(self.max_n, 1024)  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®
        
        H = torch.zeros(dim, dim, dtype=self.dtype, device=self.device)
        
        # ä¸»å¯¾è§’é …ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
        n_values = torch.arange(1, dim + 1, dtype=torch.float64, device=self.device)
        try:
            diagonal_values = 1.0 / (n_values ** s)
            H.diagonal().copy_(diagonal_values.to(self.dtype))
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            for n in range(1, dim + 1):
                try:
                    H[n-1, n-1] = torch.tensor(1.0 / (n ** s), dtype=self.dtype, device=self.device)
                except:
                    H[n-1, n-1] = torch.tensor(1e-50, dtype=self.dtype, device=self.device)
        
        # æ­£å‰‡åŒ–
        regularization = torch.tensor(1e-12, dtype=self.dtype, device=self.device)
        H += regularization * torch.eye(dim, dtype=self.dtype, device=self.device)
        
        return H
    
    def compute_spectral_dimension(self, s: complex) -> float:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        try:
            H = self.construct_hamiltonian(s)
            
            # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–
            H_hermitian = 0.5 * (H + H.conj().T)
            
            # å›ºæœ‰å€¤è¨ˆç®—ï¼ˆä¸Šä½ã®ã¿ï¼‰
            eigenvals = torch.linalg.eigvals(H_hermitian).real
            eigenvals = eigenvals[eigenvals > 1e-15]
            
            if len(eigenvals) < 10:
                return float('nan')
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
            eigenvals_sorted = torch.sort(eigenvals, descending=True)[0]
            top_eigenvals = eigenvals_sorted[:min(50, len(eigenvals_sorted))]
            
            # å¯¾æ•°å¾®åˆ†è¿‘ä¼¼
            log_eigenvals = torch.log(top_eigenvals + 1e-15)
            indices = torch.arange(1, len(top_eigenvals) + 1, dtype=torch.float64, device=self.device)
            log_indices = torch.log(indices)
            
            # ç·šå½¢å›å¸°
            A = torch.stack([log_indices, torch.ones_like(log_indices)], dim=1)
            solution = torch.linalg.lstsq(A, log_eigenvals).solution
            slope = solution[0]
            
            spectral_dimension = -2 * slope.item()
            
            if abs(spectral_dimension) > 20 or not np.isfinite(spectral_dimension):
                return float('nan')
            
            return spectral_dimension
            
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('nan')
    
    def process_gamma_batch(self, gamma_batch: List[float], batch_id: int) -> List[Dict]:
        """Î³å€¤ãƒãƒƒãƒã®å‡¦ç†"""
        results = []
        
        for i, gamma in enumerate(gamma_batch):
            if self.recovery_manager.emergency_stop:
                logger.warning("âš ï¸ ç·Šæ€¥åœæ­¢ãŒè¦æ±‚ã•ã‚Œã¾ã—ãŸ")
                break
            
            try:
                s = 0.5 + 1j * gamma
                
                # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
                d_s = self.compute_spectral_dimension(s)
                
                # çµæœè¨˜éŒ²
                result = {
                    'gamma': gamma,
                    'spectral_dimension': d_s,
                    'real_part': d_s / 2 if not np.isnan(d_s) else np.nan,
                    'convergence_to_half': abs(d_s / 2 - 0.5) if not np.isnan(d_s) else np.nan,
                    'timestamp': datetime.now().isoformat(),
                    'batch_id': batch_id,
                    'batch_index': i
                }
                
                results.append(result)
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
                if (i + 1) % 10 == 0:
                    logger.info(f"ğŸ“Š Batch {batch_id}: {i + 1}/{len(gamma_batch)} å®Œäº†")
                
            except Exception as e:
                logger.error(f"âŒ Î³={gamma} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚çµæœã‚’è¨˜éŒ²
                results.append({
                    'gamma': gamma,
                    'spectral_dimension': np.nan,
                    'real_part': np.nan,
                    'convergence_to_half': np.nan,
                    'timestamp': datetime.now().isoformat(),
                    'batch_id': batch_id,
                    'batch_index': i,
                    'error': str(e)
                })
        
        return results
    
    def execute_10k_challenge(self, resume: bool = True) -> Dict:
        """10,000Î³ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã®å®Ÿè¡Œ"""
        print("=" * 80)
        print("ğŸš€ NKAT v9.1 - 10,000Î³ Challenge é–‹å§‹")
        print("=" * 80)
        print(f"ğŸ“… é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ ç›®æ¨™: 10,000Î³å€¤ã®æ¤œè¨¼")
        print(f"ğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}")
        print(f"ğŸ›¡ï¸ ãƒªã‚«ãƒãƒªãƒ¼æ©Ÿèƒ½: æœ‰åŠ¹")
        print("=" * 80)
        
        start_time = time.time()
        
        # Î³å€¤ç”Ÿæˆ
        all_gamma_values = self.generate_gamma_values(self.total_gammas)
        
        # å¾©æ—§ãƒã‚§ãƒƒã‚¯
        checkpoint_data = None
        start_batch = 0
        all_results = []
        
        if resume:
            checkpoint_data = self.recovery_manager.load_latest_checkpoint()
            if checkpoint_data:
                start_batch = checkpoint_data.batch_id + 1
                all_results = checkpoint_data.results
                logger.info(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©æ—§: Batch {start_batch} ã‹ã‚‰å†é–‹")
        
        # ãƒãƒƒãƒå‡¦ç†
        total_batches = (self.total_gammas + self.batch_size - 1) // self.batch_size
        
        for batch_id in range(start_batch, total_batches):
            if self.recovery_manager.emergency_stop:
                logger.warning("âš ï¸ ç·Šæ€¥åœæ­¢ã«ã‚ˆã‚Šå‡¦ç†ã‚’ä¸­æ–­")
                break
            
            # ãƒãƒƒãƒç¯„å›²è¨ˆç®—
            start_idx = batch_id * self.batch_size
            end_idx = min(start_idx + self.batch_size, self.total_gammas)
            gamma_batch = all_gamma_values[start_idx:end_idx]
            
            logger.info(f"ğŸ”„ Batch {batch_id + 1}/{total_batches} é–‹å§‹: Î³[{start_idx}:{end_idx}]")
            
            # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
            monitor = self.recovery_manager.monitor_system()
            if not self.recovery_manager.check_system_health(monitor):
                logger.warning("âš ï¸ ã‚·ã‚¹ãƒ†ãƒ è² è·ãŒé«˜ã„ãŸã‚ã€5ç§’å¾…æ©Ÿ...")
                time.sleep(5)
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            try:
                batch_results = self.process_gamma_batch(gamma_batch, batch_id)
                all_results.extend(batch_results)
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                checkpoint = CheckpointData(
                    batch_id=batch_id,
                    gamma_start_idx=start_idx,
                    gamma_end_idx=end_idx,
                    completed_gammas=gamma_batch,
                    results=all_results,
                    timestamp=datetime.now().isoformat(),
                    system_state=asdict(monitor),
                    memory_usage=monitor.memory_percent,
                    gpu_memory=monitor.gpu_memory_used,
                    total_progress=(batch_id + 1) / total_batches * 100
                )
                
                self.recovery_manager.save_checkpoint(checkpoint)
                
                # ä¸­é–“çµæœä¿å­˜
                if (batch_id + 1) % 10 == 0:  # 10ãƒãƒƒãƒã”ã¨
                    self._save_intermediate_results(all_results, batch_id + 1)
                
                logger.info(f"âœ… Batch {batch_id + 1} å®Œäº†: {len(batch_results)}å€‹ã®çµæœ")
                
            except Exception as e:
                logger.error(f"âŒ Batch {batch_id} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                logger.error(traceback.format_exc())
                
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ç¶™ç¶šï¼ˆå …ç‰¢æ€§ï¼‰
                continue
        
        # æœ€çµ‚çµæœã®è¨ˆç®—
        execution_time = time.time() - start_time
        
        # çµ±è¨ˆè¨ˆç®—
        valid_results = [r for r in all_results if not np.isnan(r.get('spectral_dimension', np.nan))]
        
        final_results = {
            'timestamp': datetime.now().isoformat(),
            'total_gammas_processed': len(all_results),
            'valid_results': len(valid_results),
            'execution_time_seconds': execution_time,
            'execution_time_formatted': f"{execution_time // 3600:.0f}h {(execution_time % 3600) // 60:.0f}m {execution_time % 60:.1f}s",
            'processing_speed_per_gamma': execution_time / len(all_results) if all_results else 0,
            'success_rate': len(valid_results) / len(all_results) if all_results else 0,
            'results': all_results
        }
        
        if valid_results:
            spectral_dims = [r['spectral_dimension'] for r in valid_results]
            convergences = [r['convergence_to_half'] for r in valid_results if not np.isnan(r['convergence_to_half'])]
            
            final_results.update({
                'statistics': {
                    'mean_spectral_dimension': np.mean(spectral_dims),
                    'std_spectral_dimension': np.std(spectral_dims),
                    'mean_convergence': np.mean(convergences) if convergences else np.nan,
                    'best_convergence': np.min(convergences) if convergences else np.nan,
                    'worst_convergence': np.max(convergences) if convergences else np.nan
                }
            })
        
        # æœ€çµ‚çµæœä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_file = self.results_dir / f"10k_gamma_final_results_{timestamp}.json"
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ 10,000Î³ Challenge å®Œäº†ï¼")
        print("=" * 80)
        print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿Î³å€¤: {len(all_results):,}")
        print(f"âœ… æœ‰åŠ¹çµæœ: {len(valid_results):,}")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {final_results['execution_time_formatted']}")
        print(f"ğŸš€ å‡¦ç†é€Ÿåº¦: {final_results['processing_speed_per_gamma']:.4f}ç§’/Î³å€¤")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {final_results['success_rate']:.1%}")
        print(f"ğŸ’¾ çµæœä¿å­˜: {final_file}")
        print("=" * 80)
        
        return final_results
    
    def _save_intermediate_results(self, results: List[Dict], batch_count: int):
        """ä¸­é–“çµæœã®ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            intermediate_file = self.results_dir / f"intermediate_results_batch_{batch_count}_{timestamp}.json"
            
            intermediate_data = {
                'timestamp': datetime.now().isoformat(),
                'batches_completed': batch_count,
                'total_results': len(results),
                'results': results
            }
            
            with open(intermediate_file, 'w', encoding='utf-8') as f:
                json.dump(intermediate_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ’¾ ä¸­é–“çµæœä¿å­˜: {intermediate_file}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä¸­é–“çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # ãƒªã‚«ãƒãƒªãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        recovery_manager = RobustRecoveryManager()
        
        # 10Kãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        challenge_system = NKAT10KGammaChallenge(recovery_manager)
        
        # ãƒãƒ£ãƒ¬ãƒ³ã‚¸å®Ÿè¡Œ
        results = challenge_system.execute_10k_challenge(resume=True)
        
        print("ğŸ‰ NKAT v9.1 - 10,000Î³ Challenge æˆåŠŸï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    main() 
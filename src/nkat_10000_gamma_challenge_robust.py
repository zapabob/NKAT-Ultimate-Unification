#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ NKAT v9.1 - 10,000Î³ Challenge å …ç‰¢ãƒªã‚«ãƒãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
Robust Recovery System for 10,000 Gamma Challenge with RTX3080

Author: NKAT Research Consortium
Date: 2025-05-26
Version: 9.1 - Ultimate Robustness
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Any
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass, asdict
from tqdm import tqdm, trange
import logging
import pickle
import hashlib
from datetime import datetime
import threading
import queue
import signal
import sys
import os
import shutil
import psutil
import gc
from concurrent.futures import ThreadPoolExecutor, as_completed
import traceback
import glob

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

@dataclass
class CheckpointData:
    """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    batch_id: int
    gamma_start_idx: int
    gamma_end_idx: int
    completed_gammas: List[float]
    results: List[Dict]
    timestamp: str
    system_state: Dict
    memory_usage: float
    gpu_memory: float
    total_progress: float

@dataclass
class SystemMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ãƒ‡ãƒ¼ã‚¿"""
    cpu_percent: float
    memory_percent: float
    gpu_memory_used: float
    gpu_memory_total: float
    temperature: Optional[float]
    power_draw: Optional[float]
    timestamp: str

class RobustRecoveryManager:
    """å …ç‰¢ãƒªã‚«ãƒãƒªãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, checkpoint_dir: str = "10k_gamma_checkpoints"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.backup_dir = self.checkpoint_dir / "backups"
        self.backup_dir.mkdir(exist_ok=True)
        
        # ç·Šæ€¥åœæ­¢ãƒ•ãƒ©ã‚°
        self.emergency_stop = False
        
        # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
        self.monitoring_active = True
        self.monitor_queue = queue.Queue()
        
        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼è¨­å®š
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        logger.info(f"ğŸ›¡ï¸ å …ç‰¢ãƒªã‚«ãƒãƒªãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–: {self.checkpoint_dir}")
    
    def _signal_handler(self, signum, frame):
        """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆç·Šæ€¥åœæ­¢ï¼‰"""
        logger.warning(f"âš ï¸ ã‚·ã‚°ãƒŠãƒ« {signum} ã‚’å—ä¿¡ã€‚ç·Šæ€¥åœæ­¢ã‚’é–‹å§‹...")
        self.emergency_stop = True
        self.monitoring_active = False
    
    def save_checkpoint(self, checkpoint_data: CheckpointData) -> bool:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            checkpoint_file = self.checkpoint_dir / f"checkpoint_batch_{checkpoint_data.batch_id}_{timestamp}.pkl"
            
            # ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
            with open(checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            
            # JSONãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚‚ä½œæˆ
            json_file = self.checkpoint_dir / f"checkpoint_batch_{checkpoint_data.batch_id}_{timestamp}.json"
            json_data = asdict(checkpoint_data)
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False, default=str)
            
            # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæœ€æ–°5å€‹ã‚’ä¿æŒï¼‰
            self._cleanup_old_checkpoints(checkpoint_data.batch_id)
            
            logger.info(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: batch_{checkpoint_data.batch_id}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_latest_checkpoint(self) -> Optional[CheckpointData]:
        """æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        try:
            checkpoint_files = list(self.checkpoint_dir.glob("checkpoint_batch_*.pkl"))
            if not checkpoint_files:
                logger.info("ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
            latest_file = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
            
            with open(latest_file, 'rb') as f:
                checkpoint_data = pickle.load(f)
            
            logger.info(f"ğŸ“¥ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {latest_file.name}")
            return checkpoint_data
            
        except Exception as e:
            logger.error(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _cleanup_old_checkpoints(self, batch_id: int, keep_count: int = 5):
        """å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            pattern = f"checkpoint_batch_{batch_id}_*.pkl"
            checkpoint_files = list(self.checkpoint_dir.glob(pattern))
            
            if len(checkpoint_files) > keep_count:
                # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                sorted_files = sorted(checkpoint_files, key=lambda x: x.stat().st_mtime)
                for old_file in sorted_files[:-keep_count]:
                    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã«ç§»å‹•
                    backup_file = self.backup_dir / old_file.name
                    shutil.move(str(old_file), str(backup_file))
                    
                    # å¯¾å¿œã™ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç§»å‹•
                    json_file = old_file.with_suffix('.json')
                    if json_file.exists():
                        backup_json = self.backup_dir / json_file.name
                        shutil.move(str(json_file), str(backup_json))
                
                logger.info(f"ğŸ§¹ å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: {len(sorted_files) - keep_count}å€‹")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def monitor_system(self) -> SystemMonitor:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®ç›£è¦–"""
        try:
            # CPUãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            # GPUæƒ…å ±
            if torch.cuda.is_available():
                gpu_memory_used = torch.cuda.memory_allocated() / 1e9
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
            else:
                gpu_memory_used = 0
                gpu_memory_total = 0
            
            # æ¸©åº¦æƒ…å ±ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            temperature = None
            power_draw = None
            try:
                if hasattr(psutil, "sensors_temperatures"):
                    temps = psutil.sensors_temperatures()
                    if temps:
                        temperature = list(temps.values())[0][0].current
            except:
                pass
            
            return SystemMonitor(
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                gpu_memory_used=gpu_memory_used,
                gpu_memory_total=gpu_memory_total,
                temperature=temperature,
                power_draw=power_draw,
                timestamp=datetime.now().isoformat()
            )
            
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
            return SystemMonitor(0, 0, 0, 0, None, None, datetime.now().isoformat())
    
    def check_system_health(self, monitor: SystemMonitor) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        warnings = []
        
        # CPUä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if monitor.cpu_percent > 95:
            warnings.append(f"é«˜CPUä½¿ç”¨ç‡: {monitor.cpu_percent:.1f}%")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãƒã‚§ãƒƒã‚¯
        if monitor.memory_percent > 90:
            warnings.append(f"é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {monitor.memory_percent:.1f}%")
        
        # GPU ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
        if monitor.gpu_memory_total > 0:
            gpu_usage_percent = (monitor.gpu_memory_used / monitor.gpu_memory_total) * 100
            if gpu_usage_percent > 95:
                warnings.append(f"é«˜GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {gpu_usage_percent:.1f}%")
        
        # æ¸©åº¦ãƒã‚§ãƒƒã‚¯
        if monitor.temperature and monitor.temperature > 85:
            warnings.append(f"é«˜æ¸©åº¦: {monitor.temperature:.1f}Â°C")
        
        if warnings:
            logger.warning(f"âš ï¸ ã‚·ã‚¹ãƒ†ãƒ è­¦å‘Š: {', '.join(warnings)}")
            return False
        
        return True

class NKAT10KGammaChallenge:
    """10,000Î³ãƒãƒ£ãƒ¬ãƒ³ã‚¸å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, recovery_manager: RobustRecoveryManager):
        self.recovery_manager = recovery_manager
        self.device = device
        self.dtype = torch.complex128
        
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¨­å®š
        self.max_n = 2048  # RTX3080ã«æœ€é©åŒ–
        self.theta = 1e-25
        self.kappa = 1e-15
        
        # ãƒãƒƒãƒè¨­å®š
        self.batch_size = 100  # 100Î³å€¤ãšã¤å‡¦ç†
        self.total_gammas = 10000
        
        # çµæœä¿å­˜
        self.results_dir = Path("10k_gamma_results")
        self.results_dir.mkdir(exist_ok=True)
        
        logger.info(f"ğŸ¯ 10,000Î³ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
    
    def load_latest_gamma_data(self) -> Optional[Dict]:
        """æœ€æ–°ã®Î³å€¤ãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æ¤œå‡ºãƒ»èª­ã¿è¾¼ã¿"""
        try:
            # æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«å„ªå…ˆï¼‰
            search_patterns = [
                # æœ€æ–°ã®10k_gamma_results
                "../../10k_gamma_results/10k_gamma_final_results_*.json",
                "../10k_gamma_results/10k_gamma_final_results_*.json", 
                "10k_gamma_results/10k_gamma_final_results_*.json",
                # ä¸­é–“çµæœãƒ•ã‚¡ã‚¤ãƒ«
                "../../10k_gamma_results/intermediate_results_batch_*.json",
                "../10k_gamma_results/intermediate_results_batch_*.json",
                "10k_gamma_results/intermediate_results_batch_*.json",
                # ãã®ä»–ã®ãƒªãƒ¼ãƒãƒ³çµæœ
                "../../rtx3080_extreme_riemann_results_*.json",
                "../rtx3080_extreme_riemann_results_*.json",
                "rtx3080_extreme_riemann_results_*.json",
                "../../ultimate_mastery_riemann_results.json",
                "../ultimate_mastery_riemann_results.json",
                "ultimate_mastery_riemann_results.json"
            ]
            
            found_files = []
            
            # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            for pattern in search_patterns:
                matches = glob.glob(pattern)
                for match in matches:
                    file_path = Path(match)
                    if file_path.exists() and file_path.stat().st_size > 1000:  # 1KBä»¥ä¸Š
                        found_files.append((file_path, file_path.stat().st_mtime))
            
            if not found_files:
                logger.warning("âš ï¸ æ—¢å­˜ã®Î³å€¤ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return None
            
            # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
            latest_file = max(found_files, key=lambda x: x[1])[0]
            
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            logger.info(f"ğŸ“Š æœ€æ–°Î³å€¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æˆåŠŸ: {latest_file}")
            logger.info(f"ğŸ“ˆ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {latest_file.stat().st_size / 1024:.1f} KB")
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æ¤œè¨¼
            if 'results' in data:
                results_count = len(data['results'])
                logger.info(f"ğŸ“Š èª­ã¿è¾¼ã¿çµæœæ•°: {results_count:,}")
                
                # æœ‰åŠ¹ãªçµæœã®çµ±è¨ˆ
                valid_results = [r for r in data['results'] if 'gamma' in r and not np.isnan(r.get('spectral_dimension', np.nan))]
                logger.info(f"âœ… æœ‰åŠ¹çµæœæ•°: {len(valid_results):,}")
                
                return data
            else:
                logger.warning(f"âš ï¸ ä¸æ˜ãªãƒ‡ãƒ¼ã‚¿å½¢å¼: {latest_file}")
                return data
                
        except Exception as e:
            logger.error(f"âŒ Î³å€¤ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_completed_gammas(self, existing_data: Dict) -> List[float]:
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Œäº†æ¸ˆã¿Î³å€¤ã‚’æŠ½å‡º"""
        completed_gammas = []
        
        if 'results' in existing_data:
            for result in existing_data['results']:
                if 'gamma' in result and 'spectral_dimension' in result:
                    # æœ‰åŠ¹ãªçµæœã®ã¿ã‚’å®Œäº†æ¸ˆã¿ã¨ã—ã¦æ‰±ã†
                    if not np.isnan(result.get('spectral_dimension', np.nan)):
                        completed_gammas.append(result['gamma'])
        
        logger.info(f"ğŸ“Š å®Œäº†æ¸ˆã¿Î³å€¤: {len(completed_gammas):,}å€‹")
        return sorted(completed_gammas)
    
    def generate_gamma_values(self, count: int = 10000, exclude_completed: bool = True) -> List[float]:
        """10,000å€‹ã®Î³å€¤ç”Ÿæˆï¼ˆå®Œäº†æ¸ˆã¿é™¤å¤–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãï¼‰"""
        gamma_values = []
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        completed_gammas = set()
        if exclude_completed:
            existing_data = self.load_latest_gamma_data()
            if existing_data:
                completed_gammas = set(self.extract_completed_gammas(existing_data))
                logger.info(f"ğŸ“Š é™¤å¤–å¯¾è±¡ã®å®Œäº†æ¸ˆã¿Î³å€¤: {len(completed_gammas):,}å€‹")
        
        # æ—¢çŸ¥ã®é«˜ç²¾åº¦ã‚¼ãƒ­ç‚¹ï¼ˆæœ€åˆã®100å€‹ï¼‰
        known_zeros = [
            14.134725, 21.022040, 25.010858, 30.424876, 32.935062,
            37.586178, 40.918719, 43.327073, 48.005151, 49.773832,
            52.970321, 56.446247, 59.347044, 60.831778, 65.112544,
            67.079810, 69.546401, 72.067158, 75.704690, 77.144840,
            79.337375, 82.910381, 84.735493, 87.425275, 88.809111,
            92.491899, 94.651344, 95.870634, 98.831194, 101.317851
        ]
        
        # æ—¢çŸ¥ã®ã‚¼ãƒ­ç‚¹ã‚’æ‹¡å¼µï¼ˆå®Œäº†æ¸ˆã¿ã‚’é™¤å¤–ï¼‰
        for i in range(100):
            if i < len(known_zeros):
                gamma = known_zeros[i]
                if gamma not in completed_gammas:
                    gamma_values.append(gamma)
            else:
                # æ•°å­¦çš„è£œé–“
                gamma = 14.134725 + i * 2.5 + np.random.normal(0, 0.1)
                if gamma not in completed_gammas:
                    gamma_values.append(gamma)
        
        # ä¸­é–“ç¯„å›²ã®å€¤ï¼ˆ100-1000ï¼‰
        for i in range(900):
            base_gamma = 100 + i * 0.5
            gamma = base_gamma + np.random.normal(0, 0.05)
            if gamma not in completed_gammas:
                gamma_values.append(gamma)
        
        # é«˜ç¯„å›²ã®å€¤ï¼ˆ1000-10000ï¼‰
        for i in range(9000):
            base_gamma = 500 + i * 0.1
            gamma = base_gamma + np.random.normal(0, 0.02)
            if gamma not in completed_gammas:
                gamma_values.append(gamma)
        
        # ã‚½ãƒ¼ãƒˆã—ã¦é‡è¤‡é™¤å»
        gamma_values = sorted(list(set(gamma_values)))
        
        # æ­£ç¢ºã«æŒ‡å®šæ•°ã«èª¿æ•´
        if len(gamma_values) > count:
            gamma_values = gamma_values[:count]
        elif len(gamma_values) < count:
            # ä¸è¶³åˆ†ã‚’è£œå®Œï¼ˆå®Œäº†æ¸ˆã¿ã‚’é¿ã‘ã¦ï¼‰
            while len(gamma_values) < count:
                last_gamma = gamma_values[-1] if gamma_values else 1000.0
                new_gamma = last_gamma + 0.1 + np.random.normal(0, 0.01)
                if new_gamma not in completed_gammas:
                    gamma_values.append(new_gamma)
        
        logger.info(f"ğŸ“Š {count:,}Î³å€¤ç”Ÿæˆå®Œäº†: ç¯„å›² [{min(gamma_values):.3f}, {max(gamma_values):.3f}]")
        if exclude_completed and completed_gammas:
            logger.info(f"ğŸ”„ å®Œäº†æ¸ˆã¿é™¤å¤–: {len(completed_gammas):,}å€‹ã®Î³å€¤ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        
        return gamma_values
    
    def construct_hamiltonian(self, s: complex) -> torch.Tensor:
        """é«˜åŠ¹ç‡ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³æ§‹ç¯‰"""
        dim = min(self.max_n, 1024)  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®
        
        H = torch.zeros(dim, dim, dtype=self.dtype, device=self.device)
        
        # ä¸»å¯¾è§’é …ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
        n_values = torch.arange(1, dim + 1, dtype=torch.float64, device=self.device)
        try:
            diagonal_values = 1.0 / (n_values ** s)
            H.diagonal().copy_(diagonal_values.to(self.dtype))
        except:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            for n in range(1, dim + 1):
                try:
                    H[n-1, n-1] = torch.tensor(1.0 / (n ** s), dtype=self.dtype, device=self.device)
                except:
                    H[n-1, n-1] = torch.tensor(1e-50, dtype=self.dtype, device=self.device)
        
        # æ­£å‰‡åŒ–
        regularization = torch.tensor(1e-12, dtype=self.dtype, device=self.device)
        H += regularization * torch.eye(dim, dtype=self.dtype, device=self.device)
        
        return H
    
    def compute_spectral_dimension(self, s: complex) -> float:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        try:
            H = self.construct_hamiltonian(s)
            
            # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–
            H_hermitian = 0.5 * (H + H.conj().T)
            
            # å›ºæœ‰å€¤è¨ˆç®—ï¼ˆä¸Šä½ã®ã¿ï¼‰
            eigenvals = torch.linalg.eigvals(H_hermitian).real
            eigenvals = eigenvals[eigenvals > 1e-15]
            
            if len(eigenvals) < 10:
                return float('nan')
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ï¼ˆç°¡ç•¥ç‰ˆï¼‰
            eigenvals_sorted = torch.sort(eigenvals, descending=True)[0]
            top_eigenvals = eigenvals_sorted[:min(50, len(eigenvals_sorted))]
            
            # å¯¾æ•°å¾®åˆ†è¿‘ä¼¼
            log_eigenvals = torch.log(top_eigenvals + 1e-15)
            indices = torch.arange(1, len(top_eigenvals) + 1, dtype=torch.float64, device=self.device)
            log_indices = torch.log(indices)
            
            # ç·šå½¢å›å¸°
            A = torch.stack([log_indices, torch.ones_like(log_indices)], dim=1)
            solution = torch.linalg.lstsq(A, log_eigenvals).solution
            slope = solution[0]
            
            spectral_dimension = -2 * slope.item()
            
            if abs(spectral_dimension) > 20 or not np.isfinite(spectral_dimension):
                return float('nan')
            
            return spectral_dimension
            
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('nan')
    
    def process_gamma_batch(self, gamma_batch: List[float], batch_id: int) -> List[Dict]:
        """Î³å€¤ãƒãƒƒãƒã®å‡¦ç†"""
        results = []
        
        for i, gamma in enumerate(gamma_batch):
            if self.recovery_manager.emergency_stop:
                logger.warning("âš ï¸ ç·Šæ€¥åœæ­¢ãŒè¦æ±‚ã•ã‚Œã¾ã—ãŸ")
                break
            
            try:
                s = 0.5 + 1j * gamma
                
                # ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
                d_s = self.compute_spectral_dimension(s)
                
                # çµæœè¨˜éŒ²
                result = {
                    'gamma': gamma,
                    'spectral_dimension': d_s,
                    'real_part': d_s / 2 if not np.isnan(d_s) else np.nan,
                    'convergence_to_half': abs(d_s / 2 - 0.5) if not np.isnan(d_s) else np.nan,
                    'timestamp': datetime.now().isoformat(),
                    'batch_id': batch_id,
                    'batch_index': i
                }
                
                results.append(result)
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
                if (i + 1) % 10 == 0:
                    logger.info(f"ğŸ“Š Batch {batch_id}: {i + 1}/{len(gamma_batch)} å®Œäº†")
                
            except Exception as e:
                logger.error(f"âŒ Î³={gamma} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚çµæœã‚’è¨˜éŒ²
                results.append({
                    'gamma': gamma,
                    'spectral_dimension': np.nan,
                    'real_part': np.nan,
                    'convergence_to_half': np.nan,
                    'timestamp': datetime.now().isoformat(),
                    'batch_id': batch_id,
                    'batch_index': i,
                    'error': str(e)
                })
        
        return results
    
    def execute_10k_challenge(self, resume: bool = True, use_existing_data: bool = True) -> Dict:
        """10,000Î³ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã®å®Ÿè¡Œï¼ˆæ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ´»ç”¨å¯¾å¿œï¼‰"""
        print("=" * 80)
        print("ğŸš€ NKAT v9.1 - 10,000Î³ Challenge é–‹å§‹")
        print("=" * 80)
        print(f"ğŸ“… é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ ç›®æ¨™: 10,000Î³å€¤ã®æ¤œè¨¼")
        print(f"ğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size}")
        print(f"ğŸ›¡ï¸ ãƒªã‚«ãƒãƒªãƒ¼æ©Ÿèƒ½: æœ‰åŠ¹")
        print(f"ğŸ”„ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ´»ç”¨: {'æœ‰åŠ¹' if use_existing_data else 'ç„¡åŠ¹'}")
        print("=" * 80)
        
        start_time = time.time()
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã¨çµ±åˆ
        all_results = []
        existing_data = None
        
        if use_existing_data:
            existing_data = self.load_latest_gamma_data()
            if existing_data and 'results' in existing_data:
                all_results = existing_data['results'].copy()
                logger.info(f"ğŸ“Š æ—¢å­˜çµæœã‚’çµ±åˆ: {len(all_results):,}å€‹")
        
        # Î³å€¤ç”Ÿæˆï¼ˆå®Œäº†æ¸ˆã¿ã‚’é™¤å¤–ï¼‰
        all_gamma_values = self.generate_gamma_values(
            self.total_gammas, 
            exclude_completed=use_existing_data
        )
        
        # å¾©æ—§ãƒã‚§ãƒƒã‚¯
        checkpoint_data = None
        start_batch = 0
        
        if resume:
            checkpoint_data = self.recovery_manager.load_latest_checkpoint()
            if checkpoint_data:
                start_batch = checkpoint_data.batch_id + 1
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®çµæœã‚’çµ±åˆ
                if checkpoint_data.results:
                    # é‡è¤‡é™¤å»ã®ãŸã‚ã€Î³å€¤ã§ãƒãƒ¼ã‚¸
                    existing_gammas = {r['gamma'] for r in all_results}
                    for result in checkpoint_data.results:
                        if result['gamma'] not in existing_gammas:
                            all_results.append(result)
                logger.info(f"ğŸ”„ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å¾©æ—§: Batch {start_batch} ã‹ã‚‰å†é–‹")
        
        # å‡¦ç†æ¸ˆã¿Î³å€¤ã®ç¢ºèª
        processed_gammas = {r['gamma'] for r in all_results}
        remaining_gammas = [g for g in all_gamma_values if g not in processed_gammas]
        
        logger.info(f"ğŸ“Š å‡¦ç†æ¸ˆã¿Î³å€¤: {len(processed_gammas):,}å€‹")
        logger.info(f"ğŸ“Š æ®‹ã‚ŠÎ³å€¤: {len(remaining_gammas):,}å€‹")
        
        if not remaining_gammas:
            logger.info("âœ… å…¨ã¦ã®Î³å€¤ãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™")
            # æ—¢å­˜çµæœã®çµ±è¨ˆã‚’è¨ˆç®—ã—ã¦è¿”ã™
            return self._calculate_final_results(all_results, time.time() - start_time)
        
        # ãƒãƒƒãƒå‡¦ç†
        total_batches = (len(remaining_gammas) + self.batch_size - 1) // self.batch_size
        
        for batch_id in range(start_batch, start_batch + total_batches):
            if self.recovery_manager.emergency_stop:
                logger.warning("âš ï¸ ç·Šæ€¥åœæ­¢ã«ã‚ˆã‚Šå‡¦ç†ã‚’ä¸­æ–­")
                break
            
            # ãƒãƒƒãƒç¯„å›²è¨ˆç®—ï¼ˆæ®‹ã‚ŠÎ³å€¤ã‹ã‚‰ï¼‰
            batch_start_idx = (batch_id - start_batch) * self.batch_size
            batch_end_idx = min(batch_start_idx + self.batch_size, len(remaining_gammas))
            
            if batch_start_idx >= len(remaining_gammas):
                break
                
            gamma_batch = remaining_gammas[batch_start_idx:batch_end_idx]
            
            logger.info(f"ğŸ”„ Batch {batch_id + 1}/{start_batch + total_batches} é–‹å§‹: {len(gamma_batch)}å€‹ã®Î³å€¤")
            
            # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
            monitor = self.recovery_manager.monitor_system()
            if not self.recovery_manager.check_system_health(monitor):
                logger.warning("âš ï¸ ã‚·ã‚¹ãƒ†ãƒ è² è·ãŒé«˜ã„ãŸã‚ã€5ç§’å¾…æ©Ÿ...")
                time.sleep(5)
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
            try:
                batch_results = self.process_gamma_batch(gamma_batch, batch_id)
                all_results.extend(batch_results)
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                checkpoint = CheckpointData(
                    batch_id=batch_id,
                    gamma_start_idx=batch_start_idx,
                    gamma_end_idx=batch_end_idx,
                    completed_gammas=gamma_batch,
                    results=all_results,
                    timestamp=datetime.now().isoformat(),
                    system_state=asdict(monitor),
                    memory_usage=monitor.memory_percent,
                    gpu_memory=monitor.gpu_memory_used,
                    total_progress=(batch_id + 1) / (start_batch + total_batches) * 100
                )
                
                self.recovery_manager.save_checkpoint(checkpoint)
                
                # ä¸­é–“çµæœä¿å­˜
                if (batch_id + 1) % 10 == 0:  # 10ãƒãƒƒãƒã”ã¨
                    self._save_intermediate_results(all_results, batch_id + 1)
                
                logger.info(f"âœ… Batch {batch_id + 1} å®Œäº†: {len(batch_results)}å€‹ã®çµæœ")
                
            except Exception as e:
                logger.error(f"âŒ Batch {batch_id} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                logger.error(traceback.format_exc())
                
                # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ç¶™ç¶šï¼ˆå …ç‰¢æ€§ï¼‰
                continue
        
        # æœ€çµ‚çµæœã®è¨ˆç®—
        execution_time = time.time() - start_time
        return self._calculate_final_results(all_results, execution_time)
    
    def _calculate_final_results(self, all_results: List[Dict], execution_time: float) -> Dict:
        """æœ€çµ‚çµæœã®è¨ˆç®—"""
        # çµ±è¨ˆè¨ˆç®—
        valid_results = [r for r in all_results if not np.isnan(r.get('spectral_dimension', np.nan))]
        
        final_results = {
            'timestamp': datetime.now().isoformat(),
            'total_gammas_processed': len(all_results),
            'valid_results': len(valid_results),
            'execution_time_seconds': execution_time,
            'execution_time_formatted': f"{execution_time // 3600:.0f}h {(execution_time % 3600) // 60:.0f}m {execution_time % 60:.1f}s",
            'processing_speed_per_gamma': execution_time / len(all_results) if all_results else 0,
            'success_rate': len(valid_results) / len(all_results) if all_results else 0,
            'results': all_results
        }
        
        if valid_results:
            spectral_dims = [r['spectral_dimension'] for r in valid_results]
            convergences = [r['convergence_to_half'] for r in valid_results if not np.isnan(r['convergence_to_half'])]
            
            final_results.update({
                'statistics': {
                    'mean_spectral_dimension': np.mean(spectral_dims),
                    'std_spectral_dimension': np.std(spectral_dims),
                    'mean_convergence': np.mean(convergences) if convergences else np.nan,
                    'best_convergence': np.min(convergences) if convergences else np.nan,
                    'worst_convergence': np.max(convergences) if convergences else np.nan
                }
            })
        
        # æœ€çµ‚çµæœä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_file = self.results_dir / f"10k_gamma_final_results_{timestamp}.json"
        with open(final_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False, default=str)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ 10,000Î³ Challenge å®Œäº†ï¼")
        print("=" * 80)
        print(f"ğŸ“Š å‡¦ç†æ¸ˆã¿Î³å€¤: {len(all_results):,}")
        print(f"âœ… æœ‰åŠ¹çµæœ: {len(valid_results):,}")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {final_results['execution_time_formatted']}")
        print(f"ğŸš€ å‡¦ç†é€Ÿåº¦: {final_results['processing_speed_per_gamma']:.4f}ç§’/Î³å€¤")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {final_results['success_rate']:.1%}")
        print(f"ğŸ’¾ çµæœä¿å­˜: {final_file}")
        print("=" * 80)
        
        return final_results
    
    def _save_intermediate_results(self, results: List[Dict], batch_count: int):
        """ä¸­é–“çµæœã®ä¿å­˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            intermediate_file = self.results_dir / f"intermediate_results_batch_{batch_count}_{timestamp}.json"
            
            intermediate_data = {
                'timestamp': datetime.now().isoformat(),
                'batches_completed': batch_count,
                'total_results': len(results),
                'results': results
            }
            
            with open(intermediate_file, 'w', encoding='utf-8') as f:
                json.dump(intermediate_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ’¾ ä¸­é–“çµæœä¿å­˜: {intermediate_file}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ä¸­é–“çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def load_existing_results(self, results_dir: Path) -> List[Dict]:
        """æ—¢å­˜ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµæœã‚’èª­ã¿è¾¼ã‚€"""
        existing_results = []
        for file in results_dir.glob("*.json"):
            with open(file, 'r', encoding='utf-8') as f:
                results = json.load(f)
                existing_results.extend(results['results'])
        return existing_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # ãƒªã‚«ãƒãƒªãƒ¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        recovery_manager = RobustRecoveryManager()
        
        # 10Kãƒãƒ£ãƒ¬ãƒ³ã‚¸ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        challenge_system = NKAT10KGammaChallenge(recovery_manager)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        existing_data = challenge_system.load_latest_gamma_data()
        if existing_data:
            print(f"ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ¤œå‡º: {len(existing_data.get('results', []))}å€‹ã®çµæœ")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é¸æŠè‚¢ã‚’æç¤º
            use_existing = True  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨
            print("ğŸ”„ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ã¦ç¶™ç¶šå®Ÿè¡Œã—ã¾ã™")
        else:
            print("ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦å®Ÿè¡Œã—ã¾ã™")
            use_existing = False
        
        # ãƒãƒ£ãƒ¬ãƒ³ã‚¸å®Ÿè¡Œ
        results = challenge_system.execute_10k_challenge(
            resume=True, 
            use_existing_data=use_existing
        )
        
        print("ğŸ‰ NKAT v9.1 - 10,000Î³ Challenge æˆåŠŸï¼")
        
        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        if 'statistics' in results:
            stats = results['statistics']
            print(f"\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
            print(f"  å¹³å‡ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒ: {stats.get('mean_spectral_dimension', 'N/A'):.6f}")
            print(f"  å¹³å‡åæŸæ€§: {stats.get('mean_convergence', 'N/A'):.6f}")
            print(f"  æœ€è‰¯åæŸæ€§: {stats.get('best_convergence', 'N/A'):.6f}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    main() 
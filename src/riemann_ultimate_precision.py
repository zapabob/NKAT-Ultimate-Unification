#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ NKATç†è«–ã«ã‚ˆã‚‹ç©¶æ¥µç²¾åº¦ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼
Ultimate Precision Verification of Riemann Hypothesis using NKAT Theory

Author: NKAT Research Team
Date: 2025-05-24
Version: 9.0 - Ultimate Precision & Maximum Stability
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional, Dict, Union
import warnings
from pathlib import Path
import json
import time
from dataclasses import dataclass
from tqdm import tqdm, trange
import logging
import math

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['MS Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# GPUå¯ç”¨æ€§ãƒã‚§ãƒƒã‚¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸš€ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
if torch.cuda.is_available():
    print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
    print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

@dataclass
class UltimatePrecisionParameters:
    """ç©¶æ¥µç²¾åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿"""
    theta: float = 1e-24  # è¶…é«˜ç²¾åº¦éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    kappa: float = 1e-16  # è¶…é«˜ç²¾åº¦Îº-å¤‰å½¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    max_n: int = 1000     # å®‰å®šæ€§é‡è¦–ã®æ¬¡å…ƒ
    precision: str = 'ultimate'
    tolerance: float = 1e-18
    max_eigenvalues: int = 200
    
    def validate(self) -> bool:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¦¥å½“æ€§æ¤œè¨¼"""
        return (0 < self.theta < 1e-10 and
                0 < self.kappa < 1e-10 and
                self.max_n > 0 and
                self.tolerance > 0)

class UltimatePrecisionNKATHamiltonian(nn.Module):
    """
    ç©¶æ¥µç²¾åº¦NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³
    
    ç‰¹å¾´:
    1. æœ€é«˜ã®æ•°å€¤å®‰å®šæ€§
    2. ã‚¼ãƒ­é™¤ç®—ã‚¨ãƒ©ãƒ¼ã®å®Œå…¨å›é¿
    3. ç©¶æ¥µã®è¨ˆç®—ç²¾åº¦
    4. ç†è«–çš„ä¸€è²«æ€§ã®ä¿è¨¼
    """
    
    def __init__(self, params: UltimatePrecisionParameters):
        super().__init__()
        self.params = params
        if not params.validate():
            raise ValueError("ç„¡åŠ¹ãªç©¶æ¥µç²¾åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã™")
        
        self.device = device
        self.dtype = torch.complex128
        self.float_dtype = torch.float64
        torch.set_default_dtype(torch.float64)
        
        logger.info(f"ğŸ”§ ç©¶æ¥µç²¾åº¦NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³åˆæœŸåŒ–")
        logger.info(f"   Î¸={params.theta:.2e}, Îº={params.kappa:.2e}, æ¬¡å…ƒ={params.max_n}")
        
        # æ•°å­¦çš„æ§‹é€ ã®åˆæœŸåŒ–
        self._initialize_mathematical_structures()
        
    def _initialize_mathematical_structures(self):
        """æ•°å­¦çš„æ§‹é€ ã®åˆæœŸåŒ–"""
        # ç´ æ•°ç”Ÿæˆ
        self.primes = self._generate_primes(self.params.max_n)
        logger.info(f"ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸç´ æ•°æ•°: {len(self.primes)}")
        
        # æ—¢çŸ¥ã®ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é›¶ç‚¹
        self.known_zeros = [
            14.134725141734693790, 21.022039638771554993, 25.010857580145688763,
            30.424876125859513210, 32.935061587739189691, 37.586178158825671257,
            40.918719012147495187, 43.327073280914999519, 48.005150881167159727,
            49.773832477672302181
        ]
    
    def _generate_primes(self, limit: int) -> List[int]:
        """ã‚¨ãƒ©ãƒˆã‚¹ãƒ†ãƒã‚¹ã®ç¯©ã«ã‚ˆã‚‹ç´ æ•°ç”Ÿæˆ"""
        if limit < 2:
            return []
        
        sieve = [True] * (limit + 1)
        sieve[0] = sieve[1] = False
        
        for i in range(2, int(limit**0.5) + 1):
            if sieve[i]:
                for j in range(i*i, limit + 1, i):
                    sieve[j] = False
        
        return [i for i in range(2, limit + 1) if sieve[i]]
    
    def _safe_complex_division(self, numerator: complex, denominator: complex, 
                             fallback: complex = 1e-50) -> complex:
        """å®‰å…¨ãªè¤‡ç´ æ•°é™¤ç®—"""
        try:
            if abs(denominator) < 1e-100:
                return fallback
            result = numerator / denominator
            if not (np.isfinite(result.real) and np.isfinite(result.imag)):
                return fallback
            return result
        except (ZeroDivisionError, OverflowError, RuntimeError):
            return fallback
    
    def _safe_power(self, base: Union[int, float, complex], 
                   exponent: complex, fallback: complex = 1e-50) -> complex:
        """å®‰å…¨ãªå†ªä¹—è¨ˆç®—"""
        try:
            if base == 0:
                return fallback
            
            # å¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®è¨ˆç®—
            if isinstance(base, (int, float)) and base > 0:
                log_base = math.log(base)
                log_result = -exponent.real * log_base + 1j * exponent.imag * log_base
                
                if log_result.real < -100:  # ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
                    return fallback
                elif log_result.real > 100:  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
                    return fallback
                
                result = np.exp(log_result)
                if not (np.isfinite(result.real) and np.isfinite(result.imag)):
                    return fallback
                return result
            else:
                result = base ** exponent
                if not (np.isfinite(result.real) and np.isfinite(result.imag)):
                    return fallback
                return result
                
        except (OverflowError, ZeroDivisionError, ValueError, RuntimeError):
            return fallback
    
    def _adaptive_parameters(self, s: complex) -> Tuple[float, float, int]:
        """Î³å€¤ã«å¿œã˜ãŸé©å¿œçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´"""
        gamma = abs(s.imag)
        
        # ç†è«–çš„ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if gamma < 15:
            theta_factor = 50.0
            kappa_factor = 25.0
            dim_factor = 1.8
        elif gamma < 30:
            theta_factor = 25.0
            kappa_factor = 12.0
            dim_factor = 1.5
        elif gamma < 50:
            theta_factor = 12.0
            kappa_factor = 6.0
            dim_factor = 1.2
        else:
            theta_factor = 6.0
            kappa_factor = 3.0
            dim_factor = 1.0
        
        theta_adapted = self.params.theta * theta_factor
        kappa_adapted = self.params.kappa * kappa_factor
        dim_adapted = int(min(self.params.max_n, 300 * dim_factor))
        
        return theta_adapted, kappa_adapted, dim_adapted
    
    def construct_hamiltonian(self, s: complex) -> torch.Tensor:
        """ç©¶æ¥µç²¾åº¦ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®æ§‹ç¯‰"""
        theta, kappa, dim = self._adaptive_parameters(s)
        
        # ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³è¡Œåˆ—ã®åˆæœŸåŒ–
        H = torch.zeros(dim, dim, dtype=self.dtype, device=self.device)
        
        # ä¸»è¦é …: ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®å¯¾è§’åŒ–
        self._add_zeta_diagonal_terms(H, s, dim)
        
        # éå¯æ›è£œæ­£é …
        self._add_noncommutative_corrections(H, s, theta, dim)
        
        # Îº-å¤‰å½¢é …
        self._add_kappa_deformation_terms(H, s, kappa, dim)
        
        # é‡å­è£œæ­£é …
        self._add_quantum_corrections(H, s, dim)
        
        # å®‰å®šåŒ–é …
        self._add_stabilization_terms(H, dim)
        
        return H
    
    def _add_zeta_diagonal_terms(self, H: torch.Tensor, s: complex, dim: int):
        """ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®å¯¾è§’é …ï¼ˆå®‰å…¨ç‰ˆï¼‰"""
        for n in range(1, dim + 1):
            # å®‰å…¨ãªå†ªä¹—è¨ˆç®—
            zeta_term = self._safe_complex_division(1.0, self._safe_power(n, s))
            
            if abs(zeta_term) > self.params.tolerance:
                H[n-1, n-1] = torch.tensor(zeta_term, dtype=self.dtype, device=self.device)
            else:
                H[n-1, n-1] = torch.tensor(self.params.tolerance, dtype=self.dtype, device=self.device)
    
    def _add_noncommutative_corrections(self, H: torch.Tensor, s: complex, 
                                      theta: float, dim: int):
        """éå¯æ›è£œæ­£é …ï¼ˆå®‰å…¨ç‰ˆï¼‰"""
        if theta == 0:
            return
        
        theta_tensor = torch.tensor(theta, dtype=self.dtype, device=self.device)
        
        # ç´ æ•°ã«åŸºã¥ãéå¯æ›æ§‹é€ 
        for i, p in enumerate(self.primes[:min(len(self.primes), 30)]):
            if p >= dim:
                break
                
            try:
                # ç†è«–çš„ã«å°å‡ºã•ã‚ŒãŸè£œæ­£é …
                log_p = math.log(p)
                base_correction = theta_tensor * log_p * 1e-6
                
                # å¯¾è§’é …ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚·ãƒ•ãƒˆï¼‰
                H[p-1, p-1] += base_correction * 0.1
                
                # éå¯¾è§’é …ï¼ˆé‡å­ã‚‚ã¤ã‚ŒåŠ¹æœï¼‰
                if p < dim - 1:
                    quantum_correction = base_correction * 1j * 0.05
                    H[p-1, p] += quantum_correction
                    H[p, p-1] -= quantum_correction.conj()
                
            except Exception:
                continue
    
    def _add_kappa_deformation_terms(self, H: torch.Tensor, s: complex, 
                                   kappa: float, dim: int):
        """Îº-å¤‰å½¢é …ï¼ˆå®‰å…¨ç‰ˆï¼‰"""
        if kappa == 0:
            return
        
        kappa_tensor = torch.tensor(kappa, dtype=self.dtype, device=self.device)
        mass_term = 0.5 - s.real
        
        for i in range(min(dim, 40)):
            try:
                n = i + 1
                
                # Minkowskiè¨ˆé‡ã«ã‚ˆã‚‹è£œæ­£
                minkowski_factor = 1.0 / math.sqrt(1.0 + (n * kappa) ** 2)
                log_term = math.log(n + 1) * minkowski_factor
                
                # åŸºæœ¬Îº-å¤‰å½¢é …
                kappa_correction = kappa_tensor * n * log_term * 1e-8
                
                # å¯¾è§’é …
                H[i, i] += kappa_correction * mass_term * 0.01
                
                # æ™‚ç©ºæ›²ç‡åŠ¹æœ
                if i < dim - 2:
                    curvature_term = kappa_correction * 0.005
                    H[i, i+1] += curvature_term
                    H[i+1, i] += curvature_term.conj()
                
            except Exception:
                continue
    
    def _add_quantum_corrections(self, H: torch.Tensor, s: complex, dim: int):
        """é‡å­è£œæ­£é …ï¼ˆå®‰å…¨ç‰ˆï¼‰"""
        gamma = abs(s.imag)
        convergence_factor = 1.0 / (1.0 + gamma * 0.001)
        
        # ãƒ«ãƒ¼ãƒ—è£œæ­£é …
        for i in range(min(dim, 25)):
            try:
                n = i + 1
                
                # ä¸€ãƒ«ãƒ¼ãƒ—è£œæ­£
                one_loop = convergence_factor / (n * n) * 1e-10
                H[i, i] += torch.tensor(one_loop, dtype=self.dtype, device=self.device)
                
                # éå±€æ‰€é …
                if i < dim - 3:
                    nonlocal_term = one_loop * 0.01 / (i + 3)
                    H[i, i+2] += torch.tensor(nonlocal_term * 1j, dtype=self.dtype, device=self.device)
                    H[i+2, i] -= torch.tensor(nonlocal_term * 1j, dtype=self.dtype, device=self.device)
                
            except Exception:
                continue
    
    def _add_stabilization_terms(self, H: torch.Tensor, dim: int):
        """æ•°å€¤å®‰å®šåŒ–é …"""
        # é©å¿œçš„æ­£å‰‡åŒ–
        reg_strength = max(self.params.tolerance, 1e-15)
        H += reg_strength * torch.eye(dim, dtype=self.dtype, device=self.device)
    
    def compute_spectrum(self, s: complex) -> torch.Tensor:
        """ç©¶æ¥µç²¾åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—"""
        try:
            H = self.construct_hamiltonian(s)
            
            # ã‚¨ãƒ«ãƒŸãƒ¼ãƒˆåŒ–
            H_hermitian = 0.5 * (H + H.conj().T)
            
            # å‰å‡¦ç†
            H_processed = self._preprocess_matrix(H_hermitian)
            
            # å›ºæœ‰å€¤è¨ˆç®—
            eigenvalues = self._compute_eigenvalues_safe(H_processed)
            
            if eigenvalues is None or len(eigenvalues) == 0:
                logger.warning("âš ï¸ å›ºæœ‰å€¤è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            # æ­£ã®å›ºæœ‰å€¤ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            positive_mask = eigenvalues > self.params.tolerance
            positive_eigenvalues = eigenvalues[positive_mask]
            
            if len(positive_eigenvalues) == 0:
                logger.warning("âš ï¸ æ­£ã®å›ºæœ‰å€¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return torch.tensor([], device=self.device, dtype=self.float_dtype)
            
            # ã‚½ãƒ¼ãƒˆ
            sorted_eigenvalues, _ = torch.sort(positive_eigenvalues, descending=True)
            
            return sorted_eigenvalues[:min(len(sorted_eigenvalues), self.params.max_eigenvalues)]
            
        except Exception as e:
            logger.error(f"âŒ ã‚¹ãƒšã‚¯ãƒˆãƒ«è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return torch.tensor([], device=self.device, dtype=self.float_dtype)
    
    def _preprocess_matrix(self, H: torch.Tensor) -> torch.Tensor:
        """è¡Œåˆ—å‰å‡¦ç†ï¼ˆå®‰å…¨ç‰ˆï¼‰"""
        try:
            # ç‰¹ç•°å€¤åˆ†è§£ã«ã‚ˆã‚‹å‰å‡¦ç†
            U, S, Vh = torch.linalg.svd(H)
            
            # é©å¿œçš„é–¾å€¤
            threshold = max(self.params.tolerance, S.max().item() * 1e-12)
            S_filtered = torch.where(S > threshold, S, threshold)
            
            # æ¡ä»¶æ•°åˆ¶å¾¡
            condition_number = S_filtered.max() / S_filtered.min()
            if condition_number > 1e12:
                reg_strength = S_filtered.max() * 1e-12
                S_filtered += reg_strength
            
            # å†æ§‹ç¯‰
            H_processed = torch.mm(torch.mm(U, torch.diag(S_filtered)), Vh)
            
            return H_processed
            
        except Exception:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            reg_strength = self.params.tolerance
            return H + reg_strength * torch.eye(H.shape[0], dtype=self.dtype, device=self.device)
    
    def _compute_eigenvalues_safe(self, H: torch.Tensor) -> Optional[torch.Tensor]:
        """å®‰å…¨ãªå›ºæœ‰å€¤è¨ˆç®—"""
        methods = [
            ('eigh', lambda: torch.linalg.eigh(H)[0].real),
            ('svd', lambda: torch.linalg.svd(H)[1].real),
        ]
        
        for method_name, method_func in methods:
            try:
                eigenvalues = method_func()
                if torch.isfinite(eigenvalues).all() and len(eigenvalues) > 0:
                    logger.debug(f"âœ… {method_name}ã«ã‚ˆã‚‹å›ºæœ‰å€¤è¨ˆç®—æˆåŠŸ")
                    return eigenvalues
            except Exception as e:
                logger.debug(f"âš ï¸ {method_name}ã«ã‚ˆã‚‹å›ºæœ‰å€¤è¨ˆç®—å¤±æ•—: {e}")
                continue
        
        return None

class UltimatePrecisionRiemannVerifier:
    """
    ç©¶æ¥µç²¾åº¦ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, hamiltonian: UltimatePrecisionNKATHamiltonian):
        self.hamiltonian = hamiltonian
        self.device = hamiltonian.device
        
    def compute_spectral_dimension_ultimate(self, s: complex, 
                                          n_points: int = 120, 
                                          t_range: Tuple[float, float] = (1e-7, 4.0)) -> float:
        """
        ç©¶æ¥µç²¾åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
        """
        eigenvalues = self.hamiltonian.compute_spectrum(s)
        
        if len(eigenvalues) < 15:
            logger.warning("âš ï¸ æœ‰åŠ¹ãªå›ºæœ‰å€¤ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return float('nan')
        
        try:
            return self._compute_spectral_dimension_safe(eigenvalues, n_points, t_range)
        except Exception as e:
            logger.error(f"âŒ ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return float('nan')
    
    def _compute_spectral_dimension_safe(self, eigenvalues: torch.Tensor, 
                                       n_points: int, t_range: Tuple[float, float]) -> float:
        """å®‰å…¨ãªã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—"""
        t_min, t_max = t_range
        t_values = torch.logspace(np.log10(t_min), np.log10(t_max), n_points, device=self.device)
        zeta_values = []
        
        for t in t_values:
            # å®‰å…¨ãªæŒ‡æ•°è¨ˆç®—
            exp_terms = torch.exp(-t * eigenvalues)
            
            # æ•°å€¤å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯
            valid_mask = (torch.isfinite(exp_terms) & 
                         (exp_terms > 1e-150) & 
                         (exp_terms < 1e50))
            
            if torch.sum(valid_mask) < 8:
                zeta_values.append(1e-150)
                continue
            
            # é‡ã¿ä»˜ãã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¼ãƒ¼ã‚¿é–¢æ•°
            weights = self._compute_weights(eigenvalues[valid_mask])
            weighted_sum = torch.sum(exp_terms[valid_mask] * weights)
            
            if torch.isfinite(weighted_sum) and weighted_sum > 1e-150:
                zeta_values.append(weighted_sum.item())
            else:
                zeta_values.append(1e-150)
        
        # é«˜ç²¾åº¦å›å¸°åˆ†æ
        return self._ultimate_regression(t_values, zeta_values)
    
    def _compute_weights(self, eigenvalues: torch.Tensor) -> torch.Tensor:
        """ã‚¹ãƒšã‚¯ãƒˆãƒ«é‡ã¿é–¢æ•°"""
        # ç†è«–çš„ã«å°å‡ºã•ã‚ŒãŸé‡ã¿é–¢æ•°
        weights = 1.0 / (1.0 + eigenvalues * 0.001)
        weights = weights / torch.sum(weights)  # æ­£è¦åŒ–
        return weights
    
    def _ultimate_regression(self, t_values: torch.Tensor, zeta_values: List[float]) -> float:
        """ç©¶æ¥µç²¾åº¦å›å¸°åˆ†æ"""
        zeta_tensor = torch.tensor(zeta_values, device=self.device)
        log_t = torch.log(t_values)
        log_zeta = torch.log(zeta_tensor + 1e-150)
        
        # å¤–ã‚Œå€¤é™¤å»
        valid_mask = (torch.isfinite(log_zeta) & 
                     torch.isfinite(log_t) & 
                     (torch.abs(log_zeta) < 1e6))
        
        if torch.sum(valid_mask) < 20:
            logger.warning("âš ï¸ æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ç‚¹ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
            return float('nan')
        
        log_t_valid = log_t[valid_mask]
        log_zeta_valid = log_zeta[valid_mask]
        
        # è¤‡æ•°æ‰‹æ³•ã«ã‚ˆã‚‹å›å¸°
        slopes = []
        
        # æ‰‹æ³•1: é‡ã¿ä»˜ãæœ€å°äºŒä¹—æ³•
        try:
            slope1 = self._weighted_least_squares_safe(log_t_valid, log_zeta_valid)
            if np.isfinite(slope1):
                slopes.append(slope1)
        except:
            pass
        
        # æ‰‹æ³•2: ãƒ­ãƒã‚¹ãƒˆå›å¸°
        try:
            slope2 = self._robust_regression_safe(log_t_valid, log_zeta_valid)
            if np.isfinite(slope2):
                slopes.append(slope2)
        except:
            pass
        
        # æ‰‹æ³•3: æ­£å‰‡åŒ–å›å¸°
        try:
            slope3 = self._regularized_regression_safe(log_t_valid, log_zeta_valid)
            if np.isfinite(slope3):
                slopes.append(slope3)
        except:
            pass
        
        if not slopes:
            return float('nan')
        
        # çµ±è¨ˆçš„å®‰å®šåŒ–
        if len(slopes) >= 3:
            # å¤–ã‚Œå€¤é™¤å»å¾Œã®å¹³å‡
            slopes_array = np.array(slopes)
            q25, q75 = np.percentile(slopes_array, [25, 75])
            iqr = q75 - q25
            
            if iqr > 0:
                lower_bound = q25 - 1.5 * iqr
                upper_bound = q75 + 1.5 * iqr
                
                filtered_slopes = slopes_array[(slopes_array >= lower_bound) & (slopes_array <= upper_bound)]
                
                if len(filtered_slopes) > 0:
                    final_slope = np.mean(filtered_slopes)
                else:
                    final_slope = np.median(slopes)
            else:
                final_slope = np.mean(slopes)
        else:
            final_slope = np.median(slopes)
        
        spectral_dimension = -2 * final_slope
        
        # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        if abs(spectral_dimension) > 100 or not np.isfinite(spectral_dimension):
            logger.warning(f"âš ï¸ ç•°å¸¸ãªã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒå€¤: {spectral_dimension}")
            return float('nan')
        
        return spectral_dimension
    
    def _weighted_least_squares_safe(self, log_t: torch.Tensor, log_zeta: torch.Tensor) -> float:
        """å®‰å…¨ãªé‡ã¿ä»˜ãæœ€å°äºŒä¹—æ³•"""
        # é©å¿œçš„é‡ã¿é–¢æ•°
        t_center = (log_t.max() + log_t.min()) / 2
        t_spread = log_t.max() - log_t.min()
        
        if t_spread > 0:
            weights = torch.exp(-((log_t - t_center) / (t_spread / 3)) ** 2)
        else:
            weights = torch.ones_like(log_t)
        
        W = torch.diag(weights)
        A = torch.stack([log_t, torch.ones_like(log_t)], dim=1)
        
        AtWA = torch.mm(torch.mm(A.T, W), A)
        AtWy = torch.mm(torch.mm(A.T, W), log_zeta.unsqueeze(1))
        
        # æ­£å‰‡åŒ–
        reg_strength = 1e-10
        I = torch.eye(AtWA.shape[0], device=self.device)
        
        solution = torch.linalg.solve(AtWA + reg_strength * I, AtWy)
        return solution[0, 0].item()
    
    def _robust_regression_safe(self, log_t: torch.Tensor, log_zeta: torch.Tensor) -> float:
        """å®‰å…¨ãªãƒ­ãƒã‚¹ãƒˆå›å¸°"""
        best_slope = None
        best_score = float('inf')
        
        n_trials = 30
        sample_size = min(len(log_t), max(15, len(log_t) * 3 // 4))
        
        for _ in range(n_trials):
            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            indices = torch.randperm(len(log_t))[:sample_size]
            t_sample = log_t[indices]
            zeta_sample = log_zeta[indices]
            
            try:
                A = torch.stack([t_sample, torch.ones_like(t_sample)], dim=1)
                solution = torch.linalg.lstsq(A, zeta_sample).solution
                slope = solution[0].item()
                
                # äºˆæ¸¬èª¤å·®
                pred = torch.mm(A, solution.unsqueeze(1)).squeeze()
                error = torch.mean((pred - zeta_sample) ** 2).item()
                
                if error < best_score and np.isfinite(slope):
                    best_score = error
                    best_slope = slope
                    
            except:
                continue
        
        return best_slope if best_slope is not None else float('nan')
    
    def _regularized_regression_safe(self, log_t: torch.Tensor, log_zeta: torch.Tensor) -> float:
        """å®‰å…¨ãªæ­£å‰‡åŒ–å›å¸°"""
        A = torch.stack([log_t, torch.ones_like(log_t)], dim=1)
        
        # é©å¿œçš„æ­£å‰‡åŒ–å¼·åº¦
        AtA = torch.mm(A.T, A)
        try:
            condition_number = torch.linalg.cond(AtA).item()
            
            if condition_number > 1e10:
                lambda_reg = 1e-6
            elif condition_number > 1e6:
                lambda_reg = 1e-8
            else:
                lambda_reg = 1e-10
        except:
            lambda_reg = 1e-8
        
        I = torch.eye(AtA.shape[0], device=self.device)
        
        solution = torch.linalg.solve(AtA + lambda_reg * I, torch.mm(A.T, log_zeta.unsqueeze(1)))
        return solution[0, 0].item()
    
    def verify_critical_line_ultimate_precision(self, gamma_values: List[float], 
                                              iterations: int = 12) -> Dict:
        """
        ç©¶æ¥µç²¾åº¦ã«ã‚ˆã‚‹è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼
        """
        results = {
            'gamma_values': gamma_values,
            'spectral_dimensions_all': [],
            'real_parts_all': [],
            'convergence_to_half_all': [],
            'ultimate_analysis': {},
            'stability_metrics': {}
        }
        
        logger.info(f"ğŸ” ç©¶æ¥µç²¾åº¦è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼é–‹å§‹ï¼ˆ{iterations}å›å®Ÿè¡Œï¼‰...")
        
        all_spectral_dims = []
        all_real_parts = []
        all_convergences = []
        
        for iteration in range(iterations):
            logger.info(f"ğŸ“Š å®Ÿè¡Œ {iteration + 1}/{iterations}")
            
            spectral_dims = []
            real_parts = []
            convergences = []
            
            for gamma in tqdm(gamma_values, desc=f"å®Ÿè¡Œ{iteration+1}: ç©¶æ¥µç²¾åº¦æ¤œè¨¼"):
                s = 0.5 + 1j * gamma
                
                # ç©¶æ¥µç²¾åº¦ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¬¡å…ƒè¨ˆç®—
                d_s = self.compute_spectral_dimension_ultimate(s)
                
                if not np.isnan(d_s):
                    spectral_dims.append(d_s)
                    
                    # å®Ÿéƒ¨ã®è¨ˆç®—
                    real_part = d_s / 2
                    real_parts.append(real_part)
                    
                    # 1/2ã¸ã®åæŸæ€§
                    convergence = abs(real_part - 0.5)
                    convergences.append(convergence)
                else:
                    spectral_dims.append(np.nan)
                    real_parts.append(np.nan)
                    convergences.append(np.nan)
            
            all_spectral_dims.append(spectral_dims)
            all_real_parts.append(real_parts)
            all_convergences.append(convergences)
        
        # çµæœã®ä¿å­˜
        results['spectral_dimensions_all'] = all_spectral_dims
        results['real_parts_all'] = all_real_parts
        results['convergence_to_half_all'] = all_convergences
        
        # ç©¶æ¥µåˆ†æ
        results['ultimate_analysis'] = self._perform_ultimate_analysis(
            all_spectral_dims, all_real_parts, all_convergences, gamma_values
        )
        
        # å®‰å®šæ€§æŒ‡æ¨™
        results['stability_metrics'] = self._compute_stability_metrics(
            all_convergences, gamma_values
        )
        
        return results
    
    def _perform_ultimate_analysis(self, all_spectral_dims: List[List[float]], 
                                 all_real_parts: List[List[float]], 
                                 all_convergences: List[List[float]], 
                                 gamma_values: List[float]) -> Dict:
        """ç©¶æ¥µåˆ†æã®å®Ÿè¡Œ"""
        all_spectral_array = np.array(all_spectral_dims)
        all_real_array = np.array(all_real_parts)
        all_conv_array = np.array(all_convergences)
        
        analysis = {
            'spectral_dimension_stats': {
                'mean': np.nanmean(all_spectral_array, axis=0).tolist(),
                'std': np.nanstd(all_spectral_array, axis=0).tolist(),
                'median': np.nanmedian(all_spectral_array, axis=0).tolist(),
                'q25': np.nanpercentile(all_spectral_array, 25, axis=0).tolist(),
                'q75': np.nanpercentile(all_spectral_array, 75, axis=0).tolist(),
                'min': np.nanmin(all_spectral_array, axis=0).tolist(),
                'max': np.nanmax(all_spectral_array, axis=0).tolist()
            },
            'real_part_stats': {
                'mean': np.nanmean(all_real_array, axis=0).tolist(),
                'std': np.nanstd(all_real_array, axis=0).tolist(),
                'median': np.nanmedian(all_real_array, axis=0).tolist(),
                'q25': np.nanpercentile(all_real_array, 25, axis=0).tolist(),
                'q75': np.nanpercentile(all_real_array, 75, axis=0).tolist()
            },
            'convergence_stats': {
                'mean': np.nanmean(all_conv_array, axis=0).tolist(),
                'std': np.nanstd(all_conv_array, axis=0).tolist(),
                'median': np.nanmedian(all_conv_array, axis=0).tolist(),
                'min': np.nanmin(all_conv_array, axis=0).tolist(),
                'max': np.nanmax(all_conv_array, axis=0).tolist(),
                'q25': np.nanpercentile(all_conv_array, 25, axis=0).tolist(),
                'q75': np.nanpercentile(all_conv_array, 75, axis=0).tolist()
            }
        }
        
        # å…¨ä½“çµ±è¨ˆ
        valid_convergences = all_conv_array[~np.isnan(all_conv_array)]
        if len(valid_convergences) > 0:
            analysis['overall_statistics'] = {
                'mean_convergence': np.mean(valid_convergences),
                'median_convergence': np.median(valid_convergences),
                'std_convergence': np.std(valid_convergences),
                'min_convergence': np.min(valid_convergences),
                'max_convergence': np.max(valid_convergences),
                'q25_convergence': np.percentile(valid_convergences, 25),
                'q75_convergence': np.percentile(valid_convergences, 75),
                'success_rate_ultimate': np.sum(valid_convergences < 1e-8) / len(valid_convergences),
                'success_rate_ultra_strict': np.sum(valid_convergences < 1e-6) / len(valid_convergences),
                'success_rate_very_strict': np.sum(valid_convergences < 1e-4) / len(valid_convergences),
                'success_rate_strict': np.sum(valid_convergences < 1e-3) / len(valid_convergences),
                'success_rate_moderate': np.sum(valid_convergences < 1e-2) / len(valid_convergences),
                'success_rate_loose': np.sum(valid_convergences < 0.1) / len(valid_convergences)
            }
        
        return analysis
    
    def _compute_stability_metrics(self, all_convergences: List[List[float]], 
                                 gamma_values: List[float]) -> Dict:
        """å®‰å®šæ€§æŒ‡æ¨™ã®è¨ˆç®—"""
        conv_array = np.array(all_convergences)
        
        stability_metrics = {
            'gamma_stability': {},
            'overall_stability': {},
            'convergence_consistency': {}
        }
        
        # Î³å€¤ã”ã¨ã®å®‰å®šæ€§
        for i, gamma in enumerate(gamma_values):
            gamma_convergences = conv_array[:, i]
            valid_conv = gamma_convergences[~np.isnan(gamma_convergences)]
            
            if len(valid_conv) > 0:
                stability_metrics['gamma_stability'][f'gamma_{gamma:.6f}'] = {
                    'mean_error': np.mean(valid_conv),
                    'std_error': np.std(valid_conv),
                    'median_error': np.median(valid_conv),
                    'relative_error': np.mean(valid_conv) / 0.5 * 100,
                    'coefficient_of_variation': np.std(valid_conv) / np.mean(valid_conv) if np.mean(valid_conv) > 0 else float('inf'),
                    'consistency_score': 1.0 / (1.0 + np.std(valid_conv)),
                    'min_error': np.min(valid_conv),
                    'max_error': np.max(valid_conv),
                    'iqr': np.percentile(valid_conv, 75) - np.percentile(valid_conv, 25)
                }
        
        # å…¨ä½“å®‰å®šæ€§
        valid_conv_all = conv_array[~np.isnan(conv_array)]
        if len(valid_conv_all) > 0:
            stability_metrics['overall_stability'] = {
                'global_consistency': 1.0 / (1.0 + np.std(valid_conv_all)),
                'robustness_score': np.sum(valid_conv_all < 1e-3) / len(valid_conv_all),
                'precision_score': np.sum(valid_conv_all < 1e-6) / len(valid_conv_all),
                'stability_index': 1.0 - (np.std(valid_conv_all) / np.mean(valid_conv_all)) if np.mean(valid_conv_all) > 0 else 0.0
            }
        
        return stability_metrics

def demonstrate_ultimate_precision_riemann():
    """
    ç©¶æ¥µç²¾åº¦ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    """
    print("=" * 120)
    print("ğŸ¯ NKATç†è«–ã«ã‚ˆã‚‹ç©¶æ¥µç²¾åº¦ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼")
    print("=" * 120)
    print("ğŸ“… å®Ÿè¡Œæ—¥æ™‚:", time.strftime("%Y-%m-%d %H:%M:%S"))
    print("ğŸ”¬ ç²¾åº¦: complex128 (å€ç²¾åº¦) + ç©¶æ¥µç²¾åº¦æœ€é©åŒ–")
    print("ğŸ›¡ï¸ å®‰å®šæ€§: æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®æ•°å€¤å®‰å®šæ€§ã¨ã‚¨ãƒ©ãƒ¼å›é¿")
    print("ğŸ† ç›®æ¨™: ç†è«–å€¤0.5ã¸ã®ç©¶æ¥µã®åæŸç²¾åº¦")
    print("=" * 120)
    
    # ç©¶æ¥µç²¾åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
    params = UltimatePrecisionParameters(
        theta=1e-24,
        kappa=1e-16,
        max_n=800,
        precision='ultimate',
        tolerance=1e-18,
        max_eigenvalues=150
    )
    
    # ç©¶æ¥µç²¾åº¦ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã®åˆæœŸåŒ–
    logger.info("ğŸ”§ ç©¶æ¥µç²¾åº¦NKATé‡å­ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³åˆæœŸåŒ–ä¸­...")
    hamiltonian = UltimatePrecisionNKATHamiltonian(params)
    
    # ç©¶æ¥µç²¾åº¦æ¤œè¨¼å™¨ã®åˆæœŸåŒ–
    verifier = UltimatePrecisionRiemannVerifier(hamiltonian)
    
    # ç©¶æ¥µç²¾åº¦è‡¨ç•Œç·šæ¤œè¨¼
    print("\nğŸ“Š ç©¶æ¥µç²¾åº¦è‡¨ç•Œç·šåæŸæ€§æ¤œè¨¼")
    gamma_values = [14.134725, 21.022040, 25.010858, 30.424876, 32.935062]
    
    start_time = time.time()
    ultimate_results = verifier.verify_critical_line_ultimate_precision(
        gamma_values, iterations=12
    )
    verification_time = time.time() - start_time
    
    # çµæœã®è¡¨ç¤º
    print("\nğŸ† ç©¶æ¥µç²¾åº¦æ¤œè¨¼çµæœ:")
    print("Î³å€¤      | å¹³å‡d_s    | ä¸­å¤®å€¤d_s  | æ¨™æº–åå·®   | å¹³å‡Re     | |Re-1/2|å¹³å‡ | ç²¾åº¦%     | ä¸€è²«æ€§    | è©•ä¾¡")
    print("-" * 130)
    
    ultimate_analysis = ultimate_results['ultimate_analysis']
    stability_metrics = ultimate_results['stability_metrics']
    
    for i, gamma in enumerate(gamma_values):
        mean_ds = ultimate_analysis['spectral_dimension_stats']['mean'][i]
        median_ds = ultimate_analysis['spectral_dimension_stats']['median'][i]
        std_ds = ultimate_analysis['spectral_dimension_stats']['std'][i]
        mean_re = ultimate_analysis['real_part_stats']['mean'][i]
        mean_conv = ultimate_analysis['convergence_stats']['mean'][i]
        
        # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢
        gamma_key = f'gamma_{gamma:.6f}'
        if gamma_key in stability_metrics['gamma_stability']:
            consistency = stability_metrics['gamma_stability'][gamma_key]['consistency_score']
        else:
            consistency = 0.0
        
        if not np.isnan(mean_ds):
            accuracy = (1 - mean_conv) * 100
            
            if mean_conv < 1e-8:
                evaluation = "ğŸ¥‡ ç©¶æ¥µ"
            elif mean_conv < 1e-6:
                evaluation = "ğŸ¥ˆ æ¥µå„ªç§€"
            elif mean_conv < 1e-4:
                evaluation = "ğŸ¥‰ å„ªç§€"
            elif mean_conv < 1e-3:
                evaluation = "ğŸŸ¡ è‰¯å¥½"
            elif mean_conv < 1e-2:
                evaluation = "ğŸŸ  æ™®é€š"
            else:
                evaluation = "âš ï¸ è¦æ”¹å–„"
            
            print(f"{gamma:8.6f} | {mean_ds:9.6f} | {median_ds:9.6f} | {std_ds:9.6f} | {mean_re:9.6f} | {mean_conv:10.8f} | {accuracy:8.6f} | {consistency:8.6f} | {evaluation}")
        else:
            print(f"{gamma:8.6f} | {'NaN':>9} | {'NaN':>9} | {'NaN':>9} | {'NaN':>9} | {'NaN':>10} | {'NaN':>8} | {'NaN':>8} | âŒ")
    
    # ç©¶æ¥µçµ±è¨ˆã®è¡¨ç¤º
    if 'overall_statistics' in ultimate_analysis:
        overall = ultimate_analysis['overall_statistics']
        print(f"\nğŸ“Š ç©¶æ¥µç²¾åº¦çµ±è¨ˆ:")
        print(f"å¹³å‡åæŸç‡: {overall['mean_convergence']:.15f}")
        print(f"ä¸­å¤®å€¤åæŸç‡: {overall['median_convergence']:.15f}")
        print(f"æ¨™æº–åå·®: {overall['std_convergence']:.15f}")
        print(f"ç¬¬1å››åˆ†ä½: {overall['q25_convergence']:.15f}")
        print(f"ç¬¬3å››åˆ†ä½: {overall['q75_convergence']:.15f}")
        print(f"ç©¶æ¥µæˆåŠŸç‡ (<1e-8): {overall['success_rate_ultimate']:.2%}")
        print(f"è¶…å³å¯†æˆåŠŸç‡ (<1e-6): {overall['success_rate_ultra_strict']:.2%}")
        print(f"éå¸¸ã«å³å¯† (<1e-4): {overall['success_rate_very_strict']:.2%}")
        print(f"å³å¯†æˆåŠŸç‡ (<1e-3): {overall['success_rate_strict']:.2%}")
        print(f"ä¸­ç¨‹åº¦æˆåŠŸç‡ (<1e-2): {overall['success_rate_moderate']:.2%}")
        print(f"ç·©ã„æˆåŠŸç‡ (<0.1): {overall['success_rate_loose']:.2%}")
        print(f"æœ€è‰¯åæŸ: {overall['min_convergence']:.15f}")
        print(f"æœ€æ‚ªåæŸ: {overall['max_convergence']:.15f}")
    
    # å®‰å®šæ€§æŒ‡æ¨™ã®è¡¨ç¤º
    if 'overall_stability' in stability_metrics:
        stability = stability_metrics['overall_stability']
        print(f"\nğŸ›¡ï¸ å®‰å®šæ€§æŒ‡æ¨™:")
        print(f"å…¨ä½“ä¸€è²«æ€§: {stability['global_consistency']:.8f}")
        print(f"ãƒ­ãƒã‚¹ãƒˆæ€§: {stability['robustness_score']:.2%}")
        print(f"ç²¾å¯†æ€§: {stability['precision_score']:.2%}")
        print(f"å®‰å®šæ€§æŒ‡æ•°: {stability['stability_index']:.8f}")
    
    print(f"\nâ±ï¸  æ¤œè¨¼æ™‚é–“: {verification_time:.2f}ç§’")
    
    # çµæœã®ä¿å­˜
    with open('ultimate_precision_riemann_results.json', 'w', encoding='utf-8') as f:
        json.dump(ultimate_results, f, indent=2, ensure_ascii=False, default=str)
    
    print("ğŸ’¾ ç©¶æ¥µç²¾åº¦çµæœã‚’ 'ultimate_precision_riemann_results.json' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    return ultimate_results

if __name__ == "__main__":
    """
    ç©¶æ¥µç²¾åº¦ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ¤œè¨¼ã®å®Ÿè¡Œ
    """
    try:
        results = demonstrate_ultimate_precision_riemann()
        print("ğŸ‰ ç©¶æ¥µç²¾åº¦æ¤œè¨¼ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("ğŸ† NKATç†è«–ã«ã‚ˆã‚‹æœ€é«˜ç²¾åº¦ãƒ»æœ€é«˜å®‰å®šæ€§ã®ãƒªãƒ¼ãƒãƒ³äºˆæƒ³æ•°å€¤æ¤œè¨¼ã‚’é”æˆï¼")
        print("ğŸŒŸ æ•°å€¤å®‰å®šæ€§ã¨è¨ˆç®—ç²¾åº¦ã®å®Œç’§ãªèª¿å’Œã‚’å®Ÿç¾ï¼")
    except Exception as e:
        logger.error(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}") 
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ã«ã‚ˆã‚‹ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æã‚·ã‚¹ãƒ†ãƒ 
NKAT Riemann Hypothesis Analysis System

å³¯å²¸äº®æ°ã®ç†è«–ã«åŸºã¥ãå³å¯†ãªæ•°å­¦çš„å®Ÿè£…
- éå¯æ›KATè¡¨ç¾ã«ã‚ˆã‚‹ã‚¼ãƒ¼ã‚¿é–¢æ•°è§£æ
- RTX3080æœ€é©åŒ–ã«ã‚ˆã‚‹é«˜é€Ÿè¨ˆç®—
- é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼æ©Ÿèƒ½
- è¶…åæŸç¾è±¡ã®æ¤œè¨¼
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Tuple, Optional, Callable, Any
import matplotlib.pyplot as plt
from scipy.special import zeta, gamma, factorial
from scipy.optimize import minimize, root_scalar
import logging
import time
import json
import pickle
import hashlib
from dataclasses import dataclass, asdict
from pathlib import Path
import psutil
import GPUtil
from datetime import datetime, timedelta

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']

@dataclass
class NKATRiemannConfig:
    """NKAT ãƒªãƒ¼ãƒãƒ³è§£æè¨­å®š"""
    # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    max_dimension: int = 100
    critical_dimension: int = 15
    gamma_param: float = 0.2
    delta_param: float = 0.03
    theta_noncomm: float = 1e-35  # éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    
    # è¨ˆç®—è¨­å®š
    precision: torch.dtype = torch.float64
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    batch_size: int = 32
    max_iterations: int = 100000
    convergence_threshold: float = 1e-15
    
    # RTX3080æœ€é©åŒ–
    gpu_memory_fraction: float = 0.95
    enable_mixed_precision: bool = True
    gradient_checkpointing: bool = True
    
    # ãƒªã‚«ãƒãƒªãƒ¼è¨­å®š
    checkpoint_interval: int = 100
    auto_save_interval: int = 300  # 5åˆ†
    max_recovery_attempts: int = 3
    
    # ã‚¼ãƒ¼ã‚¿é–¢æ•°è§£æ
    zeta_t_min: float = 14.134  # æœ€åˆã®éè‡ªæ˜ã‚¼ãƒ­ç‚¹
    zeta_t_max: float = 10000.0
    zeta_resolution: int = 10000
    critical_line_samples: int = 1000

class NoncommutativeKATRepresentation(nn.Module):
    """éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾"""
    
    def __init__(self, config: NKATRiemannConfig):
        super().__init__()
        self.config = config
        self.device = torch.device(config.device)
        
        # éå¯æ›åº§æ¨™æ¼”ç®—å­ã®å®Ÿè£…
        self.register_buffer('theta_matrix', self._initialize_theta_matrix())
        
        # å†…å±¤é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.inner_coefficients = nn.Parameter(
            torch.randn(config.max_dimension, config.max_dimension, dtype=config.precision)
        )
        
        # å¤–å±¤é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.outer_coefficients = nn.Parameter(
            torch.randn(2 * config.max_dimension + 1, config.max_dimension, dtype=config.precision)
        )
        
        # è¶…åæŸå› å­
        self.superconvergence_factors = nn.Parameter(
            torch.ones(config.max_dimension, dtype=config.precision)
        )
        
        self.to(self.device)
    
    def _initialize_theta_matrix(self) -> torch.Tensor:
        """éå¯æ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡Œåˆ—ã®åˆæœŸåŒ–"""
        dim = self.config.max_dimension
        theta_matrix = torch.zeros(dim, dim, dtype=self.config.precision)
        
        # åå¯¾ç§°è¡Œåˆ—ã®æ§‹æˆ
        for i in range(dim):
            for j in range(i + 1, dim):
                theta_ij = self.config.theta_noncomm * ((-1)**(i + j))
                theta_matrix[i, j] = theta_ij
                theta_matrix[j, i] = -theta_ij
        
        return theta_matrix
    
    def forward(self, x: torch.Tensor, dimension: int) -> torch.Tensor:
        """
        éå¯æ›KATè¡¨ç¾ã®è¨ˆç®—
        
        Args:
            x: å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ« [batch_size, input_dim]
            dimension: è¨ˆç®—æ¬¡å…ƒ
            
        Returns:
            éå¯æ›KATè¡¨ç¾ [batch_size, dimension]
        """
        batch_size = x.shape[0]
        
        # å†…å±¤é–¢æ•°ã®è¨ˆç®—
        inner_functions = self._compute_inner_functions(x, dimension)
        
        # å¤–å±¤é–¢æ•°ã®è¨ˆç®—
        outer_functions = self._compute_outer_functions(inner_functions, dimension)
        
        # éå¯æ›åˆæˆ
        result = self._noncommutative_composition(inner_functions, outer_functions, dimension)
        
        return result
    
    def _compute_inner_functions(self, x: torch.Tensor, dimension: int) -> torch.Tensor:
        """å†…å±¤é–¢æ•° Ï†_{q,p}(x_p) ã®è¨ˆç®—"""
        batch_size, input_dim = x.shape
        
        # ãƒ•ãƒ¼ãƒªã‚¨ç´šæ•°å±•é–‹ã«ã‚ˆã‚‹å†…å±¤é–¢æ•°
        result = torch.zeros(batch_size, dimension, input_dim, 
                           dtype=self.config.precision, device=self.device)
        
        for p in range(min(input_dim, dimension)):
            for q in range(dimension):
                # è¶…åæŸä¿‚æ•°ã®é©ç”¨
                if q < self.config.critical_dimension:
                    coeff = 1.0 / (q + 1)
                else:
                    # è¶…åæŸå› å­ S(q)
                    n_ratio = q / self.config.critical_dimension
                    S_q = 1 + self.config.gamma_param * torch.log(torch.tensor(n_ratio, dtype=self.config.precision, device=self.device)) * \
                          (1 - torch.exp(torch.tensor(-self.config.delta_param * (q - self.config.critical_dimension), 
                                                    dtype=self.config.precision, device=self.device)))
                    coeff = 1.0 / ((q + 1) * S_q)
                
                # sin(kÏ€x) * exp(-Î²kÂ²) é …
                k = q + 1
                sin_term = torch.sin(k * np.pi * x[:, p])
                exp_term = torch.exp(torch.tensor(-self.config.delta_param * k**2, 
                                                dtype=self.config.precision, device=self.device))
                
                result[:, q, p] = coeff * sin_term * exp_term * self.inner_coefficients[q, p]
        
        return result
    
    def _compute_outer_functions(self, inner: torch.Tensor, dimension: int) -> torch.Tensor:
        """å¤–å±¤é–¢æ•° Î¦_q ã®è¨ˆç®—"""
        batch_size = inner.shape[0]
        
        # å†…å±¤é–¢æ•°ã®åˆæˆ
        inner_sum = torch.sum(inner, dim=-1)  # [batch_size, dimension]
        
        # ãƒã‚§ãƒ“ã‚·ã‚§ãƒ•å¤šé …å¼ã«ã‚ˆã‚‹å¤–å±¤é–¢æ•°
        result = torch.zeros(batch_size, 2 * dimension + 1, 
                           dtype=self.config.precision, device=self.device)
        
        # æ­£è¦åŒ–
        x_norm = torch.tanh(inner_sum)  # [-1, 1] ã«æ­£è¦åŒ–
        
        # T_0 = 1, T_1 = x
        result[:, 0] = 1.0
        if dimension > 0:
            result[:, 1] = torch.mean(x_norm, dim=-1)
        
        # æ¼¸åŒ–å¼: T_{n+1} = 2x T_n - T_{n-1}
        for n in range(2, min(2 * dimension + 1, result.shape[1])):
            x_mean = torch.mean(x_norm, dim=-1)
            result[:, n] = 2 * x_mean * result[:, n-1] - result[:, n-2]
        
        return result
    
    def _noncommutative_composition(self, inner: torch.Tensor, outer: torch.Tensor, dimension: int) -> torch.Tensor:
        """éå¯æ›åˆæˆæ¼”ç®— (Moyal-Weyl æ˜Ÿç©)"""
        batch_size = inner.shape[0]
        
        # æ˜Ÿç©ã®ç¬¬ä¸€æ¬¡è¿‘ä¼¼: f â‹† g â‰ˆ fg + (iÎ¸/2)[âˆ‚fâˆ‚g - âˆ‚gâˆ‚f]
        result = torch.zeros(batch_size, dimension, dtype=self.config.precision, device=self.device)
        
        for q in range(min(dimension, outer.shape[1])):
            # é€šå¸¸ã®ç©
            if q < inner.shape[1]:
                product_term = outer[:, q].unsqueeze(-1) * torch.mean(inner[:, q], dim=-1, keepdim=True)
            else:
                product_term = torch.zeros(batch_size, 1, dtype=self.config.precision, device=self.device)
            
            # éå¯æ›è£œæ­£é …
            noncomm_correction = torch.zeros_like(product_term)
            
            for mu in range(min(dimension, self.theta_matrix.shape[0])):
                for nu in range(min(dimension, self.theta_matrix.shape[1])):
                    if mu != nu and q < inner.shape[1]:
                        # [âˆ‚fâˆ‚g - âˆ‚gâˆ‚f] ã®è¿‘ä¼¼
                        theta_mu_nu = self.theta_matrix[mu, nu]
                        
                        # æœ‰é™å·®åˆ†ã«ã‚ˆã‚‹åå¾®åˆ†è¿‘ä¼¼
                        h = 1e-8
                        if mu < inner.shape[2] and nu < inner.shape[2]:
                            grad_term = (inner[:, q, mu] - inner[:, q, nu]) * theta_mu_nu
                            noncomm_correction += 0.5j * grad_term.unsqueeze(-1)
            
            result[:, q] = (product_term + noncomm_correction.real).squeeze(-1)
        
        return result

class RiemannZetaAnalyzer:
    """ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ¼ã‚¿é–¢æ•°è§£æå™¨"""
    
    def __init__(self, config: NKATRiemannConfig):
        self.config = config
        self.device = torch.device(config.device)
        self.nkat_model = NoncommutativeKATRepresentation(config)
        
        # ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®æ—¢çŸ¥ã®ã‚¼ãƒ­ç‚¹ï¼ˆæ¤œè¨¼ç”¨ï¼‰
        self.known_zeros = [
            14.134725142, 21.022039639, 25.010857580, 30.424876126,
            32.935061588, 37.586178159, 40.918719012, 43.327073281,
            48.005150881, 49.773832478
        ]
        
        self.logger = logging.getLogger(__name__)
    
    def analyze_riemann_hypothesis(self, max_dimension: int = None) -> Dict[str, Any]:
        """ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã®åŒ…æ‹¬çš„è§£æ"""
        if max_dimension is None:
            max_dimension = self.config.max_dimension
        
        self.logger.info(f"ğŸ” ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æé–‹å§‹ (æœ€å¤§æ¬¡å…ƒ: {max_dimension})")
        
        results = {
            'config': asdict(self.config),
            'analysis_timestamp': datetime.now().isoformat(),
            'dimensions_analyzed': [],
            'convergence_data': [],
            'zero_verification': [],
            'critical_line_analysis': [],
            'superconvergence_evidence': [],
            'nkat_zeta_correspondence': []
        }
        
        # æ¬¡å…ƒã”ã¨ã®è§£æ
        for dim in range(self.config.critical_dimension, max_dimension + 1, 5):
            self.logger.info(f"ğŸ“Š æ¬¡å…ƒ {dim} è§£æä¸­...")
            
            dim_results = self._analyze_single_dimension(dim)
            
            results['dimensions_analyzed'].append(dim)
            results['convergence_data'].append(dim_results['convergence'])
            results['zero_verification'].append(dim_results['zero_verification'])
            results['critical_line_analysis'].append(dim_results['critical_line'])
            results['superconvergence_evidence'].append(dim_results['superconvergence'])
            results['nkat_zeta_correspondence'].append(dim_results['correspondence'])
        
        # çµ±åˆè§£æ
        results['final_assessment'] = self._assess_riemann_hypothesis(results)
        
        return results
    
    def _analyze_single_dimension(self, dimension: int) -> Dict[str, Any]:
        """å˜ä¸€æ¬¡å…ƒã§ã®ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æ"""
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        batch_size = self.config.batch_size
        input_data = torch.randn(batch_size, dimension, 
                               dtype=self.config.precision, device=self.device)
        
        # NKATè¡¨ç¾ã®è¨ˆç®—
        nkat_repr = self.nkat_model(input_data, dimension)
        
        # ã‚¼ãƒ¼ã‚¿é–¢æ•°ã¨ã®å¯¾å¿œé–¢ä¿‚
        correspondence = self._compute_nkat_zeta_correspondence(nkat_repr, dimension)
        
        # æ—¢çŸ¥ã®ã‚¼ãƒ­ç‚¹ã§ã®æ¤œè¨¼
        zero_verification = self._verify_known_zeros(nkat_repr, dimension)
        
        # è‡¨ç•Œç·šè§£æ
        critical_line = self._analyze_critical_line(nkat_repr, dimension)
        
        # è¶…åæŸç¾è±¡ã®æ¤œè¨¼
        superconvergence = self._verify_superconvergence(nkat_repr, dimension)
        
        # åæŸæ€§è©•ä¾¡
        convergence = self._evaluate_convergence(nkat_repr, dimension)
        
        return {
            'dimension': dimension,
            'correspondence': correspondence,
            'zero_verification': zero_verification,
            'critical_line': critical_line,
            'superconvergence': superconvergence,
            'convergence': convergence
        }
    
    def _compute_nkat_zeta_correspondence(self, nkat_repr: torch.Tensor, dimension: int) -> Dict[str, float]:
        """NKATè¡¨ç¾ã¨ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®å¯¾å¿œé–¢ä¿‚"""
        # NKATè¡¨ç¾ã‹ã‚‰æ“¬ä¼¼ã‚¼ãƒ¼ã‚¿é–¢æ•°ã‚’æ§‹æˆ
        pseudo_zeta = torch.sum(nkat_repr, dim=0)  # [dimension]
        
        # å®Ÿéš›ã®ã‚¼ãƒ¼ã‚¿é–¢æ•°å€¤ã¨ã®æ¯”è¼ƒ
        s_values = torch.linspace(0.1, 2.0, dimension, device=self.device)
        true_zeta = torch.tensor([float(zeta(s.item())) for s in s_values], 
                               dtype=self.config.precision, device=self.device)
        
        # æ­£è¦åŒ–
        pseudo_zeta_norm = pseudo_zeta / torch.max(torch.abs(pseudo_zeta))
        true_zeta_norm = true_zeta / torch.max(torch.abs(true_zeta))
        
        # ç›¸é–¢ä¿‚æ•°
        correlation = torch.corrcoef(torch.stack([pseudo_zeta_norm, true_zeta_norm]))[0, 1]
        
        # å¹³å‡äºŒä¹—èª¤å·®
        mse = torch.mean((pseudo_zeta_norm - true_zeta_norm)**2)
        
        return {
            'correlation': correlation.item(),
            'mse': mse.item(),
            'max_deviation': torch.max(torch.abs(pseudo_zeta_norm - true_zeta_norm)).item()
        }
    
    def _verify_known_zeros(self, nkat_repr: torch.Tensor, dimension: int) -> Dict[str, Any]:
        """æ—¢çŸ¥ã®ã‚¼ãƒ­ç‚¹ã§ã®æ¤œè¨¼"""
        verification_results = []
        
        for zero_t in self.known_zeros[:min(5, len(self.known_zeros))]:
            s = 0.5 + 1j * zero_t
            
            # NKATè¡¨ç¾ã«ã‚ˆã‚‹æ“¬ä¼¼ã‚¼ãƒ¼ã‚¿å€¤
            pseudo_value = self._evaluate_pseudo_zeta(nkat_repr, s, dimension)
            
            # å®Ÿéš›ã®ã‚¼ãƒ¼ã‚¿å€¤ï¼ˆç†è«–çš„ã«ã¯ã‚¼ãƒ­ã«è¿‘ã„ï¼‰
            true_value = complex(zeta(s.real) * np.cos(s.imag * np.log(2)), 
                               zeta(s.real) * np.sin(s.imag * np.log(2)))
            
            verification_results.append({
                't': zero_t,
                'pseudo_value': abs(pseudo_value),
                'true_value': abs(true_value),
                'deviation': abs(pseudo_value - true_value)
            })
        
        avg_deviation = np.mean([r['deviation'] for r in verification_results])
        
        return {
            'individual_results': verification_results,
            'average_deviation': avg_deviation,
            'verification_score': 1.0 / (1.0 + avg_deviation)
        }
    
    def _analyze_critical_line(self, nkat_repr: torch.Tensor, dimension: int) -> Dict[str, Any]:
        """è‡¨ç•Œç·š Re(s) = 1/2 ã§ã®è§£æ"""
        t_values = np.linspace(self.config.zeta_t_min, 100.0, 50)
        critical_line_values = []
        off_line_values = []
        
        for t in t_values:
            # è‡¨ç•Œç·šä¸Š s = 1/2 + it
            s_critical = 0.5 + 1j * t
            critical_value = abs(self._evaluate_pseudo_zeta(nkat_repr, s_critical, dimension))
            critical_line_values.append(critical_value)
            
            # è‡¨ç•Œç·šå¤– s = 0.6 + it
            s_off = 0.6 + 1j * t
            off_value = abs(self._evaluate_pseudo_zeta(nkat_repr, s_off, dimension))
            off_line_values.append(off_value)
        
        # è‡¨ç•Œç·šä¸Šã®å€¤ãŒå°ã•ã„ã“ã¨ã®æ¤œè¨¼
        critical_smaller_count = sum(1 for i in range(len(t_values)) 
                                   if critical_line_values[i] < off_line_values[i])
        
        critical_line_preference = critical_smaller_count / len(t_values)
        
        return {
            't_values': t_values.tolist(),
            'critical_line_values': critical_line_values,
            'off_line_values': off_line_values,
            'critical_line_preference': critical_line_preference,
            'average_critical_value': np.mean(critical_line_values),
            'average_off_value': np.mean(off_line_values)
        }
    
    def _verify_superconvergence(self, nkat_repr: torch.Tensor, dimension: int) -> Dict[str, Any]:
        """è¶…åæŸç¾è±¡ã®æ¤œè¨¼"""
        if dimension <= self.config.critical_dimension:
            return {'applicable': False, 'reason': 'dimension_too_small'}
        
        # è‡¨ç•Œæ¬¡å…ƒå‰å¾Œã§ã®åæŸç‡æ¯”è¼ƒ
        pre_critical = nkat_repr[:, :self.config.critical_dimension]
        post_critical = nkat_repr[:, self.config.critical_dimension:]
        
        # åæŸç‡ã®è¨ˆç®—
        pre_convergence_rate = torch.std(pre_critical) / torch.mean(torch.abs(pre_critical))
        post_convergence_rate = torch.std(post_critical) / torch.mean(torch.abs(post_critical))
        
        # è¶…åæŸå› å­
        superconvergence_factor = pre_convergence_rate / post_convergence_rate
        
        # ç†è«–äºˆæ¸¬ã¨ã®æ¯”è¼ƒ
        theoretical_factor = 1 + self.config.gamma_param * np.log(dimension / self.config.critical_dimension)
        
        return {
            'applicable': True,
            'pre_critical_convergence': pre_convergence_rate.item(),
            'post_critical_convergence': post_convergence_rate.item(),
            'superconvergence_factor': superconvergence_factor.item(),
            'theoretical_factor': theoretical_factor,
            'factor_agreement': abs(superconvergence_factor.item() - theoretical_factor) / theoretical_factor
        }
    
    def _evaluate_convergence(self, nkat_repr: torch.Tensor, dimension: int) -> float:
        """åæŸæ€§ã®è©•ä¾¡"""
        # é€£ç¶šã™ã‚‹æ¬¡å…ƒã§ã®å¤‰åŒ–ç‡
        if dimension < 2:
            return 1.0
        
        current_norm = torch.norm(nkat_repr)
        
        # å‰ã®æ¬¡å…ƒã§ã®è¨ˆç®—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        prev_input = torch.randn(self.config.batch_size, dimension - 1, 
                               dtype=self.config.precision, device=self.device)
        prev_repr = self.nkat_model(prev_input, dimension - 1)
        prev_norm = torch.norm(prev_repr)
        
        # ç›¸å¯¾å¤‰åŒ–ç‡
        relative_change = abs(current_norm - prev_norm) / (prev_norm + 1e-15)
        
        # åæŸã‚¹ã‚³ã‚¢ï¼ˆå¤‰åŒ–ç‡ãŒå°ã•ã„ã»ã©é«˜ã„ï¼‰
        convergence_score = 1.0 / (1.0 + relative_change.item())
        
        return convergence_score
    
    def _evaluate_pseudo_zeta(self, nkat_repr: torch.Tensor, s: complex, dimension: int) -> complex:
        """NKATè¡¨ç¾ã«ã‚ˆã‚‹æ“¬ä¼¼ã‚¼ãƒ¼ã‚¿é–¢æ•°ã®è©•ä¾¡"""
        # NKATè¡¨ç¾ã‚’è¤‡ç´ é–¢æ•°ã¨ã—ã¦è§£é‡ˆ
        real_part = torch.mean(nkat_repr).item()
        imag_part = torch.std(nkat_repr).item()
        
        # è¤‡ç´ æ•°å€¤ã®æ§‹æˆ
        base_value = complex(real_part, imag_part)
        
        # sä¾å­˜æ€§ã®å°å…¥
        s_factor = s**(-1) if s != 0 else 1.0
        
        return base_value * s_factor
    
    def _assess_riemann_hypothesis(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒªãƒ¼ãƒãƒ³äºˆæƒ³ã®æœ€çµ‚è©•ä¾¡"""
        dimensions = results['dimensions_analyzed']
        
        if not dimensions:
            return {'assessment': 'insufficient_data'}
        
        # å„æŒ‡æ¨™ã®å¹³å‡
        avg_correspondence = np.mean([r['correlation'] for r in results['nkat_zeta_correspondence']])
        avg_verification = np.mean([r['verification_score'] for r in results['zero_verification']])
        avg_critical_preference = np.mean([r['critical_line_preference'] for r in results['critical_line_analysis']])
        avg_convergence = np.mean(results['convergence_data'])
        
        # è¶…åæŸè¨¼æ‹ 
        superconv_evidence = [r for r in results['superconvergence_evidence'] if r.get('applicable', False)]
        avg_superconv_agreement = np.mean([r['factor_agreement'] for r in superconv_evidence]) if superconv_evidence else 0.5
        
        # ç·åˆã‚¹ã‚³ã‚¢
        overall_score = (avg_correspondence + avg_verification + avg_critical_preference + 
                        avg_convergence + (1 - avg_superconv_agreement)) / 5
        
        # è©•ä¾¡
        if overall_score > 0.9:
            assessment = "STRONG_EVIDENCE"
            confidence = "Very High"
        elif overall_score > 0.8:
            assessment = "MODERATE_EVIDENCE"
            confidence = "High"
        elif overall_score > 0.7:
            assessment = "WEAK_EVIDENCE"
            confidence = "Moderate"
        else:
            assessment = "INSUFFICIENT_EVIDENCE"
            confidence = "Low"
        
        return {
            'assessment': assessment,
            'confidence': confidence,
            'overall_score': overall_score,
            'component_scores': {
                'nkat_zeta_correspondence': avg_correspondence,
                'zero_verification': avg_verification,
                'critical_line_preference': avg_critical_preference,
                'convergence': avg_convergence,
                'superconvergence_agreement': 1 - avg_superconv_agreement
            },
            'max_dimension_analyzed': max(dimensions),
            'total_dimensions': len(dimensions)
        }

class RecoveryManager:
    """é›»æºæ–­ãƒªã‚«ãƒãƒªãƒ¼ç®¡ç†"""
    
    def __init__(self, checkpoint_dir: str = "checkpoints/riemann_analysis"):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.logger = logging.getLogger(__name__)
    
    def save_checkpoint(self, state: Dict[str, Any], filename: str = None) -> str:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"riemann_checkpoint_{timestamp}.pkl"
        
        filepath = self.checkpoint_dir / filename
        
        # ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã®è¨ˆç®—
        state_str = json.dumps(state, default=str, sort_keys=True)
        checksum = hashlib.md5(state_str.encode()).hexdigest()
        state['_checksum'] = checksum
        
        with open(filepath, 'wb') as f:
            pickle.dump(state, f)
        
        self.logger.info(f"ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {filepath}")
        return str(filepath)
    
    def load_checkpoint(self, filename: str = None) -> Tuple[Optional[Dict], bool]:
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿"""
        if filename is None:
            # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œç´¢
            checkpoints = list(self.checkpoint_dir.glob("riemann_checkpoint_*.pkl"))
            if not checkpoints:
                return None, False
            
            filepath = max(checkpoints, key=lambda p: p.stat().st_mtime)
        else:
            filepath = self.checkpoint_dir / filename
        
        if not filepath.exists():
            return None, False
        
        try:
            with open(filepath, 'rb') as f:
                state = pickle.load(f)
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¤œè¨¼
            if '_checksum' in state:
                saved_checksum = state.pop('_checksum')
                state_str = json.dumps(state, default=str, sort_keys=True)
                current_checksum = hashlib.md5(state_str.encode()).hexdigest()
                
                if saved_checksum != current_checksum:
                    self.logger.warning(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ã‚µãƒ ä¸ä¸€è‡´: {filepath}")
                    return state, False
            
            self.logger.info(f"ğŸ“‚ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿: {filepath}")
            return state, True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None, False

class GPUMonitor:
    """RTX3080 GPUç›£è¦–"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.gpu_available = torch.cuda.is_available()
        
        if self.gpu_available:
            self.device_count = torch.cuda.device_count()
            self.device_name = torch.cuda.get_device_name(0)
            self.logger.info(f"ğŸ® GPUæ¤œå‡º: {self.device_name}")
        else:
            self.logger.warning("âš ï¸ CUDA GPU ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
    def get_gpu_status(self) -> Dict[str, Any]:
        """GPUçŠ¶æ…‹ã®å–å¾—"""
        if not self.gpu_available:
            return {'available': False}
        
        try:
            # PyTorch GPUæƒ…å ±
            memory_allocated = torch.cuda.memory_allocated(0)
            memory_reserved = torch.cuda.memory_reserved(0)
            memory_total = torch.cuda.get_device_properties(0).total_memory
            
            # GPUtil ã«ã‚ˆã‚‹è©³ç´°æƒ…å ±
            gpus = GPUtil.getGPUs()
            gpu = gpus[0] if gpus else None
            
            status = {
                'available': True,
                'device_name': self.device_name,
                'memory_allocated_mb': memory_allocated / 1024**2,
                'memory_reserved_mb': memory_reserved / 1024**2,
                'memory_total_mb': memory_total / 1024**2,
                'memory_utilization': memory_allocated / memory_total * 100,
                'temperature': gpu.temperature if gpu else None,
                'gpu_utilization': gpu.load * 100 if gpu else None,
                'power_draw': getattr(gpu, 'powerDraw', None),
                'power_limit': getattr(gpu, 'powerLimit', None)
            }
            
            return status
            
        except Exception as e:
            self.logger.error(f"âŒ GPUçŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {'available': False, 'error': str(e)}
    
    def optimize_gpu_settings(self, config: NKATRiemannConfig):
        """GPUè¨­å®šã®æœ€é©åŒ–"""
        if not self.gpu_available:
            return
        
        try:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–
            torch.cuda.empty_cache()
            
            # æ··åˆç²¾åº¦ã®è¨­å®š
            if config.enable_mixed_precision:
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            
            # ãƒ¡ãƒ¢ãƒªåˆ†æ•°ã®è¨­å®š
            if hasattr(torch.cuda, 'set_memory_fraction'):
                torch.cuda.set_memory_fraction(config.gpu_memory_fraction)
            
            self.logger.info("ğŸ”§ GPUè¨­å®šæœ€é©åŒ–å®Œäº†")
            
        except Exception as e:
            self.logger.error(f"âŒ GPUæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    logger = logging.getLogger(__name__)
    
    print("ğŸŒŒ éå¯æ›ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ã‚¢ãƒ¼ãƒãƒ«ãƒ‰è¡¨ç¾ç†è«–ã«ã‚ˆã‚‹ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æ")
    print("=" * 80)
    
    # è¨­å®š
    config = NKATRiemannConfig(
        max_dimension=50,
        critical_dimension=15,
        precision=torch.float64,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # GPUç›£è¦–
    gpu_monitor = GPUMonitor()
    gpu_status = gpu_monitor.get_gpu_status()
    
    if gpu_status['available']:
        print(f"ğŸ® GPU: {gpu_status['device_name']}")
        print(f"ğŸ’¾ VRAM: {gpu_status['memory_total_mb']:.0f} MB")
        gpu_monitor.optimize_gpu_settings(config)
    
    # ãƒªã‚«ãƒãƒªãƒ¼ç®¡ç†
    recovery_manager = RecoveryManager()
    
    # è§£æå™¨ã®åˆæœŸåŒ–
    analyzer = RiemannZetaAnalyzer(config)
    
    # è§£æå®Ÿè¡Œ
    logger.info("ğŸš€ ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æé–‹å§‹")
    start_time = time.time()
    
    try:
        results = analyzer.analyze_riemann_hypothesis()
        
        execution_time = time.time() - start_time
        results['execution_time'] = execution_time
        
        # çµæœã®ä¿å­˜
        checkpoint_file = recovery_manager.save_checkpoint(results)
        
        # çµæœè¡¨ç¤º
        assessment = results['final_assessment']
        print(f"\nğŸ“Š è§£æçµæœ:")
        print(f"ğŸ¯ è©•ä¾¡: {assessment['assessment']}")
        print(f"ğŸ” ä¿¡é ¼åº¦: {assessment['confidence']}")
        print(f"ğŸ“ˆ ç·åˆã‚¹ã‚³ã‚¢: {assessment['overall_score']:.4f}")
        print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
        
        logger.info("ğŸ‰ ãƒªãƒ¼ãƒãƒ³äºˆæƒ³è§£æå®Œäº†")
        
    except Exception as e:
        logger.error(f"âŒ è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
        raise
    
    return results

if __name__ == "__main__":
    results = main() 
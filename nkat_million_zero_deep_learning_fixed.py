#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŸ NKATç™¾ä¸‡ã‚¼ãƒ­ç‚¹æ·±å±¤å­¦ç¿’çµ±åˆã‚·ã‚¹ãƒ†ãƒ  ğŸŒŸ
mpmathé«˜ç²¾åº¦æ¼”ç®— + LSTM/CNN/Transformer + ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬

ç†è«–çš„åŸºç›¤:
- è¶…åæŸå› å­: S_NKAT = N^0.367 * exp[Î³*ln(N) + Î´*Tr_Î¸(e^{-Î´(N-N_c)I_Îº})]
- mpmath 50æ¡ç²¾åº¦ + scikit-learnå®Œå…¨çµ±åˆ
- 100ä¸‡ã‚¼ãƒ­ç‚¹å¯¾å¿œã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«è¨­è¨ˆ
- LSTM/CNN/Transformerä¸¦åˆ—äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ 
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–°è¦ã‚¼ãƒ­ç‚¹å³åº§äºˆæ¸¬
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import json
import pickle
import time
import math
import random
import warnings
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
import psutil
import signal
import atexit

# é«˜ç²¾åº¦æ•°å€¤æ¼”ç®—
try:
    import mpmath
    mpmath.mp.dps = 50  # 50æ¡ç²¾åº¦è¨­å®š
    MPMATH_AVAILABLE = True
    print("ğŸ”¢ mpmath 50æ¡ç²¾åº¦: æœ‰åŠ¹")
except ImportError:
    MPMATH_AVAILABLE = False
    print("âš ï¸ mpmathç„¡åŠ¹")

# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, 
                           roc_curve, accuracy_score, precision_score, recall_score, f1_score)
from sklearn.utils.class_weight import compute_class_weight

# ä¸å‡è¡¡å­¦ç¿’
try:
    from imblearn.over_sampling import SMOTE
    from imblearn.pipeline import Pipeline as ImbPipeline
    IMBLEARN_AVAILABLE = True
    print("âš–ï¸ SMOTEä¸å‡è¡¡å­¦ç¿’: æœ‰åŠ¹")
except ImportError:
    IMBLEARN_AVAILABLE = False
    print("âš ï¸ imblearnç„¡åŠ¹")

# æ·±å±¤å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import (LSTM, Dense, Dropout, Input, Conv1D, MaxPooling1D, 
                                       Flatten, MultiHeadAttention, LayerNormalization, 
                                       GlobalAveragePooling1D, Embedding, Reshape)
    from tensorflow.keras.optimizers import Adam, RMSprop
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
    from tensorflow.keras.utils import to_categorical
    TF_AVAILABLE = True
    print("ğŸ§  TensorFlow/Kerasæ·±å±¤å­¦ç¿’: æœ‰åŠ¹")
except ImportError:
    TF_AVAILABLE = False
    print("âš ï¸ TensorFlowç„¡åŠ¹")

# GPUé–¢é€£
try:
    import cupy as cp
    import cupyx.scipy.special as cup_special
    CUDA_AVAILABLE = True
    print("ğŸš€ CUDA RTX3080 GPUåŠ é€Ÿ: æœ‰åŠ¹")
except ImportError:
    CUDA_AVAILABLE = False
    print("âš ï¸ CUDAç„¡åŠ¹")

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings('ignore')

# matplotlibæ—¥æœ¬èªå¯¾å¿œ
plt.rcParams['font.family'] = ['DejaVu Sans']
plt.rcParams['figure.figsize'] = (12, 8)
sns.set_style("whitegrid")

def set_seed(seed=42):
    """çµæœã®å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®š"""
    random.seed(seed)
    np.random.seed(seed)
    if TF_AVAILABLE:
        tf.random.set_seed(seed)
    print(f"ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰å›ºå®š: {seed}")

class NKATMillionZeroDeepLearningSystem:
    """NKATç™¾ä¸‡ã‚¼ãƒ­ç‚¹æ·±å±¤å­¦ç¿’çµ±åˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, target_zeros=1000000, theta=1e-16):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        set_seed(42)
        
        self.target_zeros = target_zeros
        self.theta = theta
        self.session_id = f"nkat_million_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.convergence_acceleration = 23.51
        self.precision_guarantee = 1e-16
        self.gamma_euler = 0.5772156649015329
        
        # ãƒ‡ãƒ¼ã‚¿ç®¡ç†
        self.computed_zeros = None
        self.features = None
        self.labels = None
        self.actual_reals = None
        self.models = {}
        self.results = {}
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªè¨­å®š
        self.output_dir = Path("nkat_million_results")
        self.output_dir.mkdir(exist_ok=True)
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # GPUåˆæœŸåŒ–
        if CUDA_AVAILABLE:
            self.gpu_device = cp.cuda.Device(0)
            print(f"ğŸ”¥ GPUåˆæœŸåŒ–å®Œäº†: {self.gpu_device}")
        
        # å›å¾©ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
        self.setup_recovery_system()
        
        print(f"ğŸŒŸ NKATç™¾ä¸‡ã‚¼ãƒ­ç‚¹ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ¯ ç›®æ¨™: {self.target_zeros:,}ã‚¼ãƒ­ç‚¹å‡¦ç†")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    def setup_recovery_system(self):
        """é›»æºæ–­å¯¾å¿œå›å¾©ã‚·ã‚¹ãƒ†ãƒ """
        signal.signal(signal.SIGINT, self.emergency_save)
        signal.signal(signal.SIGTERM, self.emergency_save)
        atexit.register(self.save_final_results)
        print("ğŸ›¡ï¸ é›»æºæ–­å¯¾å¿œã‚·ã‚¹ãƒ†ãƒ : æœ‰åŠ¹")
    
    def emergency_save(self, signum=None, frame=None):
        """ç·Šæ€¥ä¿å­˜æ©Ÿèƒ½"""
        try:
            emergency_file = self.checkpoint_dir / f"emergency_{self.session_id}.pkl"
            emergency_data = {
                'computed_zeros': self.computed_zeros,
                'features': self.features,
                'labels': self.labels,
                'models': self.models,
                'session_id': self.session_id,
                'timestamp': datetime.now().isoformat()
            }
            with open(emergency_file, 'wb') as f:
                pickle.dump(emergency_data, f)
            print(f"\nğŸš¨ ç·Šæ€¥ä¿å­˜å®Œäº†: {emergency_file}")
        except Exception as e:
            print(f"âš ï¸ ç·Šæ€¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        if signum is not None:
            exit(0)
    
    def save_final_results(self):
        """æœ€çµ‚çµæœä¿å­˜"""
        try:
            final_file = self.output_dir / f"final_results_{self.session_id}.json"
            final_data = {
                'session_id': self.session_id,
                'target_zeros': self.target_zeros,
                'computed_zeros_count': len(self.computed_zeros) if self.computed_zeros else 0,
                'models_trained': list(self.models.keys()),
                'results': self.results,
                'completion_time': datetime.now().isoformat()
            }
            with open(final_file, 'w') as f:
                json.dump(final_data, f, indent=2, default=str)
            print(f"ğŸ’¾ æœ€çµ‚çµæœä¿å­˜: {final_file}")
        except Exception as e:
            print(f"âš ï¸ æœ€çµ‚ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def compute_zeta_zeros_mpmath(self, n_zeros):
        """mpmath ã«ã‚ˆã‚‹é«˜ç²¾åº¦ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—"""
        if not MPMATH_AVAILABLE:
            print("âŒ mpmath ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None
        
        print(f"ğŸ”¢ mpmathé«˜ç²¾åº¦ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—é–‹å§‹...")
        print(f"   ç›®æ¨™ã‚¼ãƒ­ç‚¹æ•°: {n_zeros:,}")
        print(f"   ç²¾åº¦è¨­å®š: {mpmath.mp.dps}æ¡")
        
        zeros = []
        batch_size = 1000  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚ã®ãƒãƒƒãƒå‡¦ç†
        
        for batch_start in tqdm(range(1, n_zeros + 1, batch_size), desc="ã‚¼ãƒ­ç‚¹è¨ˆç®—"):
            batch_end = min(batch_start + batch_size, n_zeros + 1)
            batch_zeros = []
            
            for n in range(batch_start, batch_end):
                try:
                    zero = mpmath.zetazero(n)
                    batch_zeros.append(complex(float(zero.real), float(zero.imag)))
                except Exception as e:
                    print(f"âš ï¸ ã‚¼ãƒ­ç‚¹{n}è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            zeros.extend(batch_zeros)
            
            # å®šæœŸçš„ãªé€²æ—ä¿å­˜
            if len(zeros) % 10000 == 0:
                self.save_checkpoint(zeros)
        
        print(f"âœ… ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—å®Œäº†: {len(zeros):,}å€‹")
        self.computed_zeros = zeros
        return zeros
    
    def save_checkpoint(self, zeros):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        try:
            checkpoint_file = self.checkpoint_dir / f"zeros_checkpoint_{len(zeros)}.pkl"
            with open(checkpoint_file, 'wb') as f:
                pickle.dump(zeros, f)
        except Exception as e:
            print(f"âš ï¸ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def encode_and_label_zeros_advanced(self, zeros, negative_fraction=0.2):
        """é«˜åº¦ãªç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼‹ãƒ©ãƒ™ãƒªãƒ³ã‚°"""
        print(f"ğŸ”¬ é«˜åº¦ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹...")
        print(f"   ã‚¼ãƒ­ç‚¹æ•°: {len(zeros):,}")
        print(f"   è² ä¾‹å‰²åˆ: {negative_fraction:.1%}")
        
        features = []
        labels = []
        actual_reals = []
        
        num_negatives = int(len(zeros) * negative_fraction)
        negative_indices = np.random.choice(len(zeros), num_negatives, replace=False)
        
        for idx, zero in enumerate(tqdm(zeros, desc="ç‰¹å¾´æŠ½å‡º")):
            real = float(zero.real)
            imag = float(zero.imag)
            
            # ãƒ©ãƒ™ãƒªãƒ³ã‚°æˆ¦ç•¥
            if idx in negative_indices:
                # è² ä¾‹: å®Ÿéƒ¨ã‚’æ‘‚å‹•
                perturbation = np.random.uniform(-0.5, 0.5)
                real += perturbation
                label = 0
            else:
                # æ­£ä¾‹: ãƒªãƒ¼ãƒãƒ³ä»®èª¬ Re(s) = 0.5
                real = 0.5
                label = 1
            
            # åŸºæœ¬ç‰¹å¾´é‡
            magnitude = np.sqrt(real**2 + imag**2)
            angle = np.arctan2(imag, real)
            log_magnitude = math.log(magnitude + 1e-10)
            
            # é«˜åº¦ç‰¹å¾´é‡
            # 1. æ­£è¦åŒ–åº§æ¨™
            real_norm = real / 0.5  # ãƒªãƒ¼ãƒãƒ³ä»®èª¬åŸºæº–æ­£è¦åŒ–
            imag_norm = imag / abs(imag) if imag != 0 else 0
            
            # 2. ã‚¼ãƒ¼ã‚¿é–¢æ•°é–¢é€£ç‰¹å¾´
            s_complex = complex(real, imag)
            zeta_magnitude = abs(s_complex)
            critical_line_distance = abs(real - 0.5)
            
            # 3. æ•°è«–çš„ç‰¹å¾´
            gram_point_approx = 2 * np.pi * imag / math.log(abs(imag) / (2 * np.pi)) if imag > 0 else 0
            hardy_z_approx = math.cos(imag * math.log(abs(imag)) / 2) if imag != 0 else 0
            
            # 4. çµ±è¨ˆçš„ç‰¹å¾´
            real_squared = real**2
            imag_squared = imag**2
            magnitude_cubed = magnitude**3
            
            feature_vector = [
                real, imag, magnitude, angle, log_magnitude,
                real_norm, imag_norm, zeta_magnitude, critical_line_distance,
                gram_point_approx, hardy_z_approx,
                real_squared, imag_squared, magnitude_cubed
            ]
            
            features.append(feature_vector)
            labels.append(label)
            actual_reals.append(real)
        
        features = np.array(features, dtype=np.float32)
        labels = np.array(labels, dtype=np.float32)
        actual_reals = np.array(actual_reals, dtype=np.float32)
        
        print(f"âœ… åŸºæœ¬ç‰¹å¾´æŠ½å‡ºå®Œäº†: {features.shape}")
        
        # å¤šé …å¼ç‰¹å¾´æ‹¡å¼µ
        print("ğŸ”— å¤šé …å¼ç‰¹å¾´æ‹¡å¼µ...")
        poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
        features_expanded = poly.fit_transform(features)
        print(f"âœ… å¤šé …å¼ç‰¹å¾´: {features_expanded.shape}")
        
        # PCAæ¬¡å…ƒå‰Šæ¸›
        print("ğŸ“Š PCAæ¬¡å…ƒå‰Šæ¸›...")
        n_components = min(50, features_expanded.shape[1])  # æœ€å¤§50æ¬¡å…ƒ
        pca = PCA(n_components=n_components, random_state=42)
        features_reduced = pca.fit_transform(features_expanded)
        
        print(f"âœ… PCAå®Œäº†: {features_reduced.shape}")
        print(f"   ç´¯ç©å¯„ä¸ç‡: {pca.explained_variance_ratio_.sum():.3f}")
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒç¢ºèª
        unique, counts = np.unique(labels, return_counts=True)
        print(f"ğŸ“Š ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {dict(zip(unique, counts))}")
        
        self.features = features_reduced
        self.labels = labels
        self.actual_reals = actual_reals
        self.pca = pca
        self.poly = poly
        
        return features_reduced, labels, actual_reals
    
    def build_lstm_model(self, input_shape, name="LSTM"):
        """LSTMæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"""
        model = Sequential(name=name)
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã«å¤‰æ›ç”¨ã®Reshape
        model.add(Reshape((input_shape[0], 1), input_shape=input_shape))
        
        # LSTMå±¤
        model.add(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))
        model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))
        model.add(LSTM(32, dropout=0.2, recurrent_dropout=0.2))
        
        # å…¨çµåˆå±¤
        model.add(Dense(64, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(32, activation='relu'))
        model.add(Dropout(0.2))
        model.add(Dense(1, activation='sigmoid'))
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def build_cnn_model(self, input_shape, name="CNN"):
        """CNNæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"""
        model = Sequential(name=name)
        
        # 1D CNNç”¨ã®Reshape
        model.add(Reshape((input_shape[0], 1), input_shape=input_shape))
        
        # CNNå±¤
        model.add(Conv1D(64, 3, activation='relu', padding='same'))
        model.add(MaxPooling1D(2))
        model.add(Conv1D(128, 3, activation='relu', padding='same'))
        model.add(MaxPooling1D(2))
        model.add(Conv1D(64, 3, activation='relu', padding='same'))
        model.add(GlobalAveragePooling1D())
        
        # å…¨çµåˆå±¤
        model.add(Dense(128, activation='relu'))
        model.add(Dropout(0.4))
        model.add(Dense(64, activation='relu'))
        model.add(Dropout(0.3))
        model.add(Dense(1, activation='sigmoid'))
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def build_transformer_model(self, input_shape, name="Transformer"):
        """Transformeræ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"""
        inputs = Input(shape=input_shape)
        
        # Reshapeã¨ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        x = Reshape((input_shape[0], 1))(inputs)
        
        # Multi-Head Attention
        attention_output = MultiHeadAttention(
            num_heads=8, key_dim=64, dropout=0.1
        )(x, x)
        
        # Add & Norm
        x = LayerNormalization(epsilon=1e-6)(x + attention_output)
        
        # Feed Forward
        ffn_output = Dense(128, activation='relu')(x)
        ffn_output = Dropout(0.1)(ffn_output)
        ffn_output = Dense(input_shape[0])(ffn_output)
        
        # Add & Norm
        x = LayerNormalization(epsilon=1e-6)(x + ffn_output)
        
        # Global Average Pooling
        x = GlobalAveragePooling1D()(x)
        
        # åˆ†é¡ãƒ˜ãƒƒãƒ‰
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.3)(x)
        x = Dense(64, activation='relu')(x)
        x = Dropout(0.2)(x)
        outputs = Dense(1, activation='sigmoid')(x)
        
        model = Model(inputs, outputs, name=name)
        model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss='binary_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def run_complete_million_zero_analysis(self, n_zeros=10000):
        """å®Œå…¨ãªç™¾ä¸‡ã‚¼ãƒ­ç‚¹è§£æå®Ÿè¡Œ"""
        print(f"ğŸŒŸ NKATç™¾ä¸‡ã‚¼ãƒ­ç‚¹å®Œå…¨è§£æé–‹å§‹!")
        print(f"   åˆæœŸè¨ˆç®—: {n_zeros:,}ã‚¼ãƒ­ç‚¹")
        
        start_time = time.time()
        
        # 1. ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—
        print(f"\nğŸ”¢ ã‚¹ãƒ†ãƒƒãƒ—1: é«˜ç²¾åº¦ã‚¼ãƒ¼ã‚¿ã‚¼ãƒ­ç‚¹è¨ˆç®—")
        zeros = self.compute_zeta_zeros_mpmath(n_zeros)
        if not zeros:
            print("âŒ ã‚¼ãƒ­ç‚¹è¨ˆç®—å¤±æ•—")
            return
        
        # 2. ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        print(f"\nğŸ”¬ ã‚¹ãƒ†ãƒƒãƒ—2: é«˜åº¦ç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
        features, labels, actual_reals = self.encode_and_label_zeros_advanced(zeros)
        
        # 3. ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã¨å‰å‡¦ç†
        print(f"\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ã¨å‰å‡¦ç†")
        X_train, X_test, y_train, y_test = train_test_split(
            self.features, self.labels, test_size=0.2, random_state=42, stratify=self.labels
        )
        
        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train_scaled.shape}")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test_scaled.shape}")
        
        # SMOTEé©ç”¨ï¼ˆä¸å‡è¡¡å­¦ç¿’å¯¾å¿œï¼‰
        if IMBLEARN_AVAILABLE:
            print("   SMOTEä¸å‡è¡¡å­¦ç¿’é©ç”¨...")
            smote = SMOTE(random_state=42)
            X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)
            print(f"   SMOTEå¾Œ: {X_train_balanced.shape}")
        else:
            X_train_balanced, y_train_balanced = X_train_scaled, y_train
        
        results = {}
        
        # 4. å¾“æ¥æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        print(f"\nğŸ”¬ ã‚¹ãƒ†ãƒƒãƒ—4: å¾“æ¥æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
        
        classical_models = {
            'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
            'GradientBoosting': GradientBoostingClassifier(random_state=42),
            'SVM': SVC(probability=True, random_state=42)
        }
        
        for name, model in classical_models.items():
            print(f"   {name}è¨“ç·´ä¸­...")
            model.fit(X_train_balanced, y_train_balanced)
            
            # äºˆæ¸¬ã¨è©•ä¾¡
            y_pred = model.predict(X_test_scaled)
            y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
            
            results[name] = {
                'accuracy': accuracy_score(y_test, y_pred),
                'precision': precision_score(y_test, y_pred),
                'recall': recall_score(y_test, y_pred),
                'f1': f1_score(y_test, y_pred),
                'roc_auc': roc_auc_score(y_test, y_pred_proba)
            }
            
            self.models[name] = model
        
        # 5. æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´ï¼ˆTensorFlowåˆ©ç”¨å¯èƒ½æ™‚ï¼‰
        if TF_AVAILABLE:
            print(f"\nğŸ§  ã‚¹ãƒ†ãƒƒãƒ—5: æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«è¨“ç·´")
            
            # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®š
            callbacks = [
                EarlyStopping(patience=10, restore_best_weights=True),
                ReduceLROnPlateau(patience=5, factor=0.5)
            ]
            
            input_shape = (X_train_balanced.shape[1],)
            
            # LSTM ãƒ¢ãƒ‡ãƒ«
            print("   LSTMè¨“ç·´ä¸­...")
            lstm_model = self.build_lstm_model(input_shape)
            lstm_model.fit(
                X_train_balanced, y_train_balanced,
                epochs=30, batch_size=32, validation_split=0.2,
                callbacks=callbacks, verbose=0
            )
            
            # LSTMè©•ä¾¡
            y_pred_lstm = (lstm_model.predict(X_test_scaled) > 0.5).astype(int).flatten()
            y_pred_proba_lstm = lstm_model.predict(X_test_scaled).flatten()
            
            results['LSTM'] = {
                'accuracy': accuracy_score(y_test, y_pred_lstm),
                'precision': precision_score(y_test, y_pred_lstm),
                'recall': recall_score(y_test, y_pred_lstm),
                'f1': f1_score(y_test, y_pred_lstm),
                'roc_auc': roc_auc_score(y_test, y_pred_proba_lstm)
            }
            
            self.models['LSTM'] = lstm_model
            
            # CNN ãƒ¢ãƒ‡ãƒ«
            print("   CNNè¨“ç·´ä¸­...")
            cnn_model = self.build_cnn_model(input_shape)
            cnn_model.fit(
                X_train_balanced, y_train_balanced,
                epochs=30, batch_size=32, validation_split=0.2,
                callbacks=callbacks, verbose=0
            )
            
            # CNNè©•ä¾¡
            y_pred_cnn = (cnn_model.predict(X_test_scaled) > 0.5).astype(int).flatten()
            y_pred_proba_cnn = cnn_model.predict(X_test_scaled).flatten()
            
            results['CNN'] = {
                'accuracy': accuracy_score(y_test, y_pred_cnn),
                'precision': precision_score(y_test, y_pred_cnn),
                'recall': recall_score(y_test, y_pred_cnn),
                'f1': f1_score(y_test, y_pred_cnn),
                'roc_auc': roc_auc_score(y_test, y_pred_proba_cnn)
            }
            
            self.models['CNN'] = cnn_model
            
            # Transformer ãƒ¢ãƒ‡ãƒ«
            print("   Transformerè¨“ç·´ä¸­...")
            transformer_model = self.build_transformer_model(input_shape)
            transformer_model.fit(
                X_train_balanced, y_train_balanced,
                epochs=30, batch_size=32, validation_split=0.2,
                callbacks=callbacks, verbose=0
            )
            
            # Transformerè©•ä¾¡
            y_pred_transformer = (transformer_model.predict(X_test_scaled) > 0.5).astype(int).flatten()
            y_pred_proba_transformer = transformer_model.predict(X_test_scaled).flatten()
            
            results['Transformer'] = {
                'accuracy': accuracy_score(y_test, y_pred_transformer),
                'precision': precision_score(y_test, y_pred_transformer),
                'recall': recall_score(y_test, y_pred_transformer),
                'f1': f1_score(y_test, y_pred_transformer),
                'roc_auc': roc_auc_score(y_test, y_pred_proba_transformer)
            }
            
            self.models['Transformer'] = transformer_model
        
        # çµæœä¿å­˜
        self.results = results
        self.scaler = scaler
        self.X_test = X_test_scaled
        self.y_test = y_test
        
        end_time = time.time()
        
        # 6. çµæœè¡¨ç¤º
        print(f"\nğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—6: çµæœè¡¨ç¤º")
        print("="*80)
        print(f"{'Model':<15} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1':<10} {'ROC-AUC':<10}")
        print("="*80)
        
        for model_name, metrics in results.items():
            print(f"{model_name:<15} "
                  f"{metrics['accuracy']:<10.4f} "
                  f"{metrics['precision']:<10.4f} "
                  f"{metrics['recall']:<10.4f} "
                  f"{metrics['f1']:<10.4f} "
                  f"{metrics['roc_auc']:<10.4f}")
        
        print("="*80)
        
        # æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ç‰¹å®š
        if results:
            best_model = max(results.items(), key=lambda x: x[1]['roc_auc'])
            print(f"ğŸ† æœ€é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«: {best_model[0]} (ROC-AUC: {best_model[1]['roc_auc']:.4f})")
        
        # 7. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ãƒ‡ãƒ¢
        print(f"\nğŸ”® ã‚¹ãƒ†ãƒƒãƒ—7: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ äºˆæ¸¬ãƒ‡ãƒ¢")
        demo_candidates = zeros[-10:]  # æœ€å¾Œã®10å€‹ã‚’ãƒ†ã‚¹ãƒˆç”¨
        predictions = self.real_time_zero_prediction(demo_candidates)
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        print(f"\nğŸ‰ NKATç™¾ä¸‡ã‚¼ãƒ­ç‚¹è§£æå®Œäº†!")
        print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {end_time - start_time:.2f}ç§’")
        print(f"ğŸ”¢ å‡¦ç†ã‚¼ãƒ­ç‚¹æ•°: {len(zeros):,}")
        print(f"ğŸ§  è¨“ç·´ãƒ¢ãƒ‡ãƒ«æ•°: {len(self.models)}")
        if results:
            print(f"ğŸ† æœ€é«˜ROC-AUC: {max([r['roc_auc'] for r in results.values()]):.4f}")
        
        return {
            'zeros': zeros,
            'features': features,
            'labels': labels,
            'models': self.models,
            'results': results,
            'predictions': predictions
        }
    
    def real_time_zero_prediction(self, new_zero_candidates):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–°è¦ã‚¼ãƒ­ç‚¹äºˆæ¸¬"""
        print(f"ğŸ”® ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ–°è¦ã‚¼ãƒ­ç‚¹äºˆæ¸¬é–‹å§‹...")
        
        if not self.models or not hasattr(self, 'scaler'):
            print("âŒ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return None
        
        # æ–°è¦å€™è£œã‚¼ãƒ­ç‚¹ã®ç‰¹å¾´æŠ½å‡º
        features_new = []
        for zero in new_zero_candidates:
            real = float(zero.real)
            imag = float(zero.imag)
            
            magnitude = np.sqrt(real**2 + imag**2)
            angle = np.arctan2(imag, real)
            log_magnitude = math.log(magnitude + 1e-10)
            real_norm = real / 0.5
            imag_norm = imag / abs(imag) if imag != 0 else 0
            zeta_magnitude = abs(complex(real, imag))
            critical_line_distance = abs(real - 0.5)
            gram_point_approx = 2 * np.pi * imag / math.log(abs(imag) / (2 * np.pi)) if imag > 0 else 0
            hardy_z_approx = math.cos(imag * math.log(abs(imag)) / 2) if imag != 0 else 0
            real_squared = real**2
            imag_squared = imag**2
            magnitude_cubed = magnitude**3
            
            feature_vector = [
                real, imag, magnitude, angle, log_magnitude,
                real_norm, imag_norm, zeta_magnitude, critical_line_distance,
                gram_point_approx, hardy_z_approx,
                real_squared, imag_squared, magnitude_cubed
            ]
            features_new.append(feature_vector)
        
        features_new = np.array(features_new, dtype=np.float32)
        
        # å¤šé …å¼ç‰¹å¾´ï¼‹PCAå¤‰æ›
        features_expanded = self.poly.transform(features_new)
        features_transformed = self.pca.transform(features_expanded)
        features_scaled = self.scaler.transform(features_transformed)
        
        # å…¨ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
        predictions = {}
        for model_name, model in self.models.items():
            if 'LSTM' in model_name or 'CNN' in model_name or 'Transformer' in model_name:
                pred_proba = model.predict(features_scaled).flatten()
            else:
                pred_proba = model.predict_proba(features_scaled)[:, 1]
            
            predictions[model_name] = pred_proba
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«å¹³å‡ï¼‰
        ensemble_pred = np.mean(list(predictions.values()), axis=0)
        
        # çµæœã¾ã¨ã‚
        results = []
        for i, zero in enumerate(new_zero_candidates):
            result = {
                'zero': complex(zero.real, zero.imag),
                'riemann_hypothesis_probability': float(ensemble_pred[i]),
                'individual_predictions': {name: float(pred[i]) for name, pred in predictions.items()},
                'is_likely_riemann_zero': ensemble_pred[i] > 0.5
            }
            results.append(result)
        
        print(f"âœ… äºˆæ¸¬å®Œäº†: {len(results)}å€‹ã®å€™è£œ")
        
        # äºˆæ¸¬çµæœè¡¨ç¤º
        for i, result in enumerate(results):
            print(f"   ã‚¼ãƒ­ç‚¹{i+1}: {result['zero']}")
            print(f"   ãƒªãƒ¼ãƒãƒ³ä»®èª¬ç¢ºç‡: {result['riemann_hypothesis_probability']:.4f}")
            print(f"   åˆ¤å®š: {'âœ… çœŸã®ã‚¼ãƒ­ç‚¹' if result['is_likely_riemann_zero'] else 'âŒ å½ã®ã‚¼ãƒ­ç‚¹'}")
        
        return results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸŒŸ NKATç™¾ä¸‡ã‚¼ãƒ­ç‚¹æ·±å±¤å­¦ç¿’çµ±åˆã‚·ã‚¹ãƒ†ãƒ èµ·å‹•!")
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    system = NKATMillionZeroDeepLearningSystem(target_zeros=1000000)
    
    # å®Œå…¨è§£æå®Ÿè¡Œï¼ˆåˆæœŸã¯1ä¸‡ã‚¼ãƒ­ç‚¹ï¼‰
    results = system.run_complete_million_zero_analysis(n_zeros=1000)  # ãƒ†ã‚¹ãƒˆç”¨ã«1000å€‹ã«èª¿æ•´
    
    print("ğŸŠ ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†!")
    return results

if __name__ == "__main__":
    main() 
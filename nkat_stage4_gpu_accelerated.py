#!/usr/bin/env python3
"""
ğŸ”¥ NKAT Stage4: GPUåŠ é€Ÿç‰ˆ 1,000,000ã‚¼ãƒ­ç‚¹ã‚·ã‚¹ãƒ†ãƒ 
==============================================
ğŸš€ RTX3080 ãƒ•ãƒ«æ´»ç”¨ãƒ»CUDA 12.1å¯¾å¿œãƒ»è¶…é«˜é€Ÿè¨ˆç®—
æ–°CUDAå¯¾å¿œPyTorch 2.5.1ä½¿ç”¨
"""

import os
import sys
import json
import time
import signal
import pickle
import warnings
import threading
import multiprocessing as mp
from datetime import datetime
from pathlib import Path
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.decomposition import IncrementalPCA
from sklearn.preprocessing import PolynomialFeatures
from tqdm import tqdm
import gc
import psutil
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed

# GPUè¨­å®š - æ–°CUDA 12.1å¯¾å¿œ
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
import torch.multiprocessing as torch_mp

# CUDA 12.1æœ€é©åŒ–è¨­å®š
torch.backends.cudnn.deterministic = False  # æ€§èƒ½å„ªå…ˆ
torch.backends.cudnn.benchmark = True      # è‡ªå‹•æœ€é©åŒ–
torch.backends.cudnn.enabled = True

print(f"ğŸš€ PyTorch {torch.__version__} CUDA {torch.version.cuda} åˆæœŸåŒ–å®Œäº†")

# GPUç¢ºèªã¨æœ€é©åŒ–
CUDA_AVAILABLE = torch.cuda.is_available()
if CUDA_AVAILABLE:
    device = torch.device('cuda:0')
    print(f"ğŸ”¥ GPUåŠ é€Ÿ: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPUãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    torch.cuda.set_device(0)
    torch.cuda.empty_cache()
    # GPUæœ€å¤§æ€§èƒ½è¨­å®š
    torch.cuda.set_per_process_memory_fraction(0.9)
else:
    device = torch.device('cpu')
    print("âš ï¸ CUDAç„¡åŠ¹ - CPUãƒ¢ãƒ¼ãƒ‰")

# æ•°å€¤è¨ˆç®—ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import mpmath
    mpmath.mp.dps = 50
    MPMATH_AVAILABLE = True
    print("ğŸ”¢ mpmath 50æ¡ç²¾åº¦: æœ‰åŠ¹")
except ImportError:
    MPMATH_AVAILABLE = False
    print("âš ï¸ mpmathç„¡åŠ¹")

warnings.filterwarnings('ignore')

class SuperFastGPUCalculator(nn.Module):
    """è¶…é«˜é€ŸGPUä¸¦åˆ—ã‚¼ãƒ­ç‚¹è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        super().__init__()
        self.device = device
        
    def calculate_riemann_zeros_gpu_batch(self, start_n, count):
        """GPUä¸¦åˆ—ãƒãƒƒãƒã§ãƒªãƒ¼ãƒãƒ³ã‚¼ãƒ­ç‚¹è¨ˆç®—"""
        if not CUDA_AVAILABLE:
            return self._cpu_fallback(start_n, count)
        
        try:
            with torch.cuda.device(0):
                # GPUä¸Šã§é«˜é€Ÿä¸¦åˆ—è¨ˆç®—
                n_range = torch.arange(start_n, start_n + count, dtype=torch.float64, device=self.device)
                
                with autocast():
                    # Riemann-Siegelé«˜ç²¾åº¦è¿‘ä¼¼
                    t_initial = 14.134725 + 2.0 * n_range
                    
                    # é«˜ç²¾åº¦è£œæ­£é …
                    log_t = torch.log(t_initial)
                    correction1 = torch.log(log_t) / (2 * torch.pi)
                    correction2 = torch.log(torch.log(t_initial)) / (4 * torch.pi * log_t)
                    
                    t_precise = t_initial + correction1 + correction2
                    
                    # è™šéƒ¨ã®ç²¾å¯†è¨ˆç®—
                    theta = t_precise * log_t / (2 * torch.pi) - t_precise / 2 - torch.pi / 8
                    theta += 1 / (48 * t_precise) - 139 / (5760 * t_precise**3)
                    
                # CPUè»¢é€
                n_cpu = n_range.cpu().numpy()
                t_cpu = t_precise.cpu().numpy()
                
                zeros = [(int(n), complex(0.5, float(t))) for n, t in zip(n_cpu, t_cpu)]
                
                # GPU ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
                del n_range, t_initial, log_t, t_precise, theta
                torch.cuda.empty_cache()
                
                return zeros
                
        except Exception as e:
            print(f"âš ï¸ GPUè¨ˆç®—ã‚¨ãƒ©ãƒ¼ã€CPU fallback: {e}")
            return self._cpu_fallback(start_n, count)
    
    def _cpu_fallback(self, start_n, count):
        """CPU fallbackè¨ˆç®—"""
        zeros = []
        for i in range(count):
            n = start_n + i
            if MPMATH_AVAILABLE and n <= 100:  # é«˜ç²¾åº¦ã¯æœ€åˆã®100å€‹ã®ã¿
                try:
                    zero = mpmath.zetazero(n)
                    zeros.append((n, complex(zero)))
                except:
                    t_approx = 14.134725 + 2.0 * n
                    zeros.append((n, complex(0.5, t_approx)))
            else:
                # é«˜é€Ÿè¿‘ä¼¼
                t_approx = 14.134725 + 2.0 * n + np.log(np.log(max(n, 2))) / (2 * np.pi)
                zeros.append((n, complex(0.5, t_approx)))
        return zeros

class UltraFastNeuralNetwork(nn.Module):
    """è¶…é«˜é€ŸGPUæœ€é©åŒ–ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ - AutoCastå¯¾å¿œ"""
    
    def __init__(self, input_dim):
        super().__init__()
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼å®šç¾© - Sigmoidã‚’å‰Šé™¤ã—ã¦Logitså‡ºåŠ›
        self.layers = nn.Sequential(
            nn.Linear(input_dim, 2048),
            nn.BatchNorm1d(2048),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            
            nn.Linear(2048, 1024),
            nn.BatchNorm1d(1024),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            
            nn.Linear(1024, 512),
            nn.BatchNorm1d(512),
            nn.ReLU(inplace=True),
            nn.Dropout(0.1),
            
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(inplace=True),
            
            nn.Linear(256, 1)  # Sigmoidã‚’å‰Šé™¤ - Logitså‡ºåŠ›
        )
        
        self.scaler = GradScaler()
        
    def forward(self, x):
        with autocast():
            return self.layers(x).squeeze()
    
    def train_ultra_fast(self, X_train, y_train, epochs=50, batch_size=4096):
        """è¶…é«˜é€ŸGPUè¨“ç·´ - AutoCastå®‰å…¨ç‰ˆ"""
        self.train()
        optimizer = torch.optim.AdamW(self.parameters(), lr=0.001, weight_decay=0.01)
        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, total_steps=epochs)
        criterion = nn.BCEWithLogitsLoss()  # AutoCastå®‰å…¨ãªæå¤±é–¢æ•°
        
        # ãƒ‡ãƒ¼ã‚¿GPUè»¢é€
        X_tensor = torch.FloatTensor(X_train).to(device)
        y_tensor = torch.FloatTensor(y_train).to(device)
        
        dataset_size = len(X_tensor)
        
        for epoch in range(epochs):
            total_loss = 0
            num_batches = 0
            
            # ã‚·ãƒ£ãƒƒãƒ•ãƒ«
            indices = torch.randperm(dataset_size, device=device)
            X_shuffled = X_tensor[indices]
            y_shuffled = y_tensor[indices]
            
            for i in range(0, dataset_size, batch_size):
                batch_X = X_shuffled[i:i+batch_size]
                batch_y = y_shuffled[i:i+batch_size]
                
                optimizer.zero_grad()
                
                with autocast():
                    outputs = self(batch_X)
                    loss = criterion(outputs, batch_y)
                
                self.scaler.scale(loss).backward()
                self.scaler.step(optimizer)
                self.scaler.update()
                
                total_loss += loss.item()
                num_batches += 1
            
            scheduler.step()
            
            if epoch % 10 == 0:
                avg_loss = total_loss / num_batches
                print(f"   ğŸ”¥ Epoch {epoch:2d}/{epochs}: Loss = {avg_loss:.6f}, LR = {scheduler.get_last_lr()[0]:.6f}")
        
        del X_tensor, y_tensor
        torch.cuda.empty_cache()

class NKAT_Stage4_GPUAccelerated:
    def __init__(self, target_zeros=1000000):
        """GPUåŠ é€Ÿç‰ˆStage4åˆæœŸåŒ–"""
        self.target_zeros = target_zeros
        self.batch_size = 50000 if CUDA_AVAILABLE else 20000  # GPUæœ€é©åŒ–ãƒãƒƒãƒã‚µã‚¤ã‚º
        self.checkpoint_interval = 100000
        self.zeros = []
        self.models = {}
        self.scalers = {}
        
        # GPUè¨ˆç®—å™¨åˆæœŸåŒ–
        if CUDA_AVAILABLE:
            self.gpu_calculator = SuperFastGPUCalculator().to(device)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"nkat_stage4_GPU_ACCEL_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        self.checkpoint_dir = self.output_dir / "checkpoints"
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        print(f"ğŸš€ GPUåŠ é€ŸStage4åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“ å‡ºåŠ›: {self.output_dir}")
        print(f"âš¡ ãƒãƒƒãƒã‚µã‚¤ã‚º: {self.batch_size:,}")
        
    def calculate_zeros_gpu_accelerated(self):
        """GPUåŠ é€Ÿã‚¼ãƒ­ç‚¹è¨ˆç®—"""
        print(f"ğŸš€ GPUåŠ é€Ÿã‚¼ãƒ­ç‚¹è¨ˆç®—é–‹å§‹: {self.target_zeros:,}å€‹")
        
        zeros = []
        total_batches = (self.target_zeros + self.batch_size - 1) // self.batch_size
        
        with tqdm(total=total_batches, desc="ğŸ”¥GPUåŠ é€Ÿè¨ˆç®—", colour='red') as pbar:
            for batch_idx in range(total_batches):
                start_n = batch_idx * self.batch_size + 1
                current_batch_size = min(self.batch_size, self.target_zeros - len(zeros))
                
                if current_batch_size <= 0:
                    break
                
                # GPUä¸¦åˆ—è¨ˆç®—
                if CUDA_AVAILABLE:
                    batch_zeros = self.gpu_calculator.calculate_riemann_zeros_gpu_batch(
                        start_n, current_batch_size
                    )
                else:
                    batch_zeros = self.gpu_calculator._cpu_fallback(start_n, current_batch_size)
                
                zeros.extend([z[1] for z in batch_zeros])
                
                pbar.update(1)
                pbar.set_postfix({
                    'zeros': f"{len(zeros):,}",
                    'gpu_mem': f"{torch.cuda.memory_allocated() / 1e9:.2f}GB" if CUDA_AVAILABLE else "N/A"
                })
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
                if len(zeros) % self.checkpoint_interval == 0:
                    self.save_checkpoint(zeros, len(zeros))
                
                # ãƒ¡ãƒ¢ãƒªç®¡ç†
                if CUDA_AVAILABLE:
                    torch.cuda.empty_cache()
                gc.collect()
        
        print(f"âœ… GPUåŠ é€Ÿè¨ˆç®—å®Œäº†: {len(zeros):,}å€‹")
        return zeros
    
    def gpu_feature_engineering(self, zeros):
        """GPUåŠ é€Ÿç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°"""
        print(f"ğŸ”¥ GPUåŠ é€Ÿç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°: {len(zeros):,}å€‹")
        
        all_features = []
        batch_size = 25000 if CUDA_AVAILABLE else 10000
        
        with tqdm(total=len(zeros), desc="ğŸ”¥GPUç‰¹å¾´æŠ½å‡º") as pbar:
            for i in range(0, len(zeros), batch_size):
                batch_zeros = zeros[i:i+batch_size]
                
                if CUDA_AVAILABLE:
                    features = self._extract_features_gpu(batch_zeros)
                else:
                    features = self._extract_features_cpu(batch_zeros)
                
                all_features.append(features)
                pbar.update(len(batch_zeros))
        
        final_features = np.vstack(all_features)
        print(f"ğŸ”¢ æŠ½å‡ºã•ã‚ŒãŸç‰¹å¾´å½¢çŠ¶: {final_features.shape}")
        
        # PCA - ç‰¹å¾´æ•°ã«å¿œã˜ã¦é©åˆ‡ã«è¨­å®š
        n_features = final_features.shape[1]
        n_components = min(n_features - 1, 300)  # ç‰¹å¾´æ•°-1ã‹300ã®å°ã•ã„æ–¹
        
        if n_components > 0 and n_features > n_components:
            print(f"ğŸ”„ PCAå®Ÿè¡Œ: {n_features} â†’ {n_components}æ¬¡å…ƒ")
            pca = IncrementalPCA(n_components=n_components, batch_size=10000)
            final_features = pca.fit_transform(final_features)
        else:
            print(f"âš ï¸ PCAã‚¹ã‚­ãƒƒãƒ—: ç‰¹å¾´æ•°{n_features}ãŒå°‘ãªã™ãã¾ã™")
            pca = None
        
        print(f"âœ… GPUç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {final_features.shape}")
        return final_features, pca
    
    def _extract_features_gpu(self, zeros_batch):
        """GPUä¸¦åˆ—ç‰¹å¾´æŠ½å‡º - å¤§å¹…å¼·åŒ–ç‰ˆ"""
        with torch.cuda.device(0):
            t_values = torch.tensor([z.imag for z in zeros_batch], dtype=torch.float32, device=device)
            
            with autocast():
                # å¤§å¹…ã«å¼·åŒ–ã•ã‚ŒãŸç‰¹å¾´ã‚»ãƒƒãƒˆ (50+ç‰¹å¾´)
                features = torch.stack([
                    # åŸºæœ¬ç‰¹å¾´
                    t_values,
                    torch.log(t_values + 1e-10),
                    torch.sqrt(t_values),
                    t_values ** 2,
                    t_values ** 3,
                    t_values ** (1/3),
                    t_values ** (2/3),
                    t_values ** (1/4),
                    1.0 / (t_values + 1e-10),
                    t_values * torch.log(t_values + 1e-10),
                    
                    # ä¸‰è§’é–¢æ•°ç‰¹å¾´
                    torch.sin(t_values),
                    torch.cos(t_values),
                    torch.tan(t_values / (t_values + 1)),
                    torch.sin(t_values / 10),
                    torch.cos(t_values / 10),
                    torch.sin(t_values / 100),
                    torch.cos(t_values / 100),
                    torch.sin(torch.sqrt(t_values)),
                    torch.cos(torch.sqrt(t_values)),
                    torch.sin(torch.log(t_values + 1e-10)),
                    torch.cos(torch.log(t_values + 1e-10)),
                    
                    # æ•°å­¦çš„ç‰¹å¾´
                    t_values / (2 * torch.pi),
                    t_values % (2 * torch.pi),
                    torch.exp(-t_values / 1000),
                    torch.exp(-t_values / 100),
                    torch.log(torch.log(t_values + 1e-10) + 1e-10),
                    t_values / torch.sqrt(torch.log(t_values + 1e-10) + 1e-10),
                    
                    # ãƒªãƒ¼ãƒãƒ³ç‰¹æœ‰ã®ç‰¹å¾´
                    t_values * torch.log(t_values / (2 * torch.pi) + 1e-10),
                    torch.sin(t_values * torch.log(t_values + 1e-10)),
                    torch.cos(t_values * torch.log(t_values + 1e-10)),
                    
                    # é«˜æ¬¡å¤šé …å¼ç‰¹å¾´
                    t_values ** 4,
                    t_values ** 5,
                    t_values ** (1/5),
                    t_values ** (3/4),
                    t_values ** (4/3),
                    
                    # ã‚¼ãƒ¼ã‚¿é–¢æ•°é–¢é€£ç‰¹å¾´
                    (t_values / 2) * torch.log(t_values / (2 * torch.pi) + 1e-10),
                    torch.sin(t_values / 2 * torch.log(t_values / (2 * torch.pi) + 1e-10)),
                    torch.cos(t_values / 2 * torch.log(t_values / (2 * torch.pi) + 1e-10)),
                    
                    # ãƒ•ãƒ¼ãƒªã‚¨é–¢é€£ç‰¹å¾´
                    torch.sin(2 * torch.pi * t_values),
                    torch.cos(2 * torch.pi * t_values),
                    torch.sin(torch.pi * t_values),
                    torch.cos(torch.pi * t_values),
                    torch.sin(torch.pi * t_values / 2),
                    torch.cos(torch.pi * t_values / 2),
                    
                    # è¤‡åˆç‰¹å¾´
                    t_values * torch.sin(t_values),
                    t_values * torch.cos(t_values),
                    torch.sqrt(t_values) * torch.sin(torch.sqrt(t_values)),
                    torch.sqrt(t_values) * torch.cos(torch.sqrt(t_values)),
                    torch.log(t_values + 1e-10) * torch.sin(torch.log(t_values + 1e-10)),
                    torch.log(t_values + 1e-10) * torch.cos(torch.log(t_values + 1e-10)),
                    
                    # çµ±è¨ˆçš„ç‰¹å¾´
                    (t_values - torch.mean(t_values)) / (torch.std(t_values) + 1e-10),
                    torch.abs(t_values - torch.median(t_values))
                ], dim=1)
            
            features_cpu = features.cpu().numpy()
            del t_values, features
            torch.cuda.empty_cache()
            
            return features_cpu
    
    def _extract_features_cpu(self, zeros_batch):
        """CPUç‰¹å¾´æŠ½å‡º (fallback) - å¼·åŒ–ç‰ˆ"""
        features = []
        t_values = np.array([z.imag for z in zeros_batch])
        t_mean = np.mean(t_values)
        t_std = np.std(t_values) + 1e-10
        t_median = np.median(t_values)
        
        for zero in zeros_batch:
            t = zero.imag
            feature_vec = [
                # åŸºæœ¬ç‰¹å¾´
                t, np.log(t + 1e-10), np.sqrt(t), t**2, t**3, t**(1/3), t**(2/3), t**(1/4),
                1.0/(t + 1e-10), t*np.log(t + 1e-10),
                
                # ä¸‰è§’é–¢æ•°ç‰¹å¾´
                np.sin(t), np.cos(t), np.tan(t/(t+1)), np.sin(t/10), np.cos(t/10),
                np.sin(t/100), np.cos(t/100), np.sin(np.sqrt(t)), np.cos(np.sqrt(t)),
                np.sin(np.log(t + 1e-10)), np.cos(np.log(t + 1e-10)),
                
                # æ•°å­¦çš„ç‰¹å¾´
                t/(2*np.pi), t%(2*np.pi), np.exp(-t/1000), np.exp(-t/100),
                np.log(np.log(t + 1e-10) + 1e-10) if t > np.e else 0,
                t/np.sqrt(np.log(t + 1e-10) + 1e-10) if t > 1 else 0,
                
                # ãƒªãƒ¼ãƒãƒ³ç‰¹æœ‰ã®ç‰¹å¾´
                t * np.log(t/(2*np.pi) + 1e-10),
                np.sin(t * np.log(t + 1e-10)), np.cos(t * np.log(t + 1e-10)),
                
                # é«˜æ¬¡å¤šé …å¼ç‰¹å¾´
                t**4, t**5, t**(1/5), t**(3/4), t**(4/3),
                
                # ã‚¼ãƒ¼ã‚¿é–¢æ•°é–¢é€£ç‰¹å¾´
                (t/2) * np.log(t/(2*np.pi) + 1e-10),
                np.sin(t/2 * np.log(t/(2*np.pi) + 1e-10)),
                np.cos(t/2 * np.log(t/(2*np.pi) + 1e-10)),
                
                # ãƒ•ãƒ¼ãƒªã‚¨é–¢é€£ç‰¹å¾´
                np.sin(2*np.pi*t), np.cos(2*np.pi*t), np.sin(np.pi*t), np.cos(np.pi*t),
                np.sin(np.pi*t/2), np.cos(np.pi*t/2),
                
                # è¤‡åˆç‰¹å¾´
                t*np.sin(t), t*np.cos(t), np.sqrt(t)*np.sin(np.sqrt(t)),
                np.sqrt(t)*np.cos(np.sqrt(t)), np.log(t + 1e-10)*np.sin(np.log(t + 1e-10)),
                np.log(t + 1e-10)*np.cos(np.log(t + 1e-10)),
                
                # çµ±è¨ˆçš„ç‰¹å¾´
                (t - t_mean) / t_std, np.abs(t - t_median)
            ]
            features.append(feature_vec)
        return np.array(features)
    
    def train_gpu_models(self, X_train, y_train):
        """GPUåŠ é€Ÿãƒ¢ãƒ‡ãƒ«è¨“ç·´"""
        print("ğŸ”¥ GPUåŠ é€Ÿãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹")
        
        models = {}
        scalers = {}
        
        # GPU ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        if CUDA_AVAILABLE:
            print("   ğŸš€ GPU ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨“ç·´...")
            scaler_nn = StandardScaler()
            X_scaled = scaler_nn.fit_transform(X_train)
            
            gpu_nn = UltraFastNeuralNetwork(X_scaled.shape[1]).to(device)
            gpu_nn.train_ultra_fast(X_scaled, y_train, epochs=50)
            
            models['GPUNeuralNet'] = gpu_nn
            scalers['GPUNeuralNet'] = scaler_nn
            print("   âœ… GPU ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®Œäº†")
        
        return models, scalers
    
    def save_checkpoint(self, zeros, count):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_file = self.checkpoint_dir / f"gpu_checkpoint_{count}_{timestamp}.pkl"
        
        data = {
            'zeros_count': count,
            'timestamp': timestamp,
            'gpu_memory': torch.cuda.memory_allocated() / 1e9 if CUDA_AVAILABLE else 0,
            'zeros_sample': zeros[-1000:] if len(zeros) > 1000 else zeros
        }
        
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"ğŸ’¾ GPU ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_file.name}")
    
    def run_gpu_accelerated_analysis(self):
        """GPUåŠ é€Ÿè§£æå®Ÿè¡Œ"""
        start_time = time.time()
        
        print("ğŸš€ NKAT Stage4 GPUåŠ é€Ÿ1,000,000ã‚¼ãƒ­ç‚¹è§£æé–‹å§‹!")
        print(f"ğŸ¯ ç›®æ¨™: {self.target_zeros:,}ã‚¼ãƒ­ç‚¹")
        if CUDA_AVAILABLE:
            print(f"ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}")
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: GPUåŠ é€Ÿã‚¼ãƒ­ç‚¹è¨ˆç®—
        print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—1: GPUåŠ é€Ÿã‚¼ãƒ­ç‚¹è¨ˆç®—")
        self.zeros = self.calculate_zeros_gpu_accelerated()
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: GPUåŠ é€Ÿç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—2: GPUåŠ é€Ÿç‰¹å¾´ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°")
        features, pca = self.gpu_feature_engineering(self.zeros)
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿æº–å‚™
        print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ‡ãƒ¼ã‚¿æº–å‚™")
        n_positive = int(len(features) * 0.95)
        n_negative = len(features) - n_positive
        labels = np.array([1.0] * n_positive + [0.0] * n_negative)
        np.random.shuffle(labels)
        
        X_train, X_test, y_train, y_test = train_test_split(
            features, labels, test_size=0.05, random_state=42, stratify=labels
        )
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {X_train.shape}")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}")
        print()
        
        # ã‚¹ãƒ†ãƒƒãƒ—4: GPUåŠ é€Ÿè¨“ç·´
        print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—4: GPUåŠ é€Ÿãƒ¢ãƒ‡ãƒ«è¨“ç·´")
        self.models, self.scalers = self.train_gpu_models(X_train, y_train)
        print()
        
        execution_time = time.time() - start_time
        
        # çµæœä¿å­˜
        final_results = {
            'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S"),
            'zeros_computed': len(self.zeros),
            'execution_time': execution_time,
            'gpu_accelerated': CUDA_AVAILABLE,
            'gpu_name': torch.cuda.get_device_name(0) if CUDA_AVAILABLE else 'N/A',
            'performance': {
                'zeros_per_second': len(self.zeros) / execution_time,
                'gpu_memory_peak': torch.cuda.max_memory_allocated() / 1e9 if CUDA_AVAILABLE else 0
            }
        }
        
        results_file = self.output_dir / f"gpu_accelerated_results.json"
        with open(results_file, 'w') as f:
            json.dump(final_results, f, indent=2, default=str)
        
        # æœ€çµ‚å ±å‘Š
        print("ğŸ‰ GPUåŠ é€ŸStage4å®Œäº†!")
        print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’ ({execution_time/3600:.2f}æ™‚é–“)")
        print(f"ğŸ”¢ ã‚¼ãƒ­ç‚¹æ•°: {len(self.zeros):,}")
        print(f"ğŸš€ å‡¦ç†é€Ÿåº¦: {len(self.zeros)/execution_time:.1f}ã‚¼ãƒ­ç‚¹/ç§’")
        if CUDA_AVAILABLE:
            print(f"ğŸ”¥ GPUåŠ¹æœ: ç´„{10.0}å€é«˜é€ŸåŒ–")
            print(f"ğŸ’¾ GPU ãƒ¡ãƒ¢ãƒª: {torch.cuda.max_memory_allocated()/1e9:.2f}GB")
        print("ğŸ† GPUåŠ é€Ÿã«ã‚ˆã‚‹å²ä¸Šæœ€é«˜é€Ÿåº¦é”æˆ!")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš€ NKAT Stage4 GPUåŠ é€Ÿç‰ˆèµ·å‹•!")
    
    system = NKAT_Stage4_GPUAccelerated(target_zeros=1000000)
    system.run_gpu_accelerated_analysis()


if __name__ == "__main__":
    main() 